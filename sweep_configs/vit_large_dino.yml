# Device Arguments

accelerator: cuda               # Device accelerator (e.g., "cuda" for GPU)
num_devices: 1                  # Number of devices (GPUs) to use
distributed_strategy: "auto"    # Distributed training strategy (if applicable)
force_deterministic: True      # Force deterministic behavior
precision: 32           # Training precision (e.g., "bf16-mixed")
compile: False                  # Compile the model for faster execution
workers: 16                      # Number of workers for data loading


# Model and Training Arguments

project_name: "Embedding-VitLarge-SimCLR-Finetuning"
run_name: "000-test-vit_large_simclr_finetuning" # Name for this training run form: <Issue Number>-<Purpose> eg. #123-testing-new-data
model_name_or_path: "simclr/models/ssl/vit-large-simclr-face-sweep/trfl43a/checkpoints/epoch-1-cxl-all/val/embeddings/knn_crossvideo_cos/accuracy-0.59.ckpt" # Model name or path
saved_checkpoint_path: Null     # Path to a saved model checkpoint (if resuming)
fast_dev_run: False             # Enable fast development run

seed: 42                        # Random seed for reproducibility
early_stopping_patience: 5      # Early stopping patience (number of epochs)
min_delta: 0.005
embedding_size: 256
dropout_p: 0.32
embedding_id: "linear_norm_dropout"

beta1: 0.9                      # Adam optimizer's beta1 parameter
beta2: 0.999                    # Adam optimizer's beta2 parameter
epsilon: 1e-7                   # Adam optimizer's epsilon

# L2SP Arguments
l2_alpha: 0.01                       # Alpha for the l2sp loss
l2_beta: 0.01                       # Beta for the l2sp loss
path_to_pretrained_weights: "pretrained_weights/vit_large_patch14_dinov2_lvd142m_simclr.pth"    # Path to the pretrained weights for the l2sp loss

margin: 0.5                   # Margin for the contrastive loss (triplet loss)
cross_video_masking: True
loss_dist_term: "euclidean" 
loss_mode: "online/soft/l2sp"        # Loss modes are "offline", "offline/native", "online/soft", "online/semi-hard", "online/hard", "softmax/arcface" 
lr_schedule: "cosine"           # Learning rate schedule (e.g., "linear", "cosine", "exponential", "reduce_on_plateau") TODO: add 
warmup_mode: "constant"           # Warmup mode (e.g., "linear", "cosine", "exponential", "constant")
warmup_epochs: 0                # Number of warmup epochs (if 0 no warmup is performed)
initial_lr: 1e-6                # Initial learning rate before warmup(must be > 0.0)
start_lr: 1e-6                  # Learning Rate after warmup at the beginning of 'normal' scheduling
end_lr: 1e-7                    # End learning rate (for the learning rate schedule -> cosine learning rate schedule)

batch_size: 8                # Training batch size
grad_clip: 1.0                   # Gradient clipping value
gradient_accumulation_steps: 1  # Gradient accumulation steps
max_epochs: 100               # Training goal (large number)
val_before_training: True      # Perform validation before training
kfold: True                     # Perform kfold cross validation

save_interval: 5               # Model checkpoint save interval as a fraction of total steps
embedding_save_interval: 5      # Embedding save interval

data_dir: "/workspaces/gorillatracker/data/supervised/splits/cxl_faces_openset_seed42_square_kfold-5" # <- CXL2
dataset_class: "gorillatracker.datasets.kfold_cxl.CrossEncounterKFoldCXLDataset"
data_resize_transform: 224

fix_img_size: 224
stop_saving_metric_name: "cxlkfold/val/embeddings/knn_crossvideo_cos/accuracy"
stop_saving_metric_mode: "max"
check_val_every_n_epoch: 5

force_nlet_builder: "quadlet"
dataset_names: ["cxlkfold", "bristol", "test"]
additional_val_dataset_classes: ["gorillatracker.datasets.bristol.BristolDataset", "gorillatracker.datasets.cxl.CXLDataset"]
additional_val_data_dirs: ["/workspaces/gorillatracker/data/supervised/bristol/cross_encounter_validation/cropped_frames_square_filtered", "/workspaces/gorillatracker/data/supervised/splits/cxl_faces_openset_seed42_square_kfold-5/test"]