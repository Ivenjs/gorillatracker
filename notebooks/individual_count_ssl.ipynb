{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE(liamvdv): todo: move to SSLConfig\n",
    "# from gorillatracker.ssl_pipeline.ssl_config import SSLConfig\n",
    "\n",
    "from sqlalchemy import select, text, create_engine\n",
    "from sqlalchemy.orm import Session\n",
    "from gorillatracker.ssl_pipeline.models import TrackingFrameFeature\n",
    "from gorillatracker.ssl_pipeline.dataset import GorillaDatasetKISZ\n",
    "\n",
    "\n",
    "sample = 10\n",
    "query = text(\n",
    "    f\"\"\"WITH ranked_features AS (\n",
    "    SELECT\n",
    "        tracking_id,\n",
    "        tracking_frame_feature_id,\n",
    "        bbox_width,\n",
    "        bbox_height,\n",
    "        frame_nr,\n",
    "        feature_type,\n",
    "        ROW_NUMBER() OVER (PARTITION BY tracking_id ORDER BY RANDOM()) AS rn\n",
    "    FROM tracking_frame_feature\n",
    "    WHERE feature_type = 'face_45'\n",
    "        AND bbox_width >= 184\n",
    "        AND bbox_height >= 184\n",
    "        AND tracking_id IS NOT NULL\n",
    ")\n",
    "SELECT\n",
    "    tracking_frame_feature_id\n",
    "FROM ranked_features\n",
    "WHERE rn <= {sample}\n",
    "\"\"\")\n",
    "\n",
    "# faster test query\n",
    "# query = text(\n",
    "#     \"\"\"SELECT tracking_frame_feature_id\n",
    "# FROM tracking_frame_feature\n",
    "# WHERE feature_type = 'face_45' AND bbox_width >= 184 AND bbox_height >= 184 AND tracking_id IS NOT NULL LIMIT 10\"\"\"\n",
    "# )\n",
    "\n",
    "\n",
    "# engine = create_engine(GorillaDatasetKISZ.DB_URI)\n",
    "\n",
    "# # stmt = select(TrackingFrameFeature).where(TrackingFrameFeature.tracking_frame_feature_id.in_(subquery))\n",
    "\n",
    "# with Session(engine) as session:\n",
    "#     result = session.execute(query).scalars().all()\n",
    "#     for row in result:\n",
    "#         print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import List, Literal, Optional, Tuple\n",
    "\n",
    "from PIL import Image\n",
    "from torch import Tensor\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "import gorillatracker.type_helper as gtypes\n",
    "from gorillatracker.transform_utils import SquarePad\n",
    "from gorillatracker.type_helper import Id, Label\n",
    "from gorillatracker.utils.labelencoder import LabelEncoder\n",
    "\n",
    "# devcontainer.mount add \"source=/mnt/vast-gorilla/cropped-images,target=/workspaces/gorillatracker/cropped-images,type=bind,ro\"\n",
    "base_path = \"../cropped-images/2024-04-18\"\n",
    "\n",
    "\n",
    "def cast_label_to_int(labels: List[str]) -> List[int]:\n",
    "    return LabelEncoder.encode_list(labels)\n",
    "\n",
    "\n",
    "class HackDataset(Dataset[Tuple[Id, Tensor, Label]]):\n",
    "    def get_tffs(self) -> list[TrackingFrameFeature]:\n",
    "        engine = create_engine(GorillaDatasetKISZ.DB_URI)\n",
    "\n",
    "        stmt = select(TrackingFrameFeature).where(TrackingFrameFeature.tracking_frame_feature_id.in_(query))\n",
    "\n",
    "        with Session(engine) as session:\n",
    "            return session.execute(stmt).scalars().all()\n",
    "\n",
    "    def __init__(\n",
    "        self, data_dir: str, partition: Literal[\"train\", \"val\", \"test\"], transform: Optional[gtypes.Transform] = None\n",
    "    ):\n",
    "        self.ttfs = self.get_tffs()\n",
    "        self.transform = transform\n",
    "        self.partition = partition\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.ttfs)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[Id, Tensor, Label]:\n",
    "        \"\"\"tracklets will be labels for now\"\"\"\n",
    "        ttf = self.ttfs[idx]\n",
    "\n",
    "        img = Image.open(ttf.cache_path(base_path))\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        assert ttf.tracking_id is not None\n",
    "        return ttf.tracking_frame_feature_id, img, ttf.tracking_id\n",
    "\n",
    "    @classmethod\n",
    "    def get_transforms(cls) -> gtypes.Transform:\n",
    "        return transforms.Compose(\n",
    "            [\n",
    "                SquarePad(),\n",
    "                # Uniform input, you may choose higher/lower sizes.\n",
    "                transforms.Resize(224),\n",
    "                transforms.ToTensor(),\n",
    "            ]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from gorillatracker.utils.embedding_generator import (\n",
    "    get_run,\n",
    "    get_model_from_run,\n",
    "    generate_embeddings,\n",
    "    read_embeddings_from_disk,\n",
    ")\n",
    "from torchvision import transforms\n",
    "\n",
    "regen = False\n",
    "file = \"example.pkl\"\n",
    "\n",
    "if regen:\n",
    "    run_url = \"https://wandb.ai/gorillas/Embedding-SwinV2Large-CXL-Open/runs/bp5e1rnx/workspace?nw=nwuserliamvdv\"\n",
    "    run = get_run(run_url)\n",
    "    model = get_model_from_run(run)\n",
    "    dataset = HackDataset(\"\", \"val\", transforms.Compose([HackDataset.get_transforms(), model.get_tensor_transforms()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "if regen:\n",
    "    model = get_model_from_run(run)\n",
    "    df = generate_embeddings(model, dataset, device=\"cuda\", batch_size=2600, worker=20)\n",
    "    df[\"partition\"] = \"val\"\n",
    "    df.to_pickle(file)\n",
    "else:\n",
    "    df = read_embeddings_from_disk(\"example.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"label\"].value_counts().plot(kind=\"hist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract embeddings and labels\n",
    "embeddings = np.vstack(df[\"embedding\"].values)\n",
    "labels = df[\"label\"].values\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "\n",
    "\n",
    "# Define the range of k values to test\n",
    "k_values = [5000, 10000]  # , 5000, 10000, 15000, 20000]\n",
    "\n",
    "# random.seed(42)\n",
    "# # downsample\n",
    "# combined = list(zip(embeddings, labels))\n",
    "# # Randomly sample elements\n",
    "# random_sample = random.sample(combined, len(combined) // 10)\n",
    "# # Separate the embeddings and labels\n",
    "# sampled_embeddings, sampled_labels = zip(*random_sample)\n",
    "\n",
    "\n",
    "# Initialize lists to store the evaluation metrics\n",
    "silhouette_scores = []\n",
    "davies_bouldin_scores = []\n",
    "\n",
    "# Evaluate K-Means with different k values using k-means++ initialization\n",
    "for k in k_values:\n",
    "    print(f\"Running K-Means++ with k={k}\")\n",
    "    kmeans = KMeans(n_clusters=k, init=\"k-means++\", random_state=42)\n",
    "    labels = kmeans.fit_predict(embeddings, labels)\n",
    "\n",
    "    silhouette_avg = silhouette_score(embeddings, labels)\n",
    "    davies_bouldin_avg = davies_bouldin_score(embeddings, labels)\n",
    "\n",
    "    silhouette_scores.append(silhouette_avg)\n",
    "    davies_bouldin_scores.append(davies_bouldin_avg)\n",
    "\n",
    "# Plot the evaluation metrics\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "color = \"tab:blue\"\n",
    "ax1.set_xlabel(\"Number of clusters (k)\")\n",
    "ax1.set_ylabel(\"Silhouette Score\", color=color)\n",
    "ax1.plot(k_values, silhouette_scores, \"o-\", color=color)\n",
    "ax1.tick_params(axis=\"y\", labelcolor=color)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "color = \"tab:red\"\n",
    "ax2.set_ylabel(\"Davies-Bouldin Score\", color=color)\n",
    "ax2.plot(k_values, davies_bouldin_scores, \"s-\", color=color)\n",
    "ax2.tick_params(axis=\"y\", labelcolor=color)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.title(\"Evaluation of K-Means Clustering with k-means++ Initialization\")\n",
    "plt.show()\n",
    "\n",
    "# Find the optimal number of clusters\n",
    "optimal_k_silhouette = k_values[np.argmax(silhouette_scores)]\n",
    "optimal_k_davies_bouldin = k_values[np.argmin(davies_bouldin_scores)]\n",
    "print(f\"Optimal number of clusters based on silhouette score: {optimal_k_silhouette}\")\n",
    "print(f\"Optimal number of clusters based on Davies-Bouldin score: {optimal_k_davies_bouldin}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Example data\n",
    "X = embeddings \n",
    "\n",
    "ks = [2000, 3000, 4000, 5000, 6000, 7000, 10000, 15000, 20000]\n",
    "for k in ks:\n",
    "    # Create and fit the AgglomerativeClustering model on the reduced data\n",
    "    agg_clustering = AgglomerativeClustering(n_clusters=1000, linkage=\"ward\")\n",
    "    agg_clustering.fit(X)\n",
    "\n",
    "    # Get the cluster labels\n",
    "    agg_labels = agg_clustering.labels_\n",
    "\n",
    "    # Calculate silhouette score and Davies-Bouldin score\n",
    "    sil_score = silhouette_score(X, agg_labels)\n",
    "    db_score = davies_bouldin_score(X, agg_labels)\n",
    "\n",
    "    # Print the scores\n",
    "    print(f\"Silhouette Score: {sil_score}\")\n",
    "    print(f\"Davies-Bouldin Score: {db_score}\")\n",
    "\n",
    "    # Plot the PCA results\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # PCA scatter plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.scatter(X[:, 0], X[:, 1], cmap=\"viridis\")\n",
    "    plt.title(\"PCA of the dataset\")\n",
    "    plt.xlabel(\"Principal Component 1\")\n",
    "    plt.ylabel(\"Principal Component 2\")\n",
    "    plt.colorbar(label=\"Sample Index\")\n",
    "\n",
    "    # Plot the clustering results\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=agg_labels, cmap=\"viridis\")\n",
    "    plt.title(\"Agglomerative Clustering Results\")\n",
    "    plt.xlabel(\"Principal Component 1\")\n",
    "    plt.ylabel(\"Principal Component 2\")\n",
    "    plt.colorbar(label=\"Cluster Label\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
