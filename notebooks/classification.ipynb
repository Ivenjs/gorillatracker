{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from gorillatracker.datasets.cxl import CXLDataset\n",
    "from gorillatracker.model import EfficientNetV2Wrapper\n",
    "from gorillatracker.transform_utils import SquarePad\n",
    "import pandas as pd\n",
    "import wandb\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "wandb.login()\n",
    "wandb.init(mode=\"disabled\")\n",
    "api = wandb.Api()\n",
    "\n",
    "artifact = api.artifact(\n",
    "    \"gorillas/Embedding-ALL-SPAC-Open/model-3ag1c2vf:v1\",  # your artifact name\n",
    "    type=\"model\",\n",
    ")\n",
    "artifact_dir = artifact.download()\n",
    "model = artifact_dir + \"/model.ckpt\"\n",
    "\n",
    "# load model\n",
    "checkpoint = torch.load(model, map_location=torch.device(\"cpu\"))\n",
    "\n",
    "model = EfficientNetV2Wrapper(  # switch this with the model you want to use\n",
    "    model_name_or_path=\"EfficientNetV2_Large\",\n",
    "    from_scratch=False,\n",
    "    loss_mode=\"softmax/arcface\",\n",
    "    weight_decay=0.001,\n",
    "    lr_schedule=\"cosine\",\n",
    "    warmup_mode=\"cosine\",\n",
    "    warmup_epochs=10,\n",
    "    max_epochs=100,\n",
    "    initial_lr=0.01,\n",
    "    start_lr=0.01,\n",
    "    end_lr=0.0001,\n",
    "    beta1=0.9,\n",
    "    beta2=0.999,\n",
    "    embedding_size=128,\n",
    ")\n",
    "# the following lines are necessary to load a model that was trained with arcface (the prototypes are saved in the state dict)\n",
    "model.loss_module_train.prototypes = torch.nn.Parameter(checkpoint[\"state_dict\"][\"loss_module_train.prototypes\"])\n",
    "model.loss_module_val.prototypes = torch.nn.Parameter(checkpoint[\"state_dict\"][\"loss_module_val.prototypes\"])\n",
    "\n",
    "model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "model.eval()\n",
    "\n",
    "# generate table that contains labels and images and embeddings\n",
    "df = pd.DataFrame(columns=[\"label\", \"image\", \"embedding\"])\n",
    "dataset = CXLDataset(\n",
    "    data_dir=\"/workspaces/gorillatracker/data/splits/ground_truth-cxl-face_images-openset-reid-val-0-test-0-mintraincount-3-seed-42-train-50-val-25-test-25\",\n",
    "    partition=\"val\",\n",
    "    transform=transforms.Compose(  # use the transforms that were used for the model (except of course data augmentations)\n",
    "        [\n",
    "            SquarePad(),\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "\n",
    "for i in range(len(dataset)):\n",
    "    image_tensor, label = dataset[i]\n",
    "    label_string = dataset.mapping[label]\n",
    "    image = transforms.ToPILImage()(image_tensor)\n",
    "    image_tensor = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.228, 0.224, 0.225])(\n",
    "        image_tensor\n",
    "    )  # if your model was trained with normalization, you need to normalize the images here as well\n",
    "    embedding = model(image_tensor.unsqueeze(0))\n",
    "    df = pd.concat(\n",
    "        [\n",
    "            df,\n",
    "            pd.DataFrame(\n",
    "                {\n",
    "                    \"label_string\": [label_string],\n",
    "                    \"label\": [label],\n",
    "                    \"image\": [image],\n",
    "                    \"embedding\": [embedding[0].detach().numpy()],\n",
    "                }\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        print(f\"\\rprocessed {i} images\")\n",
    "df = df.reset_index(drop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gorillatracker.metrics import knn\n",
    "import numpy as np\n",
    "\n",
    "df.label = df.label.astype(int)\n",
    "df.embedding = df.embedding.apply(lambda x: np.array(x, dtype=np.float32))\n",
    "d = knn(df.embedding, df.label.to_numpy(), k=1)\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn(df.embedding, df.label.to_numpy(), k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"label\"].value_counts().plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's filter out the individuals that have less than 3 images\n",
    "min3labels = df[\"label\"].value_counts()[df[\"label\"].value_counts() >= 3].index\n",
    "min3df = df[df[\"label\"].isin(min3labels)]\n",
    "min3df.reset_index(drop=True, inplace=True)\n",
    "min3df[\"label\"].value_counts().plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn(torch.tensor(min3df.embedding), min3df.label.to_numpy(), k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn(torch.tensor(min3df.embedding), min3df.label.to_numpy(), k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Realworld\n",
    "In a real world context, we'll see new individuals arriving over time. \n",
    "\n",
    "Options:\n",
    "- on centroids (of known classes and of new individuals)\n",
    "- with boolean filter (individuals seen at the same time)\n",
    "\n",
    "\n",
    "Relevant Metrics:\n",
    "- What is the average/min/max distance within images of an individual? \n",
    "- What is the average distance between centroids of individuals?\n",
    "- What is the change in average distance between centroids of faces for increasing margins (0.5, 0.1. 1.5, 2, 4, 8)?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll now operate on the min 3 images model:\n",
    "\n",
    "grouped = df.groupby([\"label\", \"label_string\"])[\"embedding\"].apply(lambda x: np.mean(np.vstack(x), axis=0))\n",
    "centroid_df = pd.DataFrame({\"centroid\": grouped.values})\n",
    "centroid_df[[\"label\", \"label_string\"]] = pd.DataFrame(grouped.index.tolist(), index=centroid_df.index)\n",
    "assert len(centroid_df['label'].unique()) == len(centroid_df['label_string'].unique()), \"Label does not have a 1:1 mapping with label_string\"\n",
    "centroid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "for label in centroid_df['label']:\n",
    "    centroid = centroid_df[centroid_df['label'] == label]['centroid'].values[0]\n",
    "    embeddings = df[df['label'] == label]['embedding'].tolist()\n",
    "    distances = cdist(embeddings, [centroid])\n",
    "    min_distance = np.min(distances)\n",
    "    max_distance = np.max(distances)\n",
    "    avg_distance = np.mean(distances)\n",
    "    centroid_df.loc[centroid_df['label'] == label, 'min_distance'] = min_distance\n",
    "    centroid_df.loc[centroid_df['label'] == label, 'max_distance'] = max_distance\n",
    "    centroid_df.loc[centroid_df['label'] == label, 'avg_distance'] = avg_distance\n",
    "\n",
    "centroid_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "# Compute the pairwise distances between centroids\n",
    "distances = cdist(centroid_df['centroid'].tolist(), centroid_df['centroid'].tolist())\n",
    "\n",
    "# Compute the average distance between classes\n",
    "avg_distance = np.mean(distances)\n",
    "\n",
    "avg_distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "from gorillatracker.metrics import tsne\n",
    "\n",
    "centroid_marker = 1000000\n",
    "# p = tsne(torch.tensor(centroid_df.centroid.tolist()), torch.tensor(centroid_df.label.tolist()), perplexity=min(30, len(centroid_df)-1))\n",
    "p = tsne(torch.tensor(df.embedding.tolist() + centroid_df.centroid.tolist()), torch.tensor(df.label.tolist() + [centroid_marker + c for c in centroid_df.label.tolist()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "k_means = KMeans(n_clusters=centroid_df.label.nunique(), random_state=42)\n",
    "outputs = k_means.fit_predict(embeddings)\n",
    "k_means.cluster_centers_, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add all train embeddings\n",
    "# Then add embeddings from validation and check how close they are to the centroids\n",
    "# \n",
    "# Check how many images we have per individual."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
