{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParseResult(scheme='https', netloc='wandb.ai', path='/gorillas/Embedding-SwinV2Large-CXL-Open/runs/cey2y5yp/workspace', params='', query='nw=nwuseremirhan404', fragment='') ['', 'gorillas', 'Embedding-SwinV2Large-CXL-Open', 'runs', 'cey2y5yp', 'workspace'] /gorillas/Embedding-SwinV2Large-CXL-Open/runs/cey2y5yp/workspace\n",
      "Using model from run: 999-need-a-model-2024-06-07-13-26-00\n",
      "Config: {'s': 64, 'seed': 42, 'beta1': 0.9, 'beta2': 0.999, 'debug': False, 'kfold': False, 'end_lr': 1e-07, 'margin': 1, 'resume': False, 'compile': False, 'delta_t': 50, 'epsilon': 1e-07, 'kfold_k': None, 'l2_beta': 0.01, 'offline': False, 'plugins': None, 'use_ssl': False, 'workers': 4, 'data_dir': '/workspaces/gorillatracker/data/splits/ground_truth-cxl-face_images-openset-reid-val-0-test-0-mintraincount-3-seed-42-train-50-val-25-test-25', 'l2_alpha': 0.1, 'only_val': False, 'profiler': None, 'run_name': '999-need-a-model', 'start_lr': 1e-05, 'dropout_p': 0.32, 'grad_clip': 1, 'loss_mode': 'online/soft', 'min_delta': 0.01, 'n_samples': 15, 'precision': 32, 'batch_size': 8, 'initial_lr': 1e-05, 'max_epochs': 5, 'split_path': None, 'wandb_tags': [], 'accelerator': 'cuda', 'lr_interval': 1, 'lr_schedule': 'linear', 'num_classes': [59, 29, 29], 'num_devices': 1, 'warmup_mode': 'cosine', 'width_range': [None, None], 'fast_dev_run': False, 'from_scratch': False, 'height_range': [None, None], 'k_subcenters': 2, 'project_name': 'Embedding-SwinV2Large-CXL-Open', 'weight_decay': 1, 'dataset_class': 'gorillatracker.datasets.cxl.CXLDataset', 'feature_types': ['body'], 'save_interval': 50, 'tff_selection': 'equidistant', 'use_dist_term': False, 'warmup_epochs': 0, 'embedding_size': 256, 'knn_with_train': True, 'lambda_membank': 0.5, 'min_confidence': 0.5, 'use_focal_loss': False, 'label_smoothing': 0.05, 'negative_mining': 'random', 'use_wildme_model': False, 'stepwise_schedule': False, 'use_class_weights': False, 'class_distribution': [{'0': 19, '1': 8, '2': 20, '3': 16, '4': 50, '5': 20, '6': 42, '7': 11, '8': 3, '9': 12, '10': 2, '11': 14, '12': 10, '13': 9, '14': 9, '15': 8, '16': 25, '17': 41, '18': 6, '19': 5, '20': 15, '21': 27, '22': 12, '23': 13, '24': 36, '25': 5, '26': 3, '27': 11, '28': 6, '29': 7, '30': 4, '31': 22, '32': 6, '33': 3, '34': 11, '35': 9, '36': 6, '37': 5, '38': 4, '39': 11, '40': 12, '41': 10, '42': 9, '43': 4, '44': 6, '45': 4, '46': 5, '47': 7, '48': 3, '49': 5, '50': 3, '51': 6, '52': 4, '53': 4, '54': 2, '55': 10, '56': 9, '57': 1, '58': 3}, {'59': 8, '60': 2, '61': 7, '62': 6, '63': 4, '64': 12, '65': 6, '66': 4, '67': 4, '68': 5, '69': 4, '70': 4, '71': 4, '72': 3, '73': 4, '74': 3, '75': 4, '76': 8, '77': 3, '78': 6, '79': 3, '80': 4, '81': 4, '82': 3, '83': 3, '84': 2, '85': 3, '86': 2, '87': 1}, {'88': 3, '89': 6, '90': 2, '91': 4, '92': 4, '93': 5, '94': 4, '95': 5, '96': 4, '97': 2, '98': 2, '99': 4, '100': 3, '101': 3, '102': 5, '103': 4, '104': 2, '105': 3, '106': 1, '107': 3, '108': 4, '109': 1, '110': 2, '111': 4, '112': 1, '113': 1, '114': 2, '115': 1, '116': 1}], 'model_name_or_path': 'SwinV2LargeWrapper', 'val_check_interval': 1, 'force_deterministic': True, 'num_val_dataloaders': 1, 'save_model_to_wandb': True, 'val_before_training': True, 'distributed_strategy': 'auto', 'mem_bank_start_epoch': 10, 'data_resize_transform': 192, 'saved_checkpoint_path': None, 'check_val_every_n_epoch': 10, 'data_preprocessing_only': False, 'early_stopping_patience': 50, 'embedding_save_interval': 50, 'min_images_per_tracking': 3, 'additional_val_data_dirs': None, 'teacher_model_wandb_link': '', 'path_to_pretrained_weights': 'pretrained_weights/swinv2_large.pth', 'gradient_accumulation_steps': 4, 'additional_val_dataset_classes': None, 'use_quantization_aware_training': False}\n",
      "Loading model from latest checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact model-cey2y5yp:v0, 2238.97MB. 1 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n",
      "Done. 0:0:1.6\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m transforms\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgorillatracker\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membedding_generator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_model_for_run_url\n\u001b[0;32m----> 7\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mget_model_for_run_url\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhttps://wandb.ai/gorillas/Embedding-SwinV2Large-CXL-Open/runs/cey2y5yp/workspace?nw=nwuseremirhan404\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspaces/gorillatracker/src/gorillatracker/utils/embedding_generator.py:134\u001b[0m, in \u001b[0;36mget_model_for_run_url\u001b[0;34m(run_url, eval_mode)\u001b[0m\n\u001b[1;32m    132\u001b[0m model_path \u001b[38;5;241m=\u001b[39m get_latest_model_checkpoint(run)\u001b[38;5;241m.\u001b[39mqualified_name\n\u001b[1;32m    133\u001b[0m model_cls \u001b[38;5;241m=\u001b[39m get_model_cls(args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_name_or_path\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m--> 134\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_model_from_wandb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_cls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_mode\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspaces/gorillatracker/src/gorillatracker/utils/embedding_generator.py:72\u001b[0m, in \u001b[0;36mload_model_from_wandb\u001b[0;34m(wandb_fullname, model_cls, model_config, device, eval_mode)\u001b[0m\n\u001b[1;32m     69\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(model, map_location\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     70\u001b[0m model_state_dict \u001b[38;5;241m=\u001b[39m checkpoint[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstate_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 72\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_cls\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss_module_train.prototypes\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m model_state_dict \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss_module_val.prototypes\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m model_state_dict\n\u001b[1;32m     76\u001b[0m ):  \u001b[38;5;66;03m# necessary because arcface loss also saves prototypes\u001b[39;00m\n\u001b[1;32m     77\u001b[0m     model\u001b[38;5;241m.\u001b[39mloss_module_train\u001b[38;5;241m.\u001b[39mprototypes \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mParameter(model_state_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss_module_train.prototypes\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m/workspaces/gorillatracker/src/gorillatracker/model.py:925\u001b[0m, in \u001b[0;36mSwinV2LargeWrapper.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    916\u001b[0m \u001b[38;5;66;03m# self.model.head.fc = torch.nn.Linear(\u001b[39;00m\n\u001b[1;32m    917\u001b[0m \u001b[38;5;66;03m#     in_features=self.model.head.fc.in_features, out_features=self.embedding_size\u001b[39;00m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;66;03m# ) # TODO\u001b[39;00m\n\u001b[1;32m    919\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mhead\u001b[38;5;241m.\u001b[39mfc \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mSequential(\n\u001b[1;32m    920\u001b[0m     torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mBatchNorm1d(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mhead\u001b[38;5;241m.\u001b[39mfc\u001b[38;5;241m.\u001b[39min_features),\n\u001b[1;32m    921\u001b[0m     torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mDropout(p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout_p),\n\u001b[1;32m    922\u001b[0m     torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mLinear(in_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mhead\u001b[38;5;241m.\u001b[39mfc\u001b[38;5;241m.\u001b[39min_features, out_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_size),\n\u001b[1;32m    923\u001b[0m     torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mBatchNorm1d(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_size),\n\u001b[1;32m    924\u001b[0m )\n\u001b[0;32m--> 925\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_losses\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspaces/gorillatracker/src/gorillatracker/model.py:215\u001b[0m, in \u001b[0;36mBaseModule.set_losses\u001b[0;34m(self, model, loss_mode, margin, s, delta_t, mem_bank_start_epoch, lambda_membank, embedding_size, batch_size, num_classes, class_distribution, use_focal_loss, k_subcenters, accelerator, label_smoothing, l2_alpha, l2_beta, path_to_pretrained_weights, use_class_weights, use_dist_term, **kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mset_losses\u001b[39m(\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    183\u001b[0m     model: nn\u001b[38;5;241m.\u001b[39mModule,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Dict[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[1;32m    204\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m     kfold_prefix \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfold-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkfold_k\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkfold_k \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_module_train \u001b[38;5;241m=\u001b[39m get_loss(\n\u001b[1;32m    208\u001b[0m         loss_mode,\n\u001b[1;32m    209\u001b[0m         margin\u001b[38;5;241m=\u001b[39mmargin,\n\u001b[1;32m    210\u001b[0m         embedding_size\u001b[38;5;241m=\u001b[39membedding_size,\n\u001b[1;32m    211\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m    212\u001b[0m         delta_t\u001b[38;5;241m=\u001b[39mdelta_t,\n\u001b[1;32m    213\u001b[0m         s\u001b[38;5;241m=\u001b[39ms,\n\u001b[1;32m    214\u001b[0m         num_classes\u001b[38;5;241m=\u001b[39mnum_classes[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m--> 215\u001b[0m         class_distribution\u001b[38;5;241m=\u001b[39m\u001b[43mclass_distribution\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m,\n\u001b[1;32m    216\u001b[0m         mem_bank_start_epoch\u001b[38;5;241m=\u001b[39mmem_bank_start_epoch,\n\u001b[1;32m    217\u001b[0m         lambda_membank\u001b[38;5;241m=\u001b[39mlambda_membank,\n\u001b[1;32m    218\u001b[0m         accelerator\u001b[38;5;241m=\u001b[39maccelerator,\n\u001b[1;32m    219\u001b[0m         l2_alpha\u001b[38;5;241m=\u001b[39ml2_alpha,\n\u001b[1;32m    220\u001b[0m         l2_beta\u001b[38;5;241m=\u001b[39ml2_beta,\n\u001b[1;32m    221\u001b[0m         path_to_pretrained_weights\u001b[38;5;241m=\u001b[39mpath_to_pretrained_weights,\n\u001b[1;32m    222\u001b[0m         use_focal_loss\u001b[38;5;241m=\u001b[39muse_focal_loss,\n\u001b[1;32m    223\u001b[0m         label_smoothing\u001b[38;5;241m=\u001b[39mlabel_smoothing,\n\u001b[1;32m    224\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    225\u001b[0m         k_subcenters\u001b[38;5;241m=\u001b[39mk_subcenters,\n\u001b[1;32m    226\u001b[0m         use_class_weights\u001b[38;5;241m=\u001b[39muse_class_weights,\n\u001b[1;32m    227\u001b[0m         use_dist_term\u001b[38;5;241m=\u001b[39muse_dist_term,\n\u001b[1;32m    228\u001b[0m         \u001b[38;5;66;03m# log_func=lambda x, y: self.log(\"train/\"+ x, y, on_epoch=True),\u001b[39;00m\n\u001b[1;32m    229\u001b[0m         log_func\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x, y: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkfold_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, y),\n\u001b[1;32m    230\u001b[0m         teacher_model_wandb_link\u001b[38;5;241m=\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mteacher_model_wandb_link\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    231\u001b[0m     )\n\u001b[1;32m    232\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_module_val \u001b[38;5;241m=\u001b[39m get_loss(\n\u001b[1;32m    233\u001b[0m         loss_mode,\n\u001b[1;32m    234\u001b[0m         margin\u001b[38;5;241m=\u001b[39mmargin,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    254\u001b[0m         teacher_model_wand_link\u001b[38;5;241m=\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mteacher_model_wandb_link\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    255\u001b[0m     )\n\u001b[1;32m    256\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_module_val\u001b[38;5;241m.\u001b[39meval()\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "import torch\n",
    "from gorillatracker.model import EfficientNetV2Wrapper, SwinV2BaseWrapper\n",
    "from torchvision.transforms import transforms\n",
    "from gorillatracker.utils.embedding_generator import get_model_for_run_url\n",
    "\n",
    "model = get_model_for_run_url(\"https://wandb.ai/gorillas/Embedding-SwinV2Large-CXL-Open/runs/cey2y5yp/workspace?nw=nwuseremirhan404\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Video (and look at all the gorilla ids which contain face images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspaces/gorillatracker/video_data/M002_20220725_012.mp4\n",
      "Number of gorillas with tracked faces: 7\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from gorillatracker.utils.video_models import VideoClip, _parse_tracked_video_clip\n",
    "from datetime import datetime\n",
    "from PIL import Image\n",
    "\n",
    "json_path = \"/workspaces/gorillatracker/data/derived_data/spac_gorillas_converted_labels_tracked/M002_20220725_012_tracked.json\"\n",
    "mp4_path = \"/workspaces/gorillatracker/video_data\" + json_path.split(\"spac_gorillas_converted_labels_tracked\")[1].replace(\"_tracked.json\", \".mp4\")\n",
    "print(mp4_path)\n",
    "v = VideoClip(video_id=\"\", camera_id=\"\", start_time=datetime.now())\n",
    "v = _parse_tracked_video_clip(v, json_path)\n",
    "video = cv2.VideoCapture(mp4_path)\n",
    "\n",
    "counter = 0\n",
    "frame_dict = {}\n",
    "for gorilla in v.trackings:\n",
    "    if len(gorilla.bounding_boxes_face) > 0:\n",
    "        for frame in gorilla.bounding_boxes_face:\n",
    "            try:\n",
    "                frame_dict[frame.f].append((frame, gorilla.individual_id))\n",
    "            except KeyError:\n",
    "                frame_dict[frame.f] = [(frame, gorilla.individual_id)]\n",
    "        counter+=1\n",
    "print(f\"Number of gorillas with tracked faces: {counter}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Embedding Projector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import Isomap, LocallyLinearEmbedding, MDS, SpectralEmbedding, TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import umap.umap_ as umap\n",
    "\n",
    "class EmbeddingProjector:\n",
    "    def __init__(self):\n",
    "        self.algorithms = {\n",
    "            \"tsne\": TSNE(n_components=2),\n",
    "            \"isomap\": Isomap(n_components=2),\n",
    "            \"lle\": LocallyLinearEmbedding(n_components=2),\n",
    "            \"mds\": MDS(n_components=2),\n",
    "            \"spectral\": SpectralEmbedding(n_components=2),\n",
    "            \"pca\": PCA(n_components=2),\n",
    "            \"umap\": umap.UMAP(),\n",
    "        }\n",
    "\n",
    "    def reduce_dimensions(self, embeddings, method=\"tsne\"):\n",
    "        algorithm = self.algorithms.get(method, TSNE(n_components=2))\n",
    "        return algorithm.fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract face images and embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import colorcet as cc\n",
    "\n",
    "def hex_to_rgb(hex):\n",
    "    hex = hex.replace(\"#\", \"\")\n",
    "    return tuple(int(hex[i:i+2], 16) for i in (0, 2, 4))\n",
    "\n",
    "colors = cc.glasbey\n",
    "embedding_dict = {}\n",
    "images = []\n",
    "\n",
    "for f in frame_dict.keys():\n",
    "    video.set(cv2.CAP_PROP_POS_FRAMES, f)\n",
    "    ret, img = video.read()\n",
    "    imgcopy = img.copy()\n",
    "    for frame_gorilla in frame_dict[f]:\n",
    "        cropped_img = Image.fromarray(imgcopy).crop(frame_gorilla[0].bb[0] + frame_gorilla[0].bb[1])\n",
    "        cropped_img = transform(cropped_img).unsqueeze(0)\n",
    "        embedding = model(cropped_img).detach()\n",
    "        try:\n",
    "            embedding_dict[f].append((embedding, frame_gorilla[1]))\n",
    "        except KeyError:\n",
    "            embedding_dict[f] = [(embedding, frame_gorilla[1])]\n",
    "        img = cv2.rectangle(img, frame_gorilla[0].bb[0], frame_gorilla[0].bb[1], hex_to_rgb(colors[frame_gorilla[1]]), 2)\n",
    "    images.append(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot each embedding with a slider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list.append() takes exactly one argument (2 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m embedding_dict\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m      8\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m [pair[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m pair \u001b[38;5;129;01min\u001b[39;00m embedding_dict[f]]\n\u001b[0;32m----> 9\u001b[0m     \u001b[43mall_embeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m low_dim_embeddings \u001b[38;5;241m=\u001b[39m EmbeddingProjector()\u001b[38;5;241m.\u001b[39mreduce_dimensions(torch\u001b[38;5;241m.\u001b[39mcat(all_embeddings)\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy(),\n\u001b[1;32m     12\u001b[0m                                                             method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpca\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m     13\u001b[0m min_x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m([pair[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m pair \u001b[38;5;129;01min\u001b[39;00m low_dim_embeddings])\n",
      "\u001b[0;31mTypeError\u001b[0m: list.append() takes exactly one argument (2 given)"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "import copy\n",
    "from io import BytesIO\n",
    "\n",
    "all_embeddings = []\n",
    "for f in embedding_dict.keys():\n",
    "    embeddings = [pair[0] for pair in embedding_dict[f]]\n",
    "    all_embeddings.append(*embeddings)\n",
    "\n",
    "low_dim_embeddings = EmbeddingProjector().reduce_dimensions(torch.cat(all_embeddings).detach().numpy(),\n",
    "                                                            method=\"pca\").tolist()\n",
    "min_x = min([pair[0] for pair in low_dim_embeddings])\n",
    "max_x = max([pair[0] for pair in low_dim_embeddings])\n",
    "min_y = min([pair[1] for pair in low_dim_embeddings])\n",
    "max_y = max([pair[1] for pair in low_dim_embeddings])\n",
    "\n",
    "low_dim_embedding_dict = copy.deepcopy(embedding_dict)\n",
    "for f in embedding_dict.keys():\n",
    "    for i, pair in enumerate(embedding_dict[f]):\n",
    "        low_dim_embedding_dict[f][i] = (low_dim_embeddings.pop(0), pair[1])\n",
    "\n",
    "plot_list = []\n",
    "\n",
    "for f in low_dim_embedding_dict.keys():\n",
    "    plt.xlim(min_x - 1, max_x + 1)\n",
    "    plt.ylim(min_y - 1, max_y + 1)\n",
    "    plt.grid(True)\n",
    "    for pair in low_dim_embedding_dict[f]:\n",
    "        plt.plot(pair[0][0], pair[0][1], marker='+', linestyle='None', markersize=10, color=colors[pair[1]])\n",
    "\n",
    "    plt.title(f\"Frame {f}\")\n",
    "    buffer = BytesIO()\n",
    "    plt.savefig(buffer, format='png')\n",
    "    buffer.seek(0)\n",
    "    pil_image = Image.open(buffer)\n",
    "    plot_list.append(pil_image)\n",
    "    plt.close()\n",
    "\n",
    "images_per_page = 2\n",
    "\n",
    "def display_images(page):\n",
    "    start = page\n",
    "    print(start)\n",
    "    fig, axs = plt.subplots(1, images_per_page, figsize=(15, 5))  # Create subplots\n",
    "    for i in range(images_per_page):\n",
    "        if start + i < len(images) + len(plot_list):\n",
    "            if i == 0:\n",
    "                axs[i].imshow(images[start])\n",
    "            else:\n",
    "                axs[i].imshow(plot_list[start])\n",
    "            axs[i].axis('off')\n",
    "        else:\n",
    "            axs[i].axis('off')  # Hide axes for empty subplots\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "page_selector = widgets.IntSlider(min=0, max=(len(images) + len(plot_list) - 1) // images_per_page, description='Page:')\n",
    "widgets.interact(display_images, page=page_selector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7433cd3cc6046ee94b93c0f436ca7b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='Page:', max=36), Output()), _dom_classes=('widget-intera…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.display_images(page)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from io import BytesIO\n",
    "\n",
    "low_dim_embeddings = EmbeddingProjector().reduce_dimensions(torch.cat(embeddings).detach().numpy(),\n",
    "                                                            method=\"pca\")\n",
    "low_dim_embeddings = low_dim_embeddings.tolist()\n",
    "\n",
    "x_axis, y_axis = zip(*low_dim_embeddings)\n",
    "plot_list = []\n",
    "for embedding in low_dim_embeddings:\n",
    "    plt.xlim(min(x_axis) - 1, max(x_axis) + 1)\n",
    "    plt.ylim(min(y_axis) - 1, max(y_axis) + 1)\n",
    "    plt.plot(embedding[0], embedding[1], marker='+', linestyle='None', markersize=10, color='blue')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    buffer = BytesIO()\n",
    "    plt.savefig(buffer, format='png')\n",
    "    buffer.seek(0)\n",
    "    pil_image = Image.open(buffer)\n",
    "    plot_list.append(pil_image)\n",
    "    plt.close()\n",
    "\n",
    "images_per_page = 2\n",
    "\n",
    "def display_images(page):\n",
    "    start = page\n",
    "    print(start)\n",
    "    fig, axs = plt.subplots(1, images_per_page, figsize=(15, 5))  # Create subplots\n",
    "    for i in range(images_per_page):\n",
    "        if start + i < len(faces) + len(plot_list):\n",
    "            if i == 0:\n",
    "                axs[i].imshow(faces[start])\n",
    "            else:\n",
    "                axs[i].imshow(plot_list[start])\n",
    "            axs[i].axis('off')\n",
    "        else:\n",
    "            axs[i].axis('off')  # Hide axes for empty subplots\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "page_selector = widgets.IntSlider(min=0, max=(len(faces) + len(plot_list) - 1) // images_per_page, description='Page:')\n",
    "widgets.interact(display_images, page=page_selector)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
