{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact model-3ag1c2vf:v1, 1346.85MB. 1 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n",
      "Done. 0:0:2.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "import torch\n",
    "from gorillatracker.model import EfficientNetV2Wrapper\n",
    "from torchvision.transforms import transforms\n",
    "\n",
    "wandb.login()\n",
    "wandb.init(mode=\"disabled\")\n",
    "api = wandb.Api()\n",
    "\n",
    "artifact = api.artifact(\n",
    "    \"gorillas/Embedding-ALL-SPAC-Open/model-3ag1c2vf:v1\",  # your artifact name\n",
    "    type=\"model\",\n",
    ")\n",
    "artifact_dir = artifact.download()\n",
    "model = artifact_dir + \"/model.ckpt\"\n",
    "\n",
    "# load model\n",
    "checkpoint = torch.load(model, map_location=torch.device(\"cpu\"))\n",
    "\n",
    "model = EfficientNetV2Wrapper(  # switch this with the model you want to use\n",
    "    model_name_or_path=\"EfficientNetV2_Large\",\n",
    "    from_scratch=False,\n",
    "    loss_mode=\"softmax/arcface\",\n",
    "    weight_decay=0.001,\n",
    "    lr_schedule=\"cosine\",\n",
    "    warmup_mode=\"cosine\",\n",
    "    warmup_epochs=10,\n",
    "    max_epochs=100,\n",
    "    initial_lr=0.01,\n",
    "    start_lr=0.01,\n",
    "    end_lr=0.0001,\n",
    "    beta1=0.9,\n",
    "    beta2=0.999,\n",
    "    embedding_size=128,\n",
    ")\n",
    "# the following lines are necessary to load a model that was trained with arcface (the prototypes are saved in the state dict)\n",
    "model.loss_module_train.prototypes = torch.nn.Parameter(checkpoint[\"state_dict\"][\"loss_module_train.prototypes\"])\n",
    "model.loss_module_val.prototypes = torch.nn.Parameter(checkpoint[\"state_dict\"][\"loss_module_val.prototypes\"])\n",
    "\n",
    "transform=transforms.Compose(  # use the transforms that were used for the model (except of course data augmentations)\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.228, 0.224, 0.225]), # if your model was trained with normalization, you need to normalize the images here as well\n",
    "        ]\n",
    "    )\n",
    "\n",
    "model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "model.eval()\n",
    "print(\"Model loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Video (and look at all the gorilla ids which contain face images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from gorillatracker.utils.video_models import VideoClip, _parse_tracked_video_clip\n",
    "from datetime import datetime\n",
    "from PIL import Image\n",
    "\n",
    "json_path = \"/workspaces/gorillatracker/data/derived_data/spac_gorillas_converted_labels_tracked/M002_20220529_031_tracked.json\"\n",
    "mp4_path = \"/workspaces/gorillatracker/video_data\" + json_path.split(\"spac_gorillas_converted_labels_tracked\")[1].replace(\"_tracked.json\", \".mp4\")\n",
    "print(mp4_path)\n",
    "v = VideoClip(video_id=\"\", camera_id=\"\", start_time=datetime.now())\n",
    "v = _parse_tracked_video_clip(v, json_path)\n",
    "video = cv2.VideoCapture(mp4_path)\n",
    "\n",
    "for i, _ in enumerate(v.trackings):\n",
    "    gorilla = v.trackings[i]\n",
    "    if len(gorilla.bounding_boxes_face) > 0:\n",
    "        print(i)\n",
    "        print(gorilla.bounding_boxes_face)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract face images and embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspaces/gorillatracker/video_data/M002_20220529_031.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[TrackedFrame(f=13, bb=((1755, 608), (1813, 689)), c=0.5903230905532837), TrackedFrame(f=14, bb=((1755, 606), (1814, 692)), c=0.6314746737480164), TrackedFrame(f=15, bb=((1766, 610), (1824, 694)), c=0.5695499181747437), TrackedFrame(f=16, bb=((1772, 610), (1827, 693)), c=0.7277531027793884), TrackedFrame(f=17, bb=((1775, 609), (1831, 696)), c=0.709379255771637), TrackedFrame(f=18, bb=((1778, 610), (1839, 695)), c=0.7195687890052795), TrackedFrame(f=19, bb=((1778, 609), (1848, 694)), c=0.6800578832626343), TrackedFrame(f=22, bb=((1793, 612), (1865, 695)), c=0.627274751663208), TrackedFrame(f=23, bb=((1800, 612), (1871, 696)), c=0.7736772298812866), TrackedFrame(f=24, bb=((1803, 613), (1874, 697)), c=0.7796608209609985), TrackedFrame(f=25, bb=((1805, 613), (1878, 694)), c=0.7743455171585083), TrackedFrame(f=26, bb=((1808, 617), (1884, 701)), c=0.7462272644042969), TrackedFrame(f=27, bb=((1813, 619), (1890, 699)), c=0.7425493597984314), TrackedFrame(f=28, bb=((1816, 619), (1891, 703)), c=0.751545250415802), TrackedFrame(f=29, bb=((1819, 619), (1895, 703)), c=0.7550487518310547), TrackedFrame(f=30, bb=((1824, 620), (1897, 698)), c=0.7756643891334534), TrackedFrame(f=31, bb=((1825, 619), (1900, 702)), c=0.7463307976722717), TrackedFrame(f=32, bb=((1827, 618), (1903, 703)), c=0.7050273418426514), TrackedFrame(f=33, bb=((1828, 618), (1905, 703)), c=0.6891172528266907), TrackedFrame(f=34, bb=((1831, 617), (1906, 703)), c=0.7078866958618164), TrackedFrame(f=35, bb=((1835, 617), (1910, 705)), c=0.7494577169418335), TrackedFrame(f=36, bb=((1837, 618), (1911, 707)), c=0.7472692131996155), TrackedFrame(f=37, bb=((1841, 619), (1914, 706)), c=0.7334573864936829), TrackedFrame(f=38, bb=((1844, 619), (1916, 703)), c=0.7418296337127686), TrackedFrame(f=39, bb=((1851, 622), (1918, 700)), c=0.6811541318893433), TrackedFrame(f=40, bb=((1853, 622), (1918, 704)), c=0.7226799726486206), TrackedFrame(f=41, bb=((1854, 623), (1919, 708)), c=0.7388505935668945), TrackedFrame(f=42, bb=((1857, 623), (1919, 713)), c=0.7644845843315125), TrackedFrame(f=43, bb=((1861, 623), (1920, 714)), c=0.7426022291183472), TrackedFrame(f=44, bb=((1862, 625), (1919, 716)), c=0.7738450765609741), TrackedFrame(f=45, bb=((1866, 630), (1919, 722)), c=0.7585200071334839), TrackedFrame(f=46, bb=((1866, 628), (1919, 723)), c=0.7446922063827515), TrackedFrame(f=47, bb=((1871, 630), (1919, 722)), c=0.7594582438468933), TrackedFrame(f=48, bb=((1873, 631), (1920, 722)), c=0.7599528431892395), TrackedFrame(f=49, bb=((1874, 633), (1920, 722)), c=0.7090628147125244), TrackedFrame(f=50, bb=((1876, 634), (1920, 728)), c=0.5804612040519714), TrackedFrame(f=51, bb=((1875, 633), (1919, 722)), c=0.5937541723251343)]\n",
      "37\n"
     ]
    }
   ],
   "source": [
    "gorilla = v.trackings[0] # change this to the gorilla you want to extract the embeddings from\n",
    "\n",
    "faces = []\n",
    "embeddings = []\n",
    "for frame in gorilla.bounding_boxes_face:\n",
    "    video.set(cv2.CAP_PROP_POS_FRAMES, frame.f)\n",
    "    ret, img = video.read()\n",
    "    cropped_img = Image.fromarray(img).crop(frame.bb[0] + frame.bb[1])\n",
    "    faces.append(cropped_img)\n",
    "    img = transform(cropped_img)\n",
    "    img = img.unsqueeze(0)\n",
    "    embedding = model(img)\n",
    "    embeddings.append(embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Embedding Projector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import Isomap, LocallyLinearEmbedding, MDS, SpectralEmbedding, TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import umap.umap_ as umap\n",
    "\n",
    "class EmbeddingProjector:\n",
    "    def __init__(self):\n",
    "        self.algorithms = {\n",
    "            \"tsne\": TSNE(n_components=2),\n",
    "            \"isomap\": Isomap(n_components=2),\n",
    "            \"lle\": LocallyLinearEmbedding(n_components=2),\n",
    "            \"mds\": MDS(n_components=2),\n",
    "            \"spectral\": SpectralEmbedding(n_components=2),\n",
    "            \"pca\": PCA(n_components=2),\n",
    "            \"umap\": umap.UMAP(),\n",
    "        }\n",
    "\n",
    "    def reduce_dimensions(self, embeddings, method=\"tsne\"):\n",
    "        algorithm = self.algorithms.get(method, TSNE(n_components=2))\n",
    "        return algorithm.fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot each embedding with a slider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-4.594196796417236, 3.5996522903442383], [-4.751836776733398, 3.549981117248535], [-5.030726432800293, 3.63887095451355], [-5.187254905700684, 3.4718680381774902], [-5.154721736907959, 3.462038278579712], [-5.162607669830322, 3.5103681087493896], [-5.095883846282959, 3.5854573249816895], [-5.92625617980957, 3.237489938735962], [-5.905942916870117, 2.999976873397827], [-5.8959150314331055, 2.8569793701171875], [-5.839818000793457, 2.8563294410705566], [-6.041816711425781, 2.4889419078826904], [-5.800403594970703, 2.6677162647247314], [-5.743133068084717, 2.525880813598633], [-5.573644638061523, 2.4899508953094482], [-5.793516635894775, 1.9225950241088867], [-5.9566850662231445, 1.8136204481124878], [-5.867406845092773, 1.6885408163070679], [-5.864697456359863, 1.6284581422805786], [-5.764241695404053, 1.6217055320739746], [-5.668868064880371, 1.636839747428894], [-5.524453163146973, 1.3691431283950806], [-5.416896343231201, 1.231851577758789], [-5.39024019241333, 1.4424231052398682], [-5.355198860168457, 1.731210708618164], [-5.230927467346191, 1.3700079917907715], [-5.132944107055664, 1.4042936563491821], [-5.055172920227051, 1.524610161781311], [-4.993722438812256, 1.7004566192626953], [-4.752987384796143, 1.7007670402526855], [-4.476794242858887, 1.833399772644043], [-4.250853538513184, 1.739127516746521], [-4.262433052062988, 2.017331838607788], [-4.249186992645264, 2.1165895462036133], [-3.964778184890747, 2.293886423110962], [-3.874737501144409, 2.4478626251220703], [-3.7084014415740967, 2.4141626358032227]]\n",
      "37\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a14e660cbd8e414ea59d567a16f6805d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='Page:', max=36), Output()), _dom_classes=('widget-intera…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.display_images(page)>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from io import BytesIO\n",
    "\n",
    "low_dim_embeddings = EmbeddingProjector().reduce_dimensions(torch.cat(embeddings).detach().numpy(), method=\"tsne\")\n",
    "low_dim_embeddings = low_dim_embeddings.tolist()\n",
    "\n",
    "x_axis, y_axis = zip(*low_dim_embeddings)\n",
    "plot_list = []\n",
    "for embedding in low_dim_embeddings:\n",
    "    plt.plot(embedding, marker='+', linestyle='None', markersize=10, color='blue')\n",
    "    \n",
    "    plt.xlim(min(x_axis) - 1, max(y_axis) + 1)\n",
    "    plt.ylim(min(y_axis) - 1, max(y_axis) + 1)\n",
    "    \n",
    "    plt.grid(True)\n",
    "    \n",
    "    buffer = BytesIO()\n",
    "    plt.savefig(buffer, format='png')\n",
    "    buffer.seek(0)\n",
    "    pil_image = Image.open(buffer)\n",
    "    plot_list.append(pil_image)\n",
    "    plt.close()\n",
    "\n",
    "images_per_page = 2\n",
    "\n",
    "def display_images(page):\n",
    "    start = page\n",
    "    print(start)\n",
    "    fig, axs = plt.subplots(1, images_per_page, figsize=(15, 5))  # Create subplots\n",
    "    for i in range(images_per_page):\n",
    "        if start + i < len(faces) + len(plot_list):\n",
    "            if i == 0:\n",
    "                axs[i].imshow(faces[start])\n",
    "            else:\n",
    "                axs[i].imshow(plot_list[start])\n",
    "            axs[i].axis('off')\n",
    "        else:\n",
    "            axs[i].axis('off')  # Hide axes for empty subplots\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "page_selector = widgets.IntSlider(min=0, max=(len(faces) + len(plot_list) - 1) // images_per_page, description='Page:')\n",
    "widgets.interact(display_images, page=page_selector)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
