{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33memirhan404\u001b[0m (\u001b[33mgorillas\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact model-8vymlbht:v3, 996.45MB. 1 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n",
      "Done. 0:0:3.4\n",
      "/opt/conda/envs/research/lib/python3.10/site-packages/torch/functional.py:507: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987280714/work/aten/src/ATen/native/TensorShape.cpp:3549.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "import torch\n",
    "from gorillatracker.model import EfficientNetV2Wrapper, SwinV2BaseWrapper\n",
    "from torchvision.transforms import transforms\n",
    "\n",
    "wandb.login()\n",
    "wandb.init(mode=\"disabled\")\n",
    "api = wandb.Api()\n",
    "\n",
    "artifact = api.artifact(\n",
    "    \"gorillas/Embedding-SwinV2-CXL-Open/model-8vymlbht:v3\",\n",
    "    type=\"model\",\n",
    ")\n",
    "artifact_dir = artifact.download()\n",
    "model = artifact_dir + \"/model.ckpt\"\n",
    "\n",
    "# load model\n",
    "checkpoint = torch.load(model, map_location=torch.device(\"cpu\"))\n",
    "\n",
    "model = SwinV2BaseWrapper(  # switch this with the model you want to use\n",
    "    model_name_or_path=\"SwinV2_Base\",\n",
    "    from_scratch=False,\n",
    "    loss_mode=\"offline/native\",\n",
    "    weight_decay=0.0001,\n",
    "    lr_schedule=\"linear\",\n",
    "    warmup_mode=\"linear\",\n",
    "    warmup_epochs=0,\n",
    "    max_epochs=10,\n",
    "    initial_lr=0.00001,\n",
    "    start_lr=0.00001,\n",
    "    end_lr=0.00001,\n",
    "    beta1=0.9,\n",
    "    beta2=0.999,\n",
    "    embedding_size=128,\n",
    ")\n",
    "# the following lines are necessary to load a model that was trained with arcface (the prototypes are saved in the state dict)\n",
    "#model.loss_module_train.prototypes = torch.nn.Parameter(checkpoint[\"state_dict\"][\"loss_module_train.prototypes\"])\n",
    "#model.loss_module_val.prototypes = torch.nn.Parameter(checkpoint[\"state_dict\"][\"loss_module_val.prototypes\"])\n",
    "\n",
    "transform=transforms.Compose(  # use the transforms that were used for the model (except of course data augmentations)\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Resize((192, 192)),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.228, 0.224, 0.225]), # if your model was trained with normalization, you need to normalize the images here as well\n",
    "        ]\n",
    "    )\n",
    "\n",
    "model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "model.eval()\n",
    "print(\"Model loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Video (and look at all the gorilla ids which contain face images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspaces/gorillatracker/video_data/M002_20220725_012.mp4\n",
      "Number of gorillas with tracked faces: 7\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from gorillatracker.utils.video_models import VideoClip, _parse_tracked_video_clip\n",
    "from datetime import datetime\n",
    "from PIL import Image\n",
    "\n",
    "json_path = \"/workspaces/gorillatracker/data/derived_data/spac_gorillas_converted_labels_tracked/M002_20220725_012_tracked.json\"\n",
    "mp4_path = \"/workspaces/gorillatracker/video_data\" + json_path.split(\"spac_gorillas_converted_labels_tracked\")[1].replace(\"_tracked.json\", \".mp4\")\n",
    "print(mp4_path)\n",
    "v = VideoClip(video_id=\"\", camera_id=\"\", start_time=datetime.now())\n",
    "v = _parse_tracked_video_clip(v, json_path)\n",
    "video = cv2.VideoCapture(mp4_path)\n",
    "\n",
    "counter = 0\n",
    "frame_dict = {}\n",
    "for gorilla in v.trackings:\n",
    "    if len(gorilla.bounding_boxes_face) > 0:\n",
    "        for frame in gorilla.bounding_boxes_face:\n",
    "            try:\n",
    "                frame_dict[frame.f].append((frame, gorilla.individual_id))\n",
    "            except KeyError:\n",
    "                frame_dict[frame.f] = [(frame, gorilla.individual_id)]\n",
    "        counter+=1\n",
    "print(f\"Number of gorillas with tracked faces: {counter}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Embedding Projector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import Isomap, LocallyLinearEmbedding, MDS, SpectralEmbedding, TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import umap.umap_ as umap\n",
    "\n",
    "class EmbeddingProjector:\n",
    "    def __init__(self):\n",
    "        self.algorithms = {\n",
    "            \"tsne\": TSNE(n_components=2),\n",
    "            \"isomap\": Isomap(n_components=2),\n",
    "            \"lle\": LocallyLinearEmbedding(n_components=2),\n",
    "            \"mds\": MDS(n_components=2),\n",
    "            \"spectral\": SpectralEmbedding(n_components=2),\n",
    "            \"pca\": PCA(n_components=2),\n",
    "            \"umap\": umap.UMAP(),\n",
    "        }\n",
    "\n",
    "    def reduce_dimensions(self, embeddings, method=\"tsne\"):\n",
    "        algorithm = self.algorithms.get(method, TSNE(n_components=2))\n",
    "        return algorithm.fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract face images and embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import colorcet as cc\n",
    "\n",
    "def hex_to_rgb(hex):\n",
    "    hex = hex.replace(\"#\", \"\")\n",
    "    return tuple(int(hex[i:i+2], 16) for i in (0, 2, 4))\n",
    "\n",
    "colors = cc.glasbey\n",
    "embedding_dict = {}\n",
    "images = []\n",
    "\n",
    "for f in frame_dict.keys():\n",
    "    video.set(cv2.CAP_PROP_POS_FRAMES, f)\n",
    "    ret, img = video.read()\n",
    "    imgcopy = img.copy()\n",
    "    for frame_gorilla in frame_dict[f]:\n",
    "        cropped_img = Image.fromarray(imgcopy).crop(frame_gorilla[0].bb[0] + frame_gorilla[0].bb[1])\n",
    "        cropped_img = transform(cropped_img).unsqueeze(0)\n",
    "        embedding = model(cropped_img).detach()\n",
    "        try:\n",
    "            embedding_dict[f].append((embedding, frame_gorilla[1]))\n",
    "        except KeyError:\n",
    "            embedding_dict[f] = [(embedding, frame_gorilla[1])]\n",
    "        img = cv2.rectangle(img, frame_gorilla[0].bb[0], frame_gorilla[0].bb[1], hex_to_rgb(colors[frame_gorilla[1]]), 2)\n",
    "    images.append(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot each embedding with a slider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list.append() takes exactly one argument (2 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m embedding_dict\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m      8\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m [pair[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m pair \u001b[38;5;129;01min\u001b[39;00m embedding_dict[f]]\n\u001b[0;32m----> 9\u001b[0m     \u001b[43mall_embeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m low_dim_embeddings \u001b[38;5;241m=\u001b[39m EmbeddingProjector()\u001b[38;5;241m.\u001b[39mreduce_dimensions(torch\u001b[38;5;241m.\u001b[39mcat(all_embeddings)\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy(),\n\u001b[1;32m     12\u001b[0m                                                             method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpca\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m     13\u001b[0m min_x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m([pair[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m pair \u001b[38;5;129;01min\u001b[39;00m low_dim_embeddings])\n",
      "\u001b[0;31mTypeError\u001b[0m: list.append() takes exactly one argument (2 given)"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "import copy\n",
    "from io import BytesIO\n",
    "\n",
    "all_embeddings = []\n",
    "for f in embedding_dict.keys():\n",
    "    embeddings = [pair[0] for pair in embedding_dict[f]]\n",
    "    all_embeddings.append(*embeddings)\n",
    "\n",
    "low_dim_embeddings = EmbeddingProjector().reduce_dimensions(torch.cat(all_embeddings).detach().numpy(),\n",
    "                                                            method=\"pca\").tolist()\n",
    "min_x = min([pair[0] for pair in low_dim_embeddings])\n",
    "max_x = max([pair[0] for pair in low_dim_embeddings])\n",
    "min_y = min([pair[1] for pair in low_dim_embeddings])\n",
    "max_y = max([pair[1] for pair in low_dim_embeddings])\n",
    "\n",
    "low_dim_embedding_dict = copy.deepcopy(embedding_dict)\n",
    "for f in embedding_dict.keys():\n",
    "    for i, pair in enumerate(embedding_dict[f]):\n",
    "        low_dim_embedding_dict[f][i] = (low_dim_embeddings.pop(0), pair[1])\n",
    "\n",
    "plot_list = []\n",
    "\n",
    "for f in low_dim_embedding_dict.keys():\n",
    "    plt.xlim(min_x - 1, max_x + 1)\n",
    "    plt.ylim(min_y - 1, max_y + 1)\n",
    "    plt.grid(True)\n",
    "    for pair in low_dim_embedding_dict[f]:\n",
    "        plt.plot(pair[0][0], pair[0][1], marker='+', linestyle='None', markersize=10, color=colors[pair[1]])\n",
    "\n",
    "    plt.title(f\"Frame {f}\")\n",
    "    buffer = BytesIO()\n",
    "    plt.savefig(buffer, format='png')\n",
    "    buffer.seek(0)\n",
    "    pil_image = Image.open(buffer)\n",
    "    plot_list.append(pil_image)\n",
    "    plt.close()\n",
    "\n",
    "images_per_page = 2\n",
    "\n",
    "def display_images(page):\n",
    "    start = page\n",
    "    print(start)\n",
    "    fig, axs = plt.subplots(1, images_per_page, figsize=(15, 5))  # Create subplots\n",
    "    for i in range(images_per_page):\n",
    "        if start + i < len(images) + len(plot_list):\n",
    "            if i == 0:\n",
    "                axs[i].imshow(images[start])\n",
    "            else:\n",
    "                axs[i].imshow(plot_list[start])\n",
    "            axs[i].axis('off')\n",
    "        else:\n",
    "            axs[i].axis('off')  # Hide axes for empty subplots\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "page_selector = widgets.IntSlider(min=0, max=(len(images) + len(plot_list) - 1) // images_per_page, description='Page:')\n",
    "widgets.interact(display_images, page=page_selector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7433cd3cc6046ee94b93c0f436ca7b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='Page:', max=36), Output()), _dom_classes=('widget-intera…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.display_images(page)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from io import BytesIO\n",
    "\n",
    "low_dim_embeddings = EmbeddingProjector().reduce_dimensions(torch.cat(embeddings).detach().numpy(),\n",
    "                                                            method=\"pca\")\n",
    "low_dim_embeddings = low_dim_embeddings.tolist()\n",
    "\n",
    "x_axis, y_axis = zip(*low_dim_embeddings)\n",
    "plot_list = []\n",
    "for embedding in low_dim_embeddings:\n",
    "    plt.xlim(min(x_axis) - 1, max(x_axis) + 1)\n",
    "    plt.ylim(min(y_axis) - 1, max(y_axis) + 1)\n",
    "    plt.plot(embedding[0], embedding[1], marker='+', linestyle='None', markersize=10, color='blue')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    buffer = BytesIO()\n",
    "    plt.savefig(buffer, format='png')\n",
    "    buffer.seek(0)\n",
    "    pil_image = Image.open(buffer)\n",
    "    plot_list.append(pil_image)\n",
    "    plt.close()\n",
    "\n",
    "images_per_page = 2\n",
    "\n",
    "def display_images(page):\n",
    "    start = page\n",
    "    print(start)\n",
    "    fig, axs = plt.subplots(1, images_per_page, figsize=(15, 5))  # Create subplots\n",
    "    for i in range(images_per_page):\n",
    "        if start + i < len(faces) + len(plot_list):\n",
    "            if i == 0:\n",
    "                axs[i].imshow(faces[start])\n",
    "            else:\n",
    "                axs[i].imshow(plot_list[start])\n",
    "            axs[i].axis('off')\n",
    "        else:\n",
    "            axs[i].axis('off')  # Hide axes for empty subplots\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "page_selector = widgets.IntSlider(min=0, max=(len(faces) + len(plot_list) - 1) // images_per_page, description='Page:')\n",
    "widgets.interact(display_images, page=page_selector)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
