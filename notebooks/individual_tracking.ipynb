{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import select, text, create_engine\n",
    "from sqlalchemy.orm import Session\n",
    "from gorillatracker.ssl_pipeline.models import TrackingFrameFeature, Tracking, Video, Camera, VideoFeature\n",
    "from gorillatracker.ssl_pipeline.dataset import GorillaDatasetKISZ\n",
    "import importlib\n",
    "from typing import List, Literal, Optional, Tuple\n",
    "\n",
    "from PIL import Image\n",
    "from torch import Tensor\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "import gorillatracker.type_helper as gtypes\n",
    "from gorillatracker.transform_utils import SquarePad\n",
    "from gorillatracker.type_helper import Id, Label\n",
    "from gorillatracker.utils.labelencoder import LabelEncoder\n",
    "\n",
    "from gorillatracker.data.nlet_dm import FlatNletBuilder, NletDataModule\n",
    "from gorillatracker.data.nlet import SupervisedDataset, build_onelet\n",
    "from gorillatracker.utils.embedding_generator import generate_embeddings, df_from_predictions\n",
    "from gorillatracker.utils.wandb_loader import load_model\n",
    "\n",
    "from pathlib import Path\n",
    "import gorillatracker.utils.embedding_generator as eg\n",
    "import pandas as pd\n",
    "from io import BytesIO\n",
    "import base64\n",
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "from bokeh.plotting import figure, show\n",
    "from bokeh.models import ColumnDataSource, LinearColorMapper, HoverTool, WheelZoomTool, FixedTicker, DatetimeTicker\n",
    "from bokeh.io import output_notebook\n",
    "from bokeh.resources import INLINE\n",
    "import colorcet as cc\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from bokeh.models import Slider, CustomJS, Div\n",
    "from bokeh.layouts import column, row\n",
    "from bokeh.models import Text, Line, Scatter\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "import geopy.distance\n",
    "from collections import Counter\n",
    "from IPython.display import display, clear_output\n",
    "import dill\n",
    "import torch\n",
    "from gorillatracker.model.wrappers_supervised import BaseModuleSupervised\n",
    "from gorillatracker.old_model import VisionTransformerWrapper\n",
    "\n",
    "output_notebook(INLINE)\n",
    "\n",
    "sample = 10\n",
    "query = text(\n",
    "f\"\"\"WITH ranked_features AS (\n",
    "    SELECT\n",
    "        tracking_id,\n",
    "        tracking_frame_feature_id,\n",
    "        bbox_width,\n",
    "        bbox_height,\n",
    "        frame_nr,\n",
    "        feature_type,\n",
    "        ROW_NUMBER() OVER (PARTITION BY tracking_id ORDER BY RANDOM()) AS rn\n",
    "    FROM tracking_frame_feature\n",
    "    WHERE feature_type = 'face_45'\n",
    "        AND bbox_width >= 184\n",
    "        AND bbox_height >= 184\n",
    "        AND tracking_id IS NOT NULL\n",
    ")\n",
    "SELECT\n",
    "    tracking_frame_feature_id\n",
    "FROM ranked_features\n",
    "WHERE rn <= {sample}\n",
    "\"\"\"     # 10 random samples pro tracking_id\n",
    ")\n",
    "\n",
    "engine = create_engine(GorillaDatasetKISZ.DB_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# devcontainer.mount add \"source=/mnt/vast-gorilla/cropped-images,target=/workspaces/gorillatracker/cropped-images,type=bind,ro\"\n",
    "base_path = \"../video_data/cropped-images/2024-04-18\"\n",
    "\n",
    "class HackDataset(Dataset[Tuple[Id, Tensor, Label]]):\n",
    "    def get_tffs(self) -> list[TrackingFrameFeature]:\n",
    "        engine = create_engine(GorillaDatasetKISZ.DB_URI)\n",
    "\n",
    "        stmt = select(TrackingFrameFeature, Video.video_id, Tracking.tracking_id).join(\n",
    "            Tracking, Tracking.tracking_id == TrackingFrameFeature.tracking_id).join(\n",
    "            Video, Tracking.video_id == Video.video_id).where(\n",
    "            TrackingFrameFeature.tracking_frame_feature_id.in_(query)).where(Video.start_time >= \"2022-01-01 00:00:00\")\n",
    "\n",
    "        with Session(engine) as session:\n",
    "            return session.execute(stmt).all()\n",
    "\n",
    "    def __init__(\n",
    "        self, base_dir: str, partition: Literal[\"train\", \"val\", \"test\"], nlet_builder: FlatNletBuilder, transform: Optional[gtypes.Transform] = None\n",
    "    ):\n",
    "        self.tffs = self.get_tffs()\n",
    "        self.transform = transform\n",
    "        self.partition = partition\n",
    "        self.video_ids = [tff.video_id for tff in self.tffs]\n",
    "        self.tracking_ids = [tff.tracking_id for tff in self.tffs]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.tffs)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[Id, Tensor, Label]:\n",
    "        \"\"\"tracklets will be labels for now\"\"\"\n",
    "        tff = self.tffs[idx][0]\n",
    "\n",
    "        img = Image.open(tff.cache_path(base_path))\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        assert tff.tracking_id is not None\n",
    "        return tff.tracking_frame_feature_id, img, tff.tracking_id\n",
    "    \n",
    "    @classmethod\n",
    "    def get_transforms(cls) -> gtypes.Transform:\n",
    "        return transforms.Compose(\n",
    "            [\n",
    "                SquarePad(),\n",
    "                # Uniform input, you may choose higher/lower sizes.\n",
    "                transforms.Resize(256),\n",
    "                transforms.ToTensor(),\n",
    "            ]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python3 train.py --config_path cfgs/swinv2_cxl.yml\n",
    "\n",
    "# gorillas/Embedding-SwinV2Base-CXL-Open/model-yl2lx567:v12\n",
    "if False:\n",
    "    run_url = \"https://wandb.ai/gorillas/Embedding-SwinV2Large-CXL-Open/runs/olql2abq?nw=nwuseremirhan404\"\n",
    "    model = get_model_for_run_url(run_url)\n",
    "else:\n",
    "    # path = \"/workspaces/gorillatracker/models/vit_large_dinov2_baseline.ckpt\"\n",
    "    path = \"/workspaces/gorillatracker/models/roberts_models/gorillas_models/vit_large_dinov2_bayes/fold-0-epoch-19-cxlkfold/fold-0/val/embeddings/knn5_crossvideo/accuracy-0.63.ckpt\"\n",
    "    model = BaseModuleSupervised.load_from_checkpoint(path, data_module=None, wandb_run=None, strict=False)\n",
    "    # model = load_model(VisionTransformerWrapper, path)\n",
    "model_transforms = transforms.Compose(\n",
    "    [\n",
    "        SquarePad(),\n",
    "        transforms.Resize(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(eg)\n",
    "# python3 train.py --config_path cfgs/swinv2_cxl.yml\n",
    "\n",
    "# gorillas/Embedding-SwinV2Base-CXL-Open/model-yl2lx567:v12\n",
    "data_dir = \"/workspaces/gorillatracker/data/supervised/splits/cxl_faces_square_all-in-val\"\n",
    "data_module = NletDataModule(\n",
    "    data_dir=Path(data_dir),\n",
    "    dataset_class=SupervisedDataset,\n",
    "    nlet_builder=build_onelet,\n",
    "    batch_size=32,\n",
    "    workers=10,\n",
    "    model_transforms=model_transforms,\n",
    "    training_transforms=lambda x: x,\n",
    "    dataset_names=[\"Inference\"],\n",
    ")\n",
    "print(\"test1\")\n",
    "data_module.setup(\"validate\")\n",
    "print(\"test2\")\n",
    "dataloader = data_module.val_dataloader()\n",
    "print(\"test3\")\n",
    "predictions = eg.generate_embeddings(model, dataloader)\n",
    "print(\"test4\")\n",
    "print(len(predictions[0]), len(predictions[1]), len(predictions[2]))\n",
    "df_sup = eg.df_from_predictions(predictions)\n",
    "print(\"test5\")\n",
    "df_sup.to_pickle(\"embeddings.pkl\")\n",
    "print(df_sup.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(eg)\n",
    "\n",
    "data_dir = \"/workspaces/gorillatracker/video_data/cropped-images/2024-04-18\"\n",
    "\n",
    "data_module = NletDataModule(\n",
    "    data_dir=Path(data_dir),\n",
    "    dataset_class=HackDataset,\n",
    "    nlet_builder=build_onelet,\n",
    "    batch_size=32,\n",
    "    workers=40,\n",
    "    model_transforms=model_transforms,\n",
    "    training_transforms=lambda x: x,\n",
    "    dataset_names=[\"Inference\"],\n",
    ")\n",
    "print(\"test1\")\n",
    "data_module.setup(\"validate\")\n",
    "print(\"test2\")\n",
    "dataloader = data_module.val_dataloader()\n",
    "print(\"test3\")\n",
    "predictions = eg.generate_ssl_embeddings(model, dataloader)\n",
    "print(\"test4\")\n",
    "df = eg.df_from_predictions(predictions)\n",
    "print(\"test5\")\n",
    "with open(\"ssl_embeddings_2022.pkl\", \"wb\") as dill_file:\n",
    "    dill.dump(df, dill_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"ssl_embeddings.pkl\", \"rb\") as dill_file:\n",
    "    df = dill.load(dill_file)\n",
    "# df = pd.read_pickle(\"ssl_embeddings.pkl\")\n",
    "display(df.head())\n",
    "df[\"embedding\"] = df[\"embedding\"].apply(torch.from_numpy)\n",
    "print(df[\"embedding\"].iloc[0])\n",
    "\n",
    "with open(\"embeddings.pkl\", \"rb\") as dill_file:\n",
    "    df_sup = dill.load(dill_file)\n",
    "# df_sup = pd.read_pickle(\"embeddings.pkl\")\n",
    "df_sup[\"embedding\"] = df_sup[\"embedding\"].apply(torch.from_numpy)\n",
    "display(df_sup.head())\n",
    "\n",
    "print(df.shape, df_sup.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tff_ids = df[\"id\"].tolist()\n",
    "\n",
    "# Ensure tff_ids are scalars if they are tensor objects\n",
    "tff_ids = [t.item() for t in tff_ids]\n",
    "\n",
    "# Create a mapping of tff_ids to their positions\n",
    "id_order = {id: index for index, id in enumerate(tff_ids)}\n",
    "\n",
    "print(tff_ids)\n",
    "\n",
    "# Create the SQL statement\n",
    "stmt = (\n",
    "    select(TrackingFrameFeature.tracking_frame_feature_id, TrackingFrameFeature.video_id, TrackingFrameFeature.tracking_id)\n",
    "    .where(TrackingFrameFeature.tracking_frame_feature_id.in_(tff_ids))\n",
    ")\n",
    "\n",
    "# Execute the query\n",
    "with Session(engine) as session:\n",
    "    result = session.execute(stmt).all()\n",
    "\n",
    "# Unpack the results and order them based on the original tff_ids list\n",
    "ordered_result = sorted(result, key=lambda x: id_order[x[0]])\n",
    "\n",
    "# Unzip the ordered result into separate lists\n",
    "tff_ids2, video_ids, tracking_ids = zip(*ordered_result)\n",
    "\n",
    "print(tff_ids2)\n",
    "print(len(tff_ids2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_sup = np.vstack(df_sup[\"embedding\"].values)\n",
    "labels_sup = df_sup[\"label\"].values\n",
    "labels_sup = [x.item() for x in labels_sup]\n",
    "label_strings_sup = df_sup[\"label_string\"].values\n",
    "\n",
    "embeddings = np.vstack(df[\"embedding\"].values)\n",
    "print(len(embeddings))\n",
    "labels = df[\"label\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "raw_images = []\n",
    "for tff_id in tff_ids:\n",
    "    path = Path(\n",
    "        base_path,\n",
    "        str(tff_id % 2**8),\n",
    "        str(tff_id % 2**16),\n",
    "        f\"{tff_id}.png\",\n",
    "    )\n",
    "    image = Image.open(path)\n",
    "    raw_images.append(image)\n",
    "    buffer = BytesIO()\n",
    "    image.save(buffer, format=\"JPEG\")\n",
    "    image_byte = base64.b64encode(buffer.getvalue()).decode(\"utf-8\")\n",
    "    images.append(image_byte)\n",
    "\n",
    "print(len(embeddings_sup))\n",
    "\n",
    "images_sup = []\n",
    "raw_images_sup = []\n",
    "for id in df_sup[\"id\"]:\n",
    "    img = Image.open(id)\n",
    "    raw_images_sup.append(img)\n",
    "    buffer = BytesIO()\n",
    "    img.save(buffer, format=\"JPEG\")\n",
    "    image_byte = base64.b64encode(buffer.getvalue()).decode(\"utf-8\")\n",
    "    images_sup.append(image_byte)\n",
    "\n",
    "algo = TSNE(n_components=2, perplexity=30)\n",
    "\n",
    "all_embeddings = np.vstack([embeddings, embeddings_sup])\n",
    "low_dim_em_all = algo.fit_transform(all_embeddings)\n",
    "low_dim_em = low_dim_em_all[:len(embeddings)]\n",
    "low_dim_em_sup = low_dim_em_all[len(embeddings):]\n",
    "\n",
    "print(len(low_dim_em_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = figure(tools=\"pan, wheel_zoom, box_zoom, reset\")\n",
    "\n",
    "cds = ColumnDataSource(data={'x': low_dim_em[:, 0], 'y': low_dim_em[:, 1], 'video_id': video_ids, 'tracking_id': tracking_ids, 'image': images})\n",
    "\n",
    "exp_cmap = LinearColorMapper(palette=cc.glasbey, \n",
    "                             low = min(tracking_ids), \n",
    "                             high = max(tracking_ids))\n",
    "fig.scatter(\n",
    "    source=cds,\n",
    "    size=12,\n",
    "    line_color=\"black\",\n",
    "    fill_color={\"field\": \"tracking_id\", \"transform\": exp_cmap},\n",
    ")\n",
    "fig.toolbar.active_scroll = fig.select_one(WheelZoomTool)\n",
    "\n",
    "hover = HoverTool(tooltips='<img src=\"data:image/jpeg;base64,@image\" width=\"128\" height=\"128\">')\n",
    "fig.add_tools(hover)\n",
    "show(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = figure(tools=\"pan, wheel_zoom, box_zoom, reset\")\n",
    "\n",
    "cds_sup = ColumnDataSource(data={'x': low_dim_em_sup[:, 0], 'y': low_dim_em_sup[:, 1], 'label': labels_sup, 'label_str': label_strings_sup, 'image': images_sup})\n",
    "\n",
    "exp_cmap_sup = LinearColorMapper(palette=cc.glasbey, \n",
    "                             low = min(labels_sup), \n",
    "                             high = max(labels_sup))\n",
    "fig.scatter(\n",
    "    source=cds_sup,\n",
    "    size=12,\n",
    "    line_color=\"black\",\n",
    "    fill_color={\"field\": \"label\", \"transform\": exp_cmap_sup},\n",
    ")\n",
    "fig.toolbar.active_scroll = fig.select_one(WheelZoomTool)\n",
    "\n",
    "fig.scatter(\n",
    "    source=cds,\n",
    "    size=12,\n",
    "    line_color=\"black\",\n",
    "    fill_color={\"field\": \"tracking_id\", \"transform\": exp_cmap},\n",
    "    alpha=0.2\n",
    ")\n",
    "\n",
    "hover = HoverTool(tooltips=\"\"\"<div>\n",
    "                    <img src=\"data:image/jpeg;base64,@image\" width=\"128\" height=\"128\">\n",
    "                    <div><strong>Label:</strong> @label_str</div>\n",
    "                  </div>\"\"\")\n",
    "fig.add_tools(hover)\n",
    "show(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "knn = KNeighborsClassifier(n_neighbors=k)\n",
    "\n",
    "knn.fit(embeddings_sup, label_strings_sup)\n",
    "\n",
    "distance_threshold = 10\n",
    "\n",
    "# Get the distances and indices of the nearest neighbors\n",
    "distances, indices = knn.kneighbors(embeddings)\n",
    "\n",
    "# Classify the new embeddings\n",
    "predictions = knn.predict(embeddings)\n",
    "\n",
    "# Introduce the threshold distance condition\n",
    "new_len = 0\n",
    "valid_predictions = []\n",
    "for i, (dist, pred) in enumerate(zip(distances, predictions)):\n",
    "    if dist[0] > distance_threshold:\n",
    "        valid_predictions.append(-1)  # -1 for invalid label\n",
    "    else:\n",
    "        valid_predictions.append(pred)\n",
    "        new_len+=1\n",
    "\n",
    "display(\"Valid Predictions:\", valid_predictions)\n",
    "\n",
    "print(new_len, len(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_pages = []\n",
    "for i, pred in enumerate(valid_predictions):\n",
    "    if pred != -1:\n",
    "        closest = [(raw_images_sup[indices[i][j]], label_strings_sup[indices[i][j]], distances[i][j]) for j in range(k)]\n",
    "        image_pages.append((raw_images[i], closest, pred))\n",
    "        \n",
    "def display_images(page):\n",
    "    fig, axs = plt.subplots(1, k+1)  # Create subplots\n",
    "    if page < len(image_pages): \n",
    "        img = image_pages[page][0]\n",
    "        axs[0].imshow(img)\n",
    "        axs[0].set_title(\"Prediction: {}\".format(image_pages[page][2]), fontsize=8)\n",
    "        for i in range(k):\n",
    "            img, label, dist = image_pages[page][1][i]\n",
    "            axs[i+1].imshow(img)\n",
    "            axs[i+1].set_title(str(i) + \". closest image \\n dist: \" + str(round(dist, 3)) + \"\\n Label: \" + label, fontsize=8)\n",
    "            axs[i+1].axis(\"off\")\n",
    "        else:\n",
    "            axs[i].axis(\"off\")  # Hide axes for empty subplots\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "page_selector = widgets.IntSlider(min=0, max=(len(image_pages) - 1), description=\"Page:\")\n",
    "widgets.interact(display_images, page=page_selector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_dict = {}\n",
    "\n",
    "# Create a dictionary to map classes to image indices\n",
    "for i, cls in enumerate(valid_predictions):\n",
    "    if cls == -1:\n",
    "        continue\n",
    "    if cls not in index_dict:\n",
    "        index_dict[cls] = []\n",
    "    index_dict[cls].append(i)\n",
    "    \n",
    "# Convert the dictionary to a list of tuples for easy indexing\n",
    "index_list = list(index_dict.items())\n",
    "\n",
    "# Constants\n",
    "IMAGES_PER_PAGE = 6  # Number of images to display per page\n",
    "\n",
    "def display_images(class_index, page):\n",
    "    if class_index < len(index_list):\n",
    "        class_name, indices = index_list[class_index]\n",
    "        start_idx = page * IMAGES_PER_PAGE\n",
    "        end_idx = start_idx + IMAGES_PER_PAGE\n",
    "        num_imgs = len(indices[start_idx:end_idx])\n",
    "        \n",
    "        fig, axs = plt.subplots(1, num_imgs, figsize=(15, 5))\n",
    "        fig.suptitle(f\"Class {class_name} - Page {page}\")\n",
    "\n",
    "        # If there is only one image, axs is not an array, so we handle that case\n",
    "        if num_imgs == 1:\n",
    "            axs = [axs]\n",
    "\n",
    "        for ax, idx in zip(axs, indices[start_idx:end_idx]):\n",
    "            img = raw_images[idx]\n",
    "            ax.imshow(img)\n",
    "            ax.axis(\"off\")\n",
    "\n",
    "        # Hide unused subplots if any\n",
    "        for ax in axs[num_imgs:]:\n",
    "            ax.axis(\"off\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"Invalid class index.\")\n",
    "        \n",
    "# Create a slider widget to select the page within the selected class\n",
    "page_selector = widgets.IntSlider(min=0, max=0, description=\"Page:\")\n",
    "\n",
    "def update_page_selector(*args):\n",
    "    class_index = class_selector.value\n",
    "    num_pages = (len(index_list[class_index][1]) + IMAGES_PER_PAGE - 1) // IMAGES_PER_PAGE\n",
    "    page_selector.max = num_pages - 1\n",
    "    page_selector.value = 0  # Reset to the first page when class changes\n",
    "\n",
    "# Create a dropdown widget to select the class\n",
    "class_selector = widgets.Dropdown(\n",
    "    options=[(f\"Class {cls}\", i) for i, (cls, _) in enumerate(index_list)],\n",
    "    description=\"Class:\"\n",
    ")\n",
    "class_selector.observe(update_page_selector, 'value')\n",
    "\n",
    "# Display the widgets and images\n",
    "widgets.interact(display_images, class_index=class_selector, page=page_selector)\n",
    "\n",
    "# Initialize the page selector based on the initial class selection\n",
    "update_page_selector()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels = df.loc[cluster_indices[cls], \"label\"].values\n",
    "\n",
    "# rest_cluster_indices = [x for i,x in enumerate(cluster_indices) if i != cls]\n",
    "\n",
    "# low_dim_em_class = low_dim_em[cluster_indices[cls]]\n",
    "\n",
    "low_dim_em_classes = [(cls, [low_dim_em[x] for x in indices]) for cls, indices in index_dict.items()]\n",
    "\n",
    "fig = figure(tools=\"pan, wheel_zoom, box_zoom, reset\")\n",
    "\n",
    "cds1 = ColumnDataSource(data={'x': low_dim_em[:, 0], 'y': low_dim_em[:, 1], 'video_id': video_ids, 'tracking_id': tracking_ids})\n",
    "\n",
    "fig.scatter(\n",
    "    source=cds1,\n",
    "    size=12,\n",
    "    line_color=\"black\",\n",
    "    fill_color=\"gray\",\n",
    "    alpha=0.3\n",
    ")\n",
    "\n",
    "exp_cmap = LinearColorMapper(palette=cc.glasbey, \n",
    "                             low = min(tracking_ids), \n",
    "                             high = max(tracking_ids))\n",
    "\n",
    "x = [low_dim_em_classes[x][1] for x in range(len(index_list))]\n",
    "\n",
    "source = ColumnDataSource(data={'x': x, 'video_id': [[video_ids[idx] for idx in index_list[x][1]] for x in range(len(index_list))], 'tracking_id': [[tracking_ids[idx] for idx in index_list[x][1]] for x in range(len(index_list))], 'class': [low_dim_em_classes[x][0] for x in range(len(index_list))]})\n",
    "source_visible = ColumnDataSource(data={'x': np.array(low_dim_em_classes[0][1])[:, 0], 'y': np.array(low_dim_em_classes[0][1])[:, 1], 'video_id': [video_ids[idx] for idx in index_list[0][1]], 'tracking_id': [tracking_ids[idx] for idx in index_list[0][1]], 'class': [low_dim_em_classes[0][0]]})\n",
    "\n",
    "display(source.data[\"video_id\"])\n",
    "\n",
    "fig.scatter(\n",
    "    source=source_visible,\n",
    "    size=12,\n",
    "    line_color=\"black\",\n",
    "    fill_color={\"field\": \"tracking_id\", \"transform\": exp_cmap},\n",
    ")\n",
    "\n",
    "div = Div(text=f\"<b>Class: {low_dim_em_classes[0][0]}</b>\")\n",
    "\n",
    "slider = Slider(start=0, end=len(index_list)-1, value=0, step=1, title=\"Cluster\")\n",
    "slider.js_on_change(\"value\", CustomJS(args=dict(source_visible=source_visible, source_avaliable=source, slider=slider, div=div), code=\"\"\"\n",
    "    var data_visible = source_visible.data;\n",
    "    var data = source_avaliable.data;\n",
    "    \n",
    "    // Update x values\n",
    "    const dataArray = data.x[slider.value.toString()];\n",
    "    const firstColumn = dataArray.map(row => row[0]);\n",
    "    \n",
    "    data_visible.x = firstColumn;\n",
    "    \n",
    "    // Update y values\n",
    "    const secondColumn = dataArray.map(row => row[1]);\n",
    "    \n",
    "    data_visible.y = secondColumn;\n",
    "\n",
    "    // Update video_id\n",
    "    data_visible.video_id = data.video_id[slider.value.toString()];\n",
    "\n",
    "    // Update tracking_id\n",
    "    data_visible.tracking_id = data.tracking_id[slider.value.toString()];\n",
    "    \n",
    "    data_visible.class = data.class[slider.value.toString()];\n",
    "\n",
    "    div.text = \"<b>Class: \" + data_visible.class + \"</b>\";\n",
    "    \n",
    "    source_visible.change.emit();\n",
    "\"\"\"))\n",
    "\n",
    "\n",
    "fig.toolbar.active_scroll = fig.select_one(WheelZoomTool)\n",
    "\n",
    "hover = HoverTool(tooltips=[(\"video_id\", \"@video_id\"), (\"tracking_id\", \"@tracking_id\")])\n",
    "fig.add_tools(hover)\n",
    "show(column(fig, column(slider, div)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = GorillaDatasetKISZ().engine\n",
    "session_cls = sessionmaker(bind=engine)\n",
    "\n",
    "sorted_videos_list = []\n",
    "\n",
    "for cls in range(len(index_list)):\n",
    "    videos = [video_ids[idx] for idx in index_list[cls][1]]\n",
    "    print(len(videos))\n",
    "    stmt = select(Video.camera_id, Video.start_time, Video.video_id).where(Video.video_id.in_(videos)).where(Video.start_time.isnot(None))\n",
    "\n",
    "    with session_cls() as session:\n",
    "        result = session.execute(stmt).all()\n",
    "\n",
    "    sorted_videos = sorted(result, key=lambda x: x[1])\n",
    "    print(len(sorted_videos))\n",
    "    print(sorted_videos)\n",
    "    \n",
    "    sorted_videos_list.append((index_list[cls][0], sorted_videos))\n",
    "    \n",
    "print(sorted_videos_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stmt = (select(Video.video_id, VideoFeature.value).join(Video, Video.video_id == VideoFeature.video_id)\n",
    "        .where(Video.video_id.in_(video_ids)))\n",
    "\n",
    "with session_cls() as session:\n",
    "    video_labels = session.execute(stmt).all()\n",
    "    \n",
    "    video_labels_map = dict(video_labels)\n",
    "\n",
    "print(video_labels_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stmt = (\n",
    "    select(Camera.camera_id, Camera.longitude, Camera.latitude).where(Camera.longitude.isnot(None)).where(Camera.latitude.isnot(None))\n",
    "    )\n",
    "\n",
    "with session_cls() as session:\n",
    "    result = session.execute(stmt)\n",
    "    all2 = sorted(result.fetchall(), key=lambda x: x[0])\n",
    "\n",
    "mapping = {x : (y,z) for x, y, z in all2}\n",
    "\n",
    "id, x, y = zip(*all2)\n",
    "\n",
    "def better_camera(id1, id2, valid_videos):\n",
    "    count1 = 0\n",
    "    count2 = 0\n",
    "    for vid in valid_videos:\n",
    "        if vid.camera_id == id1:\n",
    "            count1 += 1\n",
    "        if vid.camera_id == id2:\n",
    "            count2 += 1\n",
    "    return count1 > count2\n",
    "\n",
    "valid_videos_list = []\n",
    "\n",
    "model_error = 0\n",
    "model_error_match = 0\n",
    "for cls, videos in sorted_videos_list:\n",
    "    valid_videos = []\n",
    "    error_count = 0\n",
    "    match_error_count = 0\n",
    "    \n",
    "    for vid in videos:\n",
    "        if vid[0] in id and vid[1] is not None:\n",
    "            valid_videos.append(vid)\n",
    "\n",
    "    final_videos = []\n",
    "    # print(len(valid_videos))\n",
    "    skip_next = False\n",
    "    for i in range(len(valid_videos)):\n",
    "        label_mismatch = False\n",
    "        try:\n",
    "            if cls[:2] != video_labels_map[valid_videos[i].video_id]:\n",
    "                print(cls[:2], video_labels_map[valid_videos[i].video_id])\n",
    "                match_error_count += 1\n",
    "                label_mismatch = True\n",
    "        except(KeyError):\n",
    "            pass\n",
    "        if i == len(valid_videos) - 1:\n",
    "            break\n",
    "        if skip_next:\n",
    "            skip_next = False\n",
    "            continue\n",
    "        time = valid_videos[i+1].start_time - valid_videos[i].start_time\n",
    "        x1, y1 = mapping[valid_videos[i].camera_id]\n",
    "        x2, y2 = mapping[valid_videos[i+1].camera_id]\n",
    "        dist = geopy.distance.distance((y1, x1), (y2, x2)).m\n",
    "        # print(\"time\", (time.seconds//3600))\n",
    "        # print(\"dist\", dist)\n",
    "        avg_speed = 0.0115741 # m/s\n",
    "        if dist/time.seconds > avg_speed:\n",
    "            error_count+=1\n",
    "            # print(i)\n",
    "            if better_camera(valid_videos[i].camera_id, valid_videos[i+1].camera_id, valid_videos):\n",
    "                if not label_mismatch:\n",
    "                    final_videos.append(valid_videos[i])\n",
    "            else:\n",
    "                try:\n",
    "                    if cls[:2] == video_labels_map[valid_videos[i+1].video_id]:\n",
    "                        final_videos.append(valid_videos[i+1])\n",
    "                        skip_next = True\n",
    "                except(KeyError):\n",
    "                    final_videos.append(valid_videos[i+1])\n",
    "                    skip_next = True\n",
    "        else:\n",
    "            if not label_mismatch:\n",
    "                final_videos.append(valid_videos[i])\n",
    "    if len(valid_videos) > 0:\n",
    "        model_error += error_count/len(valid_videos)\n",
    "        model_error_match += match_error_count/len(valid_videos)\n",
    "    valid_videos = final_videos\n",
    "    valid_videos_list.append((cls, valid_videos))\n",
    "\n",
    "model_error /= len(sorted_videos_list)\n",
    "print(\"model error: \", model_error)\n",
    "\n",
    "model_error_match /= len(sorted_videos_list)\n",
    "print(\"model error match: \", model_error_match)\n",
    "\n",
    "p = figure(x_range=(26.98, 27.12), y_range=(12.78, 12.89),  width=550, height=550)\n",
    "p.image_url(url=['wald.jpg'], x=26.5, y=13, w=1, h=0.5)\n",
    "\n",
    "cam_source = ColumnDataSource(data=dict(x=x, y=y, text=id))\n",
    "glyph = Text(x=\"x\", y=\"y\", text=\"text\", text_align=\"center\", text_font_size=\"10pt\", text_color=\"white\")\n",
    "p.add_glyph(cam_source, glyph)\n",
    "print(valid_videos_list)\n",
    "valid_videos = valid_videos_list[10][1]\n",
    "\n",
    "source = ColumnDataSource(data=dict(x=[mapping[x][0] for x, _, _ in valid_videos], y=[mapping[x][1] for x, _, _ in valid_videos], time=[x.strftime('%Y-%m-%d %H:%M:%S') for _, x, _ in valid_videos]))\n",
    "source_visible = ColumnDataSource(data=dict(x=[mapping[valid_videos[0][0]][0]], y=[mapping[valid_videos[0][0]][1]], time=[valid_videos[0][1].strftime('%Y-%m-%d %H:%M:%S')]))\n",
    "p.scatter(x=\"x\", y=\"y\", size=10, source=source_visible, fill_color=\"red\", line_color=\"black\")\n",
    "\n",
    "div = Div(text=f\"<b>Time: {valid_videos[0][1]}</b>\")\n",
    "\n",
    "slider = Slider(start=0, end=len(valid_videos)-1, value=0, step=1, title=\"Time\")\n",
    "slider.js_on_change(\"value\", CustomJS(args=dict(source_visible=source_visible, source_avaliable=source, slider=slider, div=div), code=\"\"\"\n",
    "    var data_visible = source_visible.data;\n",
    "    var data = source_avaliable.data;\n",
    "    data_visible.x = [data.x[slider.value.toString()]];\n",
    "    data_visible.y = [data.y[slider.value.toString()]];\n",
    "    data_visible.time = data.time[slider.value.toString()];\n",
    "    div.text = \"<b>Time: \" + data_visible.time + \"</b>\";\n",
    "    source_visible.change.emit();\n",
    "\"\"\"))\n",
    "\n",
    "p.xaxis.axis_label = \"Longitude\"\n",
    "p.yaxis.axis_label = \"Latitude\"\n",
    "\n",
    "q = figure(x_range=(26.98, 27.12), y_range=(12.78, 12.89), width=550, height=550)\n",
    "q.image_url(url=['wald.jpg'], x=26.5, y=13, w=1, h=0.5)\n",
    "\n",
    "points = []\n",
    "for (camera_id, start_time, _) in valid_videos:\n",
    "    (long, lat) = mapping[camera_id]\n",
    "    if long is None or lat is None:\n",
    "        continue\n",
    "    points.append((long + np.random.normal(0.0005, 0.0015), lat + np.random.normal(0.0005, 0.0015)))\n",
    "        \n",
    "q.add_glyph(cam_source, glyph)\n",
    "\n",
    "x,y = zip(*points)\n",
    "\n",
    "q.scatter(x, y, size=10, fill_color=\"red\", line_color=\"black\")\n",
    "\n",
    "# plt.hist2d(x,y, bins=1000, cmap=\"twilight\", range=[[26.98, 27.12], [12.78, 12.89]])\n",
    "# plt.xlabel(\"Longitude (2.17km between ticks)\")\n",
    "# plt.ylabel(\"Latitude (2.22km between ticks)\")\n",
    "# plt.xlim(26.98, 27.12)\n",
    "# plt.ylim(12.78, 12.89)\n",
    "# plt.title(\"Tracklet distribution heatmap over camera positions\")\n",
    "# for i, (x, y) in mapping.items():\n",
    "#     if x is None or y is None:\n",
    "#         continue\n",
    "#     plt.text(x, y, f\"{i}\", ha=\"center\", fontsize=6)\n",
    "\n",
    "show(row(column(p, column(slider, div)), q))\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_starting_camera(valid_videos):\n",
    "    camera_counts = Counter([vid.camera_id for vid in valid_videos])\n",
    "    print(camera_counts)\n",
    "    return camera_counts.most_common(1)[0][0]\n",
    "\n",
    "distance_sorted_cameras_map = {}\n",
    "data_values_map = {}\n",
    "\n",
    "valid_camera_ids_map = {}\n",
    "for cls, valid_videos in valid_videos_list:\n",
    "    valid_camera_ids = list(dict.fromkeys([vid.camera_id for vid in valid_videos]))\n",
    "    valid_camera_ids_map[cls] = valid_camera_ids\n",
    "\n",
    "valid_cls_list = []\n",
    "for cls, valid_videos in valid_videos_list:\n",
    "    if(len(valid_videos) == 0):\n",
    "        continue\n",
    "    valid_cls_list.append(cls)\n",
    "    c_ids = valid_camera_ids_map[cls]\n",
    "    distance_sorted_cameras = []\n",
    "    \n",
    "    start = find_starting_camera(valid_videos)\n",
    "    distances = []\n",
    "    for c_id in c_ids:\n",
    "        x, y = mapping[start]\n",
    "        x2, y2 = mapping[c_id]\n",
    "        dist = geopy.distance.distance((y, x), (y2, x2)).km\n",
    "        distances.append((c_id, dist))\n",
    "    distances = sorted(distances, key = lambda t: t[1])\n",
    "    distance_sorted_cameras = [x for x, _ in distances]\n",
    "    distance_sorted_cameras_map[cls] = distance_sorted_cameras\n",
    "    \n",
    "    data_values_map[cls] = [(c_id, time) for c_id, time, _ in valid_videos]\n",
    "\n",
    "\n",
    "# x = [x.strftime('%Y-%m-%d-%H-%M-%S') for _, x in data_values_map[\"VI40\"]]\n",
    "# y = [str(y) for y, _ in data_values_map[\"VI40\"]]\n",
    "\n",
    "# fig = figure(x_range=x, y_range=list(dict.fromkeys(y)))\n",
    "\n",
    "# print(x)\n",
    "# print(y)\n",
    "\n",
    "# cds = ColumnDataSource(data={'x': x, 'y': y})\n",
    "\n",
    "# glyph = Line(x=\"x\", y=\"y\", line_color=\"black\", line_width=2)\n",
    "\n",
    "# fig.add_glyph(cds, glyph)\n",
    "# fig.xaxis.major_label_orientation = \"vertical\"\n",
    "\n",
    "# show(fig)\n",
    "\n",
    "def update_page(change):\n",
    "    cls = class_selector.value\n",
    "    x = [x.strftime('%Y-%m-%d-%H-%M-%S') for _, x in data_values_map[cls]]\n",
    "    y = [str(y) for y, _ in data_values_map[cls]]\n",
    "    \n",
    "    # Clear previous output\n",
    "    clear_output(wait=True)\n",
    "    display(class_selector)\n",
    "    \n",
    "    print(x)\n",
    "    print(y)\n",
    "    \n",
    "    fig = figure(x_range=x, y_range=list(map(str, distance_sorted_cameras_map[cls])))\n",
    "    cds = ColumnDataSource(data={'x': x, 'y': y})\n",
    "    glyph = Scatter(x=\"x\", y=\"y\", fill_color=\"black\", line_color=\"black\", size=7)\n",
    "    fig.add_glyph(cds, glyph)\n",
    "    fig.xaxis.major_label_orientation = \"vertical\"\n",
    "    \n",
    "    # Show the updated figure\n",
    "    show(fig)\n",
    "\n",
    "# Create a dropdown widget to select the class\n",
    "class_selector = widgets.Dropdown(\n",
    "    options=[cls for cls in valid_cls_list],\n",
    "    description=\"Class:\"\n",
    ")\n",
    "\n",
    "# Link the dropdown to the update_page function\n",
    "class_selector.observe(update_page, names='value')\n",
    "\n",
    "# Display the dropdown\n",
    "display(class_selector)\n",
    "\n",
    "# Initial plot\n",
    "update_page(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stmt = (select(Video.video_id, Video.camera_id, Video.start_time).where(Video.video_id.in_(video_ids)))\n",
    "\n",
    "with session_cls() as session:\n",
    "    result = session.execute(stmt).all()\n",
    "\n",
    "print(video_ids)\n",
    "print(result)\n",
    "\n",
    "dct = {cls: [] for cls in index_dict.keys()}\n",
    "for cls, indices in index_dict.items():\n",
    "    for idx in indices:\n",
    "        new_vid_ids = [x[0] for x in result]\n",
    "        index = new_vid_ids.index(video_ids[idx])\n",
    "        dct[cls].append(result[index].camera_id)\n",
    "        \n",
    "print(dct)\n",
    "\n",
    "social_groups = {cls[:2]: [] for cls in index_dict.keys()}\n",
    "\n",
    "for cls in index_dict.keys():\n",
    "    social_groups[cls[:2]].append(cls)\n",
    "    \n",
    "print(social_groups)\n",
    "\n",
    "def similarity(a, b):\n",
    "    counter1, counter2 = Counter(a), Counter(b)\n",
    "    score = 0\n",
    "    real_len = len(a)\n",
    "    for cam, count in counter1.items():\n",
    "        if cam not in counter2:\n",
    "            score += count\n",
    "        else:\n",
    "            score += abs(count - counter2[cam])\n",
    "    for cam, count in counter2.items():\n",
    "        if cam not in counter1:\n",
    "            score += count\n",
    "            real_len += 1\n",
    "    \n",
    "    score /= real_len\n",
    "    \n",
    "    return score\n",
    "\n",
    "grp = social_groups[\"US\"]\n",
    "print(grp)\n",
    "for i in range(len(grp)):\n",
    "    for j in range(i+1, len(grp)):\n",
    "        if i == j:\n",
    "            continue\n",
    "        print(grp[i], grp[j])\n",
    "        print(dct[grp[i]])\n",
    "        print(dct[grp[j]])\n",
    "        print(similarity(dct[grp[i]], dct[grp[j]]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_matrix = np.zeros((len(index_dict.keys()), len(index_dict.keys())))\n",
    "\n",
    "for i, cls1 in enumerate(sorted(index_dict.keys())):\n",
    "    for j, cls2 in enumerate(sorted(index_dict.keys())):\n",
    "        if i == j:\n",
    "            similarity_matrix[i, j] = np.inf\n",
    "            continue\n",
    "        similarity_matrix[i, j] = round(similarity(dct[cls1], dct[cls2]), 2)\n",
    "        \n",
    "print('\\n'.join(['\\t'.join([str(cell) for cell in row]) for row in similarity_matrix]))\n",
    "\n",
    "min_value = np.min(similarity_matrix)\n",
    "\n",
    "# Find the index of the minimum value\n",
    "min_index = np.unravel_index(np.argmin(similarity_matrix), similarity_matrix.shape)\n",
    "\n",
    "print(min_value, min_index, list(sorted(index_dict.keys()))[min_index[0]], list(sorted(index_dict.keys()))[min_index[1]])\n",
    "print(np.average(similarity_matrix[np.isfinite(similarity_matrix)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_grouping = {}\n",
    "\n",
    "valid_video_ids = {}\n",
    "for cls, videos in valid_videos_list:\n",
    "    for vid in videos:\n",
    "        if vid.video_id not in valid_video_ids:\n",
    "            valid_video_ids[vid.video_id] = []\n",
    "        valid_video_ids[vid.video_id].append(cls)\n",
    "        \n",
    "print(valid_video_ids)\n",
    "\n",
    "test_dict = {}\n",
    "for key in valid_video_ids.keys():\n",
    "    try:\n",
    "        test_dict[key] = (video_labels_map[key], valid_video_ids[key])\n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "print(test_dict)\n",
    "print(video_labels_map)\n",
    "\n",
    "same_video_individuals = {}\n",
    "for vid, cls_list in valid_video_ids.items():\n",
    "    for i in range(len(cls_list)):\n",
    "        # for j in range(len(cls_list)):\n",
    "        #     if i == j:\n",
    "        #         continue\n",
    "        #     if cls_list[i] not in same_video_individuals:\n",
    "        #         same_video_individuals[cls_list[i]] = set()\n",
    "        #     same_video_individuals[cls_list[i]].add(cls_list[j])\n",
    "        try:\n",
    "            same_video_individuals[cls_list[i]].update(cls_list)\n",
    "        except:\n",
    "            same_video_individuals[cls_list[i]] = Counter(cls_list)\n",
    "        \n",
    "        try:\n",
    "            new_grouping[cls_list[i][:2]].add(cls_list)\n",
    "        except:\n",
    "            new_grouping[cls_list[i][:2]] = set(cls_list)\n",
    "            \n",
    "print(same_video_individuals)\n",
    "\n",
    "print(new_grouping)\n",
    "for key in new_grouping.keys():\n",
    "    print(key, [x for x in same_video_individuals.keys() if x[:2] == key])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
