{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### imports and metric calc init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from itertools import islice\n",
    "from typing import Any, Dict, List, Literal, Optional\n",
    "\n",
    "import lightning as L\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import PIL\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import torch\n",
    "import torchmetrics as tm\n",
    "import tqdm\n",
    "import wandb\n",
    "from pytorch_grad_cam import GradCAM\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "from scipy import stats\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from torch.utils.data import DataLoader as Dataloader\n",
    "from torchmetrics.functional import pairwise_euclidean_distance\n",
    "from torchvision.transforms import ToPILImage\n",
    "\n",
    "import gorillatracker.type_helper as gtypes\n",
    "from gorillatracker.data.contrastive_sampler import ContrastiveKFoldValSampler, get_individual, get_individual_video_id\n",
    "from gorillatracker.data.nlet_dm import NletDataModule\n",
    "from gorillatracker.utils.labelencoder import LinearSequenceEncoder\n",
    "\n",
    "params = {\n",
    "    \"font.size\": 11,\n",
    "    \"font.family\": \"serif\",\n",
    "}\n",
    "\n",
    "\n",
    "plt.rcParams.update(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_partition_from_dataframe(\n",
    "    data: pd.DataFrame, partition: Literal[\"val\", \"train\", \"test\"] = \"val\"\n",
    ") -> tuple[pd.DataFrame, torch.Tensor, torch.Tensor, list[gtypes.Id], torch.Tensor]:\n",
    "    partition_df = data.where(data[\"partition\"] == partition).dropna()\n",
    "    partition_labels = torch.tensor(partition_df[\"label\"].tolist()).long()\n",
    "    partition_embeddings = np.stack(partition_df[\"embedding\"].apply(np.array)).astype(np.float32)\n",
    "    partition_embeddings = torch.tensor(partition_embeddings)\n",
    "    partition_ids = partition_df[\"id\"].tolist()\n",
    "    partition_encoded_labels = torch.tensor(partition_df[\"encoded_label\"].tolist()).long()\n",
    "\n",
    "    return partition_df, partition_labels, partition_embeddings, partition_ids, partition_encoded_labels\n",
    "\n",
    "\n",
    "def _get_crossvideo_masks(\n",
    "    labels: torch.Tensor, ids: list[gtypes.Id], min_samples: int = 3\n",
    ") -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    distance_mask = torch.zeros((len(labels), len(labels)))\n",
    "    classification_mask = torch.zeros(len(labels))\n",
    "\n",
    "    vids_per_id: defaultdict[str, defaultdict[str, int]] = defaultdict(\n",
    "        lambda: defaultdict(lambda: 0)\n",
    "    )  # NOTE: individual_id -> (individual_video_id -> num_images))\n",
    "    idx_per_vid: defaultdict[str, list[int]] = defaultdict(list)\n",
    "    for i, id in enumerate(ids):\n",
    "        individual_video_id = get_individual_video_id(id)\n",
    "        vids_per_id[get_individual(id)][individual_video_id] += 1\n",
    "        idx_per_vid[individual_video_id].append(i)\n",
    "\n",
    "    for i, id in enumerate(ids):\n",
    "        individual_video_id = get_individual_video_id(id)\n",
    "\n",
    "        distance_mask_ = [True] * len(ids)\n",
    "        for idx in idx_per_vid[individual_video_id]:\n",
    "            distance_mask_[idx] = False\n",
    "        distance_mask[i] = torch.tensor(distance_mask_)  # 1 if not same video, 0 if same video\n",
    "\n",
    "        if (\n",
    "            sum(vids_per_id[get_individual(id)].values()) - vids_per_id[get_individual(id)][individual_video_id]\n",
    "            >= min_samples\n",
    "        ):\n",
    "            classification_mask[i] = True\n",
    "\n",
    "    return distance_mask.bool(), classification_mask.bool()\n",
    "\n",
    "\n",
    "def knn(\n",
    "    data: pd.DataFrame,\n",
    "    average: Literal[\"micro\", \"macro\", \"weighted\", \"none\"] = \"weighted\",\n",
    "    k: int = 5,\n",
    "    use_train_embeddings: bool = False,\n",
    "    use_crossvideo_positives: bool = False,\n",
    "    distance_metric: Literal[\"euclidean\", \"cosine\"] = \"euclidean\",\n",
    "    use_filter: bool = False,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Algorithmic Description:\n",
    "    1. Calculate the distance matrix between all embeddings (len(embeddings) x len(embeddings))\n",
    "       Set the diagonal of the distance matrix to a large value so that the distance to itself is ignored\n",
    "    2. For each embedding find the k closest [smallest distances] embeddings (len(embeddings) x k)\n",
    "       First find the indexes, the map to the labels (numbers).\n",
    "    3. Create classification matrix where every embedding has a row with the probability for each class in it's top k surroundings (len(embeddings) x num_classes)\n",
    "    4. Select only the validation part of the classification matrix (len(val_embeddings) x num_classes)\n",
    "    5. Calculate the accuracy, accuracy_top5, auroc and f1 score: Either choose highest probability as class as matched class or check if any of the top 5 classes matches.\n",
    "    \"\"\"\n",
    "\n",
    "    # convert embeddings and labels to tensors\n",
    "    _, _, val_embeddings, val_ids, val_labels = get_partition_from_dataframe(data, partition=\"val\")\n",
    "    train_labels, train_embeddings = torch.Tensor([]), torch.Tensor([])\n",
    "    if use_train_embeddings:\n",
    "        _, _, train_embeddings, _, train_labels = get_partition_from_dataframe(data, partition=\"train\")\n",
    "\n",
    "    # NOTE(rob2u): k // 2 + 1 for majority +1 because one is classified\n",
    "    min_amount = k // 2 + 2 if use_filter else 0\n",
    "    val_labels_unique, val_labels_counts = torch.unique(val_labels, return_counts=True)\n",
    "\n",
    "    classification_mask = torch.zeros(\n",
    "        len(val_labels)\n",
    "    ).bool()  # NOTE(rob2u): mask to filter for classification metric calculation\n",
    "    classification_mask.fill_(True)\n",
    "\n",
    "    for label, count in zip(val_labels_unique, val_labels_counts):\n",
    "        if count < min_amount:\n",
    "            classification_mask[val_labels == label] = False\n",
    "\n",
    "    combined_embeddings = torch.cat([train_embeddings, val_embeddings], dim=0)\n",
    "    combined_labels = torch.cat([train_labels, val_labels], dim=0)\n",
    "\n",
    "    num_classes: int = int(torch.max(combined_labels).item() + 1)\n",
    "    assert num_classes == len(np.unique(combined_labels))\n",
    "    if num_classes < k:\n",
    "        k = num_classes\n",
    "\n",
    "    distance_matrix: torch.Tensor\n",
    "    if distance_metric == \"cosine\":\n",
    "        distance_matrix = (\n",
    "            torch.nn.functional.cosine_similarity(\n",
    "                combined_embeddings.unsqueeze(0), combined_embeddings.unsqueeze(1), dim=-1\n",
    "            )\n",
    "            * -1.0\n",
    "            + 1.0\n",
    "        )  # range [0, 2]\n",
    "    elif distance_metric == \"euclidean\":\n",
    "        distance_matrix = pairwise_euclidean_distance(combined_embeddings)  # range [0, inf]\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown distance metric: {distance_metric}\")\n",
    "\n",
    "    distance_matrix.fill_diagonal_(float(\"inf\"))\n",
    "\n",
    "    distance_mask: torch.Tensor  # NOTE(rob2u): mask to filter for distance calculation\n",
    "    samples_left = [\n",
    "        sum(val_labels == val_labels[i]).item() - 1 for i in range(len(val_labels)) if classification_mask[i]\n",
    "    ]\n",
    "    if use_crossvideo_positives:\n",
    "        distance_mask, classification_mask_cv = _get_crossvideo_masks(val_labels, val_ids)\n",
    "        samples_left = [\n",
    "            sum([distance_mask[i][j] for j in range(len(val_labels)) if val_labels[i] == val_labels[j]]).item()\n",
    "            for i in range(len(val_labels))\n",
    "            if classification_mask_cv[i]\n",
    "        ]\n",
    "\n",
    "        classification_mask = classification_mask & classification_mask_cv\n",
    "        if use_train_embeddings:  # add train embeddings to the distance mask (shapes would not match otherwise)\n",
    "            train_distance_mask = torch.ones((len(train_labels), len(train_labels) + len(val_labels)))\n",
    "            distance_mask = torch.cat([torch.ones((len(val_labels), len(train_labels))), distance_mask], dim=1)\n",
    "            distance_mask = torch.cat([train_distance_mask, distance_mask], dim=0)\n",
    "            distance_mask = distance_mask.bool()\n",
    "        distance_matrix[~distance_mask] = float(\"inf\")\n",
    "\n",
    "    _, closest_indices = torch.topk(\n",
    "        distance_matrix,\n",
    "        k,\n",
    "        largest=False,\n",
    "        sorted=True,\n",
    "    )\n",
    "    assert closest_indices.shape == (len(combined_embeddings), k)\n",
    "\n",
    "    closest_labels = combined_labels[closest_indices]\n",
    "    assert closest_labels.shape == closest_indices.shape\n",
    "\n",
    "    classification_matrix = torch.zeros((len(combined_embeddings), num_classes))\n",
    "    for i in range(num_classes):\n",
    "        classification_matrix[:, i] = torch.sum(closest_labels == i, dim=1) / k\n",
    "\n",
    "    # NOTE(rob2u): break ties by using the nearest neighbor (tie is when the the two closest neighbors have the same label)\n",
    "    for i in range(len(combined_embeddings)):\n",
    "        max_prob = torch.max(classification_matrix[i])\n",
    "        max_prob_indices = torch.where(max_prob - classification_matrix[i] < 1e-6)[0]\n",
    "\n",
    "        if len(max_prob_indices) == 1:\n",
    "            continue\n",
    "            # add 1e-6 to the closest indice of the max_prob_indices substract elsewhere (in max_prob_indices)\n",
    "\n",
    "        classification_matrix[i, max_prob_indices] += (1e-6) / len(max_prob_indices)\n",
    "        for j in range(k):\n",
    "            if closest_indices[i][j] in max_prob_indices:\n",
    "                classification_matrix[i][closest_labels[i][j].int()] += 1e-6\n",
    "                break\n",
    "\n",
    "    assert classification_matrix.shape == (len(combined_embeddings), num_classes)\n",
    "\n",
    "    # Select only the validation part of the classification matrix\n",
    "    val_classification_matrix = classification_matrix[-len(val_embeddings) :]\n",
    "\n",
    "    val_classification_matrix = val_classification_matrix[classification_mask]\n",
    "    val_labels = val_labels[classification_mask]\n",
    "\n",
    "    return val_classification_matrix, val_labels, samples_left"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gorillatracker.model.wrappers_supervised import BaseModuleSupervised\n",
    "\n",
    "model = BaseModuleSupervised.load_from_checkpoint(\n",
    "    \"/workspaces/gorillatracker/models/roberts_models/gorillas_models/vit_large_dinov2_bayes/fold-0-epoch-19-cxlkfold/fold-0/val/embeddings/knn5_crossvideo/accuracy-0.63.ckpt\",\n",
    "    data_module=None,\n",
    "    strict=False,\n",
    ").model_wrapper\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### init cxl-dataset for fold-0 and bristol dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from gorillatracker.data.nlet import SupervisedKFoldDataset, SupervisedDataset\n",
    "from gorillatracker.data.builder import build_onelet\n",
    "from torchvision.transforms import ToTensor, Compose, Resize, Normalize\n",
    "from gorillatracker.transform_utils import SquarePad\n",
    "\n",
    "transformations = Compose(\n",
    "    [  # NOTE(rob2u): Square Padding and to Tensor is applied in the dataset\n",
    "        Resize((224, 224)),\n",
    "        Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "cxl_dataset_fold_0_train = SupervisedKFoldDataset(\n",
    "    Path(\"/workspaces/gorillatracker/data/supervised/splits/cxl_faces_openset_seed42_square_kfold-5\"),\n",
    "    build_onelet,\n",
    "    \"train\",\n",
    "    0,\n",
    "    5,\n",
    "    transformations,\n",
    "    aug_num_ops=0,\n",
    "    aug_magnitude=0,\n",
    ")\n",
    "\n",
    "cxl_dataset_fold_0_val = SupervisedKFoldDataset(\n",
    "    Path(\"/workspaces/gorillatracker/data/supervised/splits/cxl_faces_openset_seed42_square_kfold-5\"),\n",
    "    build_onelet,\n",
    "    \"test\",\n",
    "    0,\n",
    "    5,\n",
    "    transformations,\n",
    "    aug_num_ops=0,\n",
    "    aug_magnitude=0,\n",
    ")\n",
    "\n",
    "bristol_dataset = SupervisedDataset(\n",
    "    Path(\n",
    "        \"/workspaces/gorillatracker/data/supervised/bristol/cross_encounter_validation/cropped_frames_square_filtered\"\n",
    "    ),\n",
    "    build_onelet,\n",
    "    \"val\",  # NOTE(rob2u): we have specified a directory without a val directory therefore all the data is used for validation\n",
    "    transformations,\n",
    "    aug_num_ops=0,\n",
    "    aug_magnitude=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bristol_labels = []\n",
    "bristol_embeddings = []\n",
    "bristol_ids = []\n",
    "\n",
    "for sample in bristol_dataset:\n",
    "    sample_id = sample[0][0]\n",
    "    sample_img = sample[1][0]\n",
    "    sample_label = sample[2][0]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        embedding = model(sample_img.unsqueeze(0).cuda()).cpu().squeeze(0).numpy()\n",
    "\n",
    "    bristol_labels.append(sample_label)\n",
    "    bristol_embeddings.append(embedding)\n",
    "    bristol_ids.append(sample_id)\n",
    "\n",
    "bristol_df = pd.DataFrame(\n",
    "    {\n",
    "        \"label\": bristol_labels,\n",
    "        \"embedding\": bristol_embeddings,\n",
    "        \"id\": bristol_ids,\n",
    "        \"partition\": \"val\",\n",
    "        \"dataset\": \"bristol\",\n",
    "    }\n",
    ")\n",
    "\n",
    "cxl_val_labels = []\n",
    "cxl_val_embeddings = []\n",
    "cxl_val_ids = []\n",
    "\n",
    "for sample in cxl_dataset_fold_0_val:\n",
    "    sample_id = sample[0][0]\n",
    "    sample_img = sample[1][0]\n",
    "    sample_label = sample[2][0]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        embedding = model(sample_img.unsqueeze(0).cuda()).cpu().squeeze(0).numpy()\n",
    "\n",
    "    cxl_val_labels.append(sample_label)\n",
    "    cxl_val_embeddings.append(embedding)\n",
    "    cxl_val_ids.append(sample_id)\n",
    "\n",
    "\n",
    "cxl_val_df = pd.DataFrame(\n",
    "    {\n",
    "        \"label\": cxl_val_labels,\n",
    "        \"embedding\": cxl_val_embeddings,\n",
    "        \"id\": cxl_val_ids,\n",
    "        \"partition\": \"val\",\n",
    "        \"dataset\": \"cxl\",\n",
    "    }\n",
    ")\n",
    "\n",
    "cxl_train_labels = []\n",
    "cxl_train_embeddings = []\n",
    "cxl_train_ids = []\n",
    "\n",
    "for sample in cxl_dataset_fold_0_train:\n",
    "    sample_id = sample[0][0]\n",
    "    sample_img = sample[1][0]\n",
    "    sample_label = sample[2][0]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        embedding = model(sample_img.unsqueeze(0).cuda()).cpu().squeeze(0).numpy()\n",
    "\n",
    "    cxl_train_labels.append(sample_label)\n",
    "    cxl_train_embeddings.append(embedding)\n",
    "    cxl_train_ids.append(sample_id)\n",
    "\n",
    "cxl_train_df = pd.DataFrame(\n",
    "    {\n",
    "        \"label\": cxl_train_labels,\n",
    "        \"embedding\": cxl_train_embeddings,\n",
    "        \"id\": cxl_train_ids,\n",
    "        \"partition\": \"train\",\n",
    "        \"dataset\": \"cxl\",\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "# encode all labels\n",
    "from gorillatracker.utils.labelencoder import LinearSequenceEncoder\n",
    "\n",
    "label_encoder = LinearSequenceEncoder()\n",
    "bristol_df[\"encoded_label\"] = label_encoder.encode_list(bristol_df[\"label\"].tolist())\n",
    "label_encoder_2 = LinearSequenceEncoder()\n",
    "cxl_val_df[\"encoded_label\"] = label_encoder_2.encode_list(cxl_val_df[\"label\"].tolist())\n",
    "label_encoder_3 = LinearSequenceEncoder()\n",
    "cxl_train_df[\"encoded_label\"] = label_encoder_3.encode_list(cxl_train_df[\"label\"].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "cxl_val_predictions, cxl_val_labels, samples_left = knn(\n",
    "    cxl_val_df,\n",
    "    k=5,\n",
    "    use_train_embeddings=False,\n",
    "    use_crossvideo_positives=False,\n",
    "    distance_metric=\"euclidean\",\n",
    "    use_filter=True,\n",
    ")\n",
    "\n",
    "samples_left = np.array(samples_left)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(np.unique(cxl_val_df[\"label\"].tolist()))\n",
    "\n",
    "accuracies = tm.functional.accuracy(\n",
    "    cxl_val_predictions, cxl_val_labels, task=\"multiclass\", num_classes=num_classes, average=\"none\"\n",
    ")\n",
    "\n",
    "bad_macro_accuracy = tm.functional.accuracy(\n",
    "    cxl_val_predictions, cxl_val_labels, task=\"multiclass\", num_classes=num_classes, average=\"macro\"\n",
    ")\n",
    "\n",
    "weighted_accuracy = tm.functional.accuracy(\n",
    "    cxl_val_predictions, cxl_val_labels, task=\"multiclass\", num_classes=num_classes, average=\"weighted\"\n",
    ")\n",
    "print(\"Bad Macro Accuracy: \", bad_macro_accuracy)\n",
    "print(\"Weighted Accuracy: \", weighted_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calculate correct macro accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_2_count_acc = {}\n",
    "\n",
    "for label in np.unique(cxl_val_df[\"encoded_label\"].tolist()):\n",
    "    label_count = sum(cxl_val_df[\"encoded_label\"].tolist() == label)\n",
    "    label_count_left = sum(cxl_val_labels == label).item()\n",
    "    acc = accuracies[label].item()\n",
    "    label_2_count_acc[label] = (label_count, label_count_left, acc)\n",
    "\n",
    "macro_acc = sum([acc for _, left, acc in label_2_count_acc.values() if left > 0]) / sum(\n",
    "    [1 for _, left, _ in label_2_count_acc.values() if left > 0]\n",
    ")\n",
    "weighted_acc = sum([acc * left for _, left, acc in label_2_count_acc.values()]) / sum(\n",
    "    [left for _, left, _ in label_2_count_acc.values() if left > 0]\n",
    ")\n",
    "\n",
    "print(\"Macro Accuracy: \", macro_acc)\n",
    "print(\"Weighted Accuracy: \", weighted_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(samples_left)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buckets = [5, 10, 20, 30, 10000]\n",
    "true_rate_buckets = {\n",
    "    5: [],\n",
    "    10: [],\n",
    "    20: [],\n",
    "    30: [],\n",
    "    10000: [],\n",
    "}\n",
    "\n",
    "for samples_left_, cxl_val_prediction, cxl_val_label in zip(samples_left, cxl_val_predictions, cxl_val_labels):\n",
    "    pred = torch.argmax(cxl_val_prediction).item()\n",
    "\n",
    "    # get bucket\n",
    "    for bucket in buckets:\n",
    "        if samples_left_ <= bucket:\n",
    "            true_rate_buckets[bucket].append(pred == cxl_val_label.item())\n",
    "            break\n",
    "\n",
    "\n",
    "true_rate_buckets_avg = {\n",
    "    bucket: np.mean(values) if len(values) > 0 else 0.0 for bucket, values in true_rate_buckets.items()\n",
    "}\n",
    "\n",
    "plt.bar(\n",
    "    range(len(true_rate_buckets_avg)),\n",
    "    list(true_rate_buckets_avg.values()),\n",
    "    align=\"center\",\n",
    "    color=\"orange\",\n",
    "    edgecolor=\"black\",\n",
    ")\n",
    "plt.xticks(range(len(true_rate_buckets_avg)), [\"3 - 5\", \"6 - 10\", \"11 - 20\", \"21 - 30\", \"> 30\"])\n",
    "plt.grid(axis=\"y\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.xlabel(\"Number of samples used for classification\")\n",
    "sns.despine()\n",
    "# plt.savefig(\"plots/macro/true_rate_knn5_CROSSVIDEO_fold0.pdf\", bbox_inches=\"tight\", dpi=500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot accuracy vs average samples left for this class\n",
    "samples_left_avg = {\n",
    "    label: (\n",
    "        sum([samples_left[i] for i in range(len(cxl_val_labels)) if cxl_val_labels[i] == label])\n",
    "        / sum([1 for i in range(len(cxl_val_labels)) if cxl_val_labels[i] == label])\n",
    "        if sum(cxl_val_labels == label) > 0\n",
    "        else 0\n",
    "    )\n",
    "    for label in np.unique(cxl_val_df[\"encoded_label\"].tolist())\n",
    "}\n",
    "\n",
    "\n",
    "samples_left_avg = [samples_left_avg[label] for label in np.unique(cxl_val_df[\"encoded_label\"].tolist())]\n",
    "\n",
    "# filter to only keep the labels that have samples left\n",
    "samples_left_avg, accuracies_samples_left = zip(\n",
    "    *[(samples_left_avg[i], accuracies[i].item()) for i in range(len(samples_left_avg)) if sum(cxl_val_labels == i) > 0]\n",
    ")\n",
    "\n",
    "plt.scatter(samples_left_avg, accuracies_samples_left, color=\"darkred\", marker=\"x\", s=100)\n",
    "plt.grid()\n",
    "plt.xlabel(\"Average Samples Left for classification\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "sns.despine()\n",
    "# plt.savefig(\"plots/macro/accuracy_vs_samples_left_knn5_CROSSVIDEO_fold0.pdf\", bbox_inches=\"tight\", dpi=500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot accuracy vs samples (not left but total) for class\n",
    "\n",
    "samples_total = {\n",
    "    label: sum(cxl_val_labels == label).item() for label in np.unique(cxl_val_df[\"encoded_label\"].tolist())\n",
    "}\n",
    "\n",
    "samples_total = [samples_total[label] for label in np.unique(cxl_val_df[\"encoded_label\"].tolist())]\n",
    "\n",
    "samples_total, accuracies_samples_total = zip(\n",
    "    *[(samples_total[i], accuracies[i]) for i in range(len(samples_total)) if sum(cxl_val_labels == i) > 0]\n",
    ")\n",
    "\n",
    "plt.scatter(samples_total, accuracies_samples_total, color=\"darkred\", marker=\"x\", s=100)\n",
    "plt.grid()\n",
    "sns.despine()\n",
    "plt.xlabel(\"Samples per class\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.savefig(\"plots/macro/accuracy_vs_samples_classified_knn5_test.pdf\", bbox_inches=\"tight\", dpi=500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
