{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import Isomap, LocallyLinearEmbedding, MDS, SpectralEmbedding, TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import umap.umap_ as umap\n",
    "import numpy as np\n",
    "import wandb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from io import BytesIO\n",
    "import base64\n",
    "from bokeh.plotting import figure, show\n",
    "from bokeh.models import ColumnDataSource, HoverTool\n",
    "from bokeh.io import output_notebook\n",
    "from bokeh.resources import INLINE\n",
    "import colorcet as cc\n",
    "import pandas as pd\n",
    "from gorillatracker.transform_utils import SquarePad\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "output_notebook(INLINE)\n",
    "\n",
    "\n",
    "class EmbeddingProjector:\n",
    "    def __init__(self):\n",
    "        self.algorithms = {\n",
    "            \"tsne\": TSNE(n_components=2),\n",
    "            \"isomap\": Isomap(n_components=2),\n",
    "            \"lle\": LocallyLinearEmbedding(n_components=2),\n",
    "            \"mds\": MDS(n_components=2),\n",
    "            \"spectral\": SpectralEmbedding(n_components=2),\n",
    "            \"pca\": PCA(n_components=2),\n",
    "            \"umap\": umap.UMAP(),\n",
    "        }\n",
    "\n",
    "    def reduce_dimensions(self, embeddings, method=\"tsne\"):\n",
    "        # handle --fast_dev_run where there is a reduced number of embeddings\n",
    "        assert len(embeddings) > 2\n",
    "        algorithm = TSNE(n_components=2, perplexity=1)\n",
    "        if len(embeddings) > 30:\n",
    "            algorithm = self.algorithms.get(method, TSNE(n_components=2))\n",
    "        return algorithm.fit_transform(embeddings)\n",
    "\n",
    "    def plot_clusters(\n",
    "        self, low_dim_embeddings, labels, og_labels, images, title=\"Embedding Projector\", figsize=(12, 10)\n",
    "    ):\n",
    "        color_names = cc.glasbey\n",
    "        color_lst = [color_names[label * 2] for label in labels]\n",
    "        data = {\n",
    "            \"x\": low_dim_embeddings[:, 0],\n",
    "            \"y\": low_dim_embeddings[:, 1],\n",
    "            \"color\": color_lst,\n",
    "            \"class\": og_labels,\n",
    "            \"image\": images,\n",
    "        }\n",
    "\n",
    "        fig = figure(tools=\"pan, wheel_zoom, box_zoom, reset\")\n",
    "        fig.scatter(\n",
    "            x=\"x\",\n",
    "            y=\"y\",\n",
    "            size=12,\n",
    "            fill_color=\"color\",\n",
    "            line_color=\"black\",\n",
    "            source=ColumnDataSource(data=data),\n",
    "            legend_field=\"class\",\n",
    "        )\n",
    "\n",
    "        hover = HoverTool(tooltips='<img src=\"data:image/jpeg;base64,@image\" width=\"128\" height=\"128\">')\n",
    "        fig.add_tools(hover)\n",
    "\n",
    "        output_notebook()\n",
    "        show(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Embeddings by loading a model an running it on a val dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gorillatracker.data.builder import build_onelet\n",
    "from gorillatracker.data.nlet import SupervisedDataset\n",
    "from gorillatracker.model.wrappers_supervised import BaseModuleSupervised, TimmEvalWrapper\n",
    "import pandas as pd\n",
    "from torchvision import transforms\n",
    "from gorillatracker.transform_utils import SquarePad\n",
    "from pathlib import Path\n",
    "\n",
    "val_ds_path = Path(\"/workspaces/gorillatracker/data/supervised/splits/cxl_faces_openset_seed42_square_kfold-5/fold-0\")\n",
    "# cpkt_path = \"/workspaces/gorillatracker/models/roberts_models/gorillas_loss/vit_base_loss_hard/fold-0-epoch-9-cxlkfold/fold-0/val/embeddings/knn5_crossvideo/accuracy-0.63.ckpt\"\n",
    "# model = BaseModuleSupervised.load_from_checkpoint(checkpoint_path=cpkt_path, data_module=None, wandb_run=None)\n",
    "# model.eval()\n",
    "\n",
    "model = TimmEvalWrapper(\n",
    "    backbone_name=\"vit_large_patch14_dinov2.lvd142m\",\n",
    "    img_size=224,\n",
    ")\n",
    "model.to(\"cuda\")\n",
    "\n",
    "df = pd.DataFrame(columns=[\"label\", \"image\", \"embedding\"])\n",
    "dataset = SupervisedDataset(\n",
    "    base_dir=val_ds_path,\n",
    "    nlet_builder=build_onelet,\n",
    "    partition=\"val\",\n",
    "    transform=transforms.Compose(  # use the transforms that were used for the model (except of course data augmentations)\n",
    "        [\n",
    "            transforms.Resize((224, 224)),\n",
    "        ]\n",
    "    ),\n",
    "    aug_num_ops=0,\n",
    "    aug_magnitude=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "from gorillatracker.utils.labelencoder import LabelEncoder\n",
    "\n",
    "for i in tqdm.tqdm(range(len(dataset))):\n",
    "    ids, image_tensors, labels = dataset[i]\n",
    "    img_id = ids[0]\n",
    "    image_tensor = image_tensors[0]\n",
    "    label = labels[0]\n",
    "\n",
    "    label_string = LabelEncoder.decode(label)\n",
    "    image = transforms.ToPILImage()(image_tensor)\n",
    "    image_tensor = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.228, 0.224, 0.225])(\n",
    "        image_tensor\n",
    "    )  # if your model was trained with normalization, you need to normalize the images here as well\n",
    "    image_tensor = image_tensor.to(\"cuda\")\n",
    "\n",
    "    embedding = model(image_tensor.unsqueeze(0))\n",
    "    df = pd.concat(\n",
    "        [\n",
    "            df,\n",
    "            pd.DataFrame(\n",
    "                {\n",
    "                    \"label_string\": [label_string],\n",
    "                    \"label\": [label],\n",
    "                    \"image\": [image],\n",
    "                    \"embedding\": [embedding[0].detach().cpu().numpy()],\n",
    "                }\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "df = df.reset_index(drop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import base64\n",
    "\n",
    "labels = df[\"label\"].to_numpy()\n",
    "embeddings = df[\"embedding\"].to_numpy()\n",
    "embeddings = np.stack(embeddings)\n",
    "\n",
    "pil_images = df[\"image\"]\n",
    "images = []\n",
    "for image in pil_images:\n",
    "    buffer = BytesIO()\n",
    "    image.save(buffer, format=\"JPEG\")\n",
    "    image_byte = base64.b64encode(buffer.getvalue()).decode(\"utf-8\")\n",
    "    images.append(image_byte)\n",
    "\n",
    "ep = EmbeddingProjector()\n",
    "low_dim_embeddings = ep.reduce_dimensions(embeddings, method=\"umap\")\n",
    "ep.plot_clusters(low_dim_embeddings, labels, df[\"label_string\"], images, title=\"Embedding Projector\", figsize=(20, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maybe a better overview of the data when len(individual) > 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out classes that have less than 3 images\n",
    "df_filtered = df.groupby(\"label\").filter(lambda x: len(x) > 3)\n",
    "\n",
    "labels_filtered = df_filtered[\"label\"].to_numpy()\n",
    "embeddings_filtered = df_filtered[\"embedding\"].to_numpy()\n",
    "embeddings_filtered = np.stack(embeddings_filtered)\n",
    "\n",
    "pil_images_filtered = df_filtered[\"image\"]\n",
    "images = []\n",
    "for image in pil_images_filtered:\n",
    "    buffer = BytesIO()\n",
    "    image.save(buffer, format=\"JPEG\")\n",
    "    image_byte = base64.b64encode(buffer.getvalue()).decode(\"utf-8\")\n",
    "    images.append(image_byte)\n",
    "\n",
    "ep = EmbeddingProjector()\n",
    "low_dim_embeddings = ep.reduce_dimensions(embeddings_filtered, method=\"tsne\")\n",
    "ep.plot_clusters(\n",
    "    low_dim_embeddings,\n",
    "    labels_filtered,\n",
    "    df_filtered[\"label_string\"],\n",
    "    images,\n",
    "    title=\"Embedding Projector\",\n",
    "    figsize=(12, 10),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now add Grad-CAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchmetrics as tm\n",
    "\n",
    "\n",
    "# calculate weights by averaging embeddings of each class (if no arcface was used then donot use this function)\n",
    "def get_weights(embeddings, labels):\n",
    "    embeddings = torch.tensor(embeddings)\n",
    "    num_classes = 29  # NOTE: this is hardcoded for now\n",
    "    weights = torch.zeros((num_classes, 128))\n",
    "    for i in range(num_classes):\n",
    "        weights[i] = torch.mean(embeddings[labels == i], dim=0)\n",
    "    weights = torch.nn.functional.normalize(weights, dim=1)\n",
    "    return weights\n",
    "\n",
    "\n",
    "def fc_layer2(\n",
    "    embeddings,\n",
    "    labels,\n",
    "    s=64.0,\n",
    "    **kwargs,\n",
    "):\n",
    "    num_classes = 29  # NOTE: this is hardcoded for now\n",
    "\n",
    "    embeddings = torch.tensor(embeddings)\n",
    "    labels = torch.tensor(labels)\n",
    "\n",
    "    # calculate weights by averaging embeddings of each class\n",
    "    weights = get_weights(embeddings, labels)\n",
    "\n",
    "    weights = torch.nn.functional.normalize(weights, dim=1)\n",
    "    embeddings = torch.nn.functional.normalize(embeddings, dim=1)\n",
    "    output = torch.nn.functional.linear(embeddings, weights)\n",
    "    output = output * s\n",
    "    final_output = torch.nn.functional.softmax(output, dim=1)\n",
    "\n",
    "    preds = torch.max(final_output, dim=1)[1]\n",
    "\n",
    "    accuracy = tm.functional.accuracy(\n",
    "        final_output, labels, task=\"multiclass\", num_classes=num_classes, average=\"weighted\"\n",
    "    )\n",
    "    assert accuracy is not None\n",
    "    accuracy_top5 = tm.functional.accuracy(final_output, labels, task=\"multiclass\", num_classes=num_classes, top_k=5)\n",
    "    assert accuracy_top5 is not None\n",
    "    auroc = tm.functional.auroc(final_output, labels, task=\"multiclass\", num_classes=num_classes)\n",
    "    assert auroc is not None\n",
    "    f1 = tm.functional.f1_score(final_output, labels, task=\"multiclass\", num_classes=num_classes, average=\"weighted\")\n",
    "    assert f1 is not None\n",
    "    return (\n",
    "        {\n",
    "            \"accuracy\": accuracy.item(),\n",
    "            \"accuracy_top5\": accuracy_top5.item(),\n",
    "            \"auroc\": auroc.item(),\n",
    "            \"f1\": f1.item(),\n",
    "        },\n",
    "        weights,\n",
    "        preds,\n",
    "        labels,\n",
    "    )\n",
    "\n",
    "\n",
    "labels = df[\"label\"].to_numpy().astype(int)\n",
    "results, weights, preds, labels = fc_layer2(embeddings, labels)\n",
    "\n",
    "\n",
    "false_predictions_indices = np.where(preds != labels)[0]\n",
    "false_predictions = df.iloc[false_predictions_indices]\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get some insights into the weights (can be skipped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "weights_np = weights.detach().numpy()\n",
    "weights_df = pd.DataFrame(weights_np)\n",
    "print(weights_df.head())\n",
    "\n",
    "sns.set_theme(style=\"white\")\n",
    "ax = sns.boxplot(data=weights_df.to_numpy(), orient=\"h\", palette=\"Set2\")\n",
    "ax.set_title(\"Distribution of weights in the final layer\")\n",
    "\n",
    "weights_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply on Grad-Cam with correctly weighted embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_grad_cam import GradCAM\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "import PIL\n",
    "\n",
    "\n",
    "def get_grad_cam_layer():\n",
    "    return model.model.features[-1][-1]\n",
    "\n",
    "\n",
    "def log_grad_cam_images_to_wandb(model, samples, transform=lambda x: x) -> None:  # TODO\n",
    "    # NOTE(liamvdv): inverse grad cam support to model since we might not be using\n",
    "    #                a model which grad cam does not support.\n",
    "    # NOTE(liamvdv): Transform models may have different interpretations.\n",
    "    # print(samples)\n",
    "    assert model is not None, \"Must only call log_grad_cam_images... after model was initialized.\"\n",
    "\n",
    "    target_layer = get_grad_cam_layer()\n",
    "    get_reshape_transform = getattr(model, \"get_grad_cam_reshape_transform\", lambda: None)\n",
    "    cam = GradCAM(model=model, target_layers=[target_layer], reshape_transform=get_reshape_transform())\n",
    "\n",
    "    images_pred = []\n",
    "    images_target = []\n",
    "    for sample in samples:\n",
    "        # a row (nlet) can either be (ap, p, n) OR (ap, p, n, an)\n",
    "        img, label = sample\n",
    "        img_to_fit = transform(img)\n",
    "        grayscale_cam = cam(input_tensor=img_to_fit.unsqueeze(0), targets=label, aug_smooth=False, eigen_smooth=True)\n",
    "        #\n",
    "        # Overlay heatmap on original image\n",
    "        heatmap = grayscale_cam[0, :]\n",
    "        image = np.array(transforms.ToPILImage()(img)).astype(np.float32) / 255.0  # NOTE(liamvdv): needs be normalized\n",
    "        image_with_heatmap = show_cam_on_image(image, heatmap, use_rgb=True)\n",
    "        images_pred.append(PIL.Image.fromarray(image_with_heatmap))\n",
    "    return images_pred\n",
    "\n",
    "\n",
    "# NOTE: this is a custom output target that is necessary to use gradcam with arcface if no arcface was used then you can use no target as well\n",
    "class CustomOutputTarget:\n",
    "    def __init__(self, category):\n",
    "        self.category = category\n",
    "        self.weights = get_weights(embeddings, labels)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        if len(x.shape) == 1:\n",
    "            x = x.unsqueeze(0)\n",
    "\n",
    "        x = torch.nn.functional.normalize(x, dim=1)\n",
    "        output = torch.nn.functional.linear(x, weights)\n",
    "        output = output * 64.0\n",
    "        x = torch.nn.functional.softmax(output, dim=1)\n",
    "        return x[:, self.category]\n",
    "\n",
    "\n",
    "amount = 10\n",
    "imgs = []\n",
    "for index in range(amount):\n",
    "    sample_imgs = transforms.ToTensor()(df[\"image\"][index])\n",
    "    sample_labels = torch.tensor([df[\"label\"][index]])\n",
    "\n",
    "    # one hot encode labels\n",
    "    sample_labels = [CustomOutputTarget(sample_labels[0])]\n",
    "\n",
    "    samples = [(sample_imgs, sample_labels)]\n",
    "    imgs.append(\n",
    "        log_grad_cam_images_to_wandb(\n",
    "            model,\n",
    "            samples,\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.228, 0.224, 0.225]),\n",
    "        )\n",
    "    )\n",
    "\n",
    "imgs = [img for sublist in imgs for img in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to pil images and show with matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(amount, 2, figsize=(15, 7 * amount))\n",
    "\n",
    "for index in range(amount):\n",
    "    axs[index, 0].imshow(imgs[index])\n",
    "    axs[index, 1].imshow(df[\"image\"][index])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now show the images that were wrongly classified\n",
    "amount = len(false_predictions_indices)\n",
    "print(amount)\n",
    "imgs = []\n",
    "for index in false_predictions_indices[:amount]:\n",
    "    sample_imgs = transforms.ToTensor()(df[\"image\"][index])\n",
    "    sample_labels = torch.tensor([df[\"label\"][index]])\n",
    "\n",
    "    # one hot encode labels\n",
    "    sample_labels = [CustomOutputTarget(sample_labels[0])]\n",
    "\n",
    "    samples = [(sample_imgs, sample_labels)]\n",
    "    imgs.append(\n",
    "        log_grad_cam_images_to_wandb(\n",
    "            model,\n",
    "            samples,\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.228, 0.224, 0.225]),\n",
    "        )\n",
    "    )\n",
    "\n",
    "imgs = [img for sublist in imgs for img in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to pil images and show with matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(amount, 2, figsize=(15, 7 * amount))\n",
    "\n",
    "for index in range(amount):\n",
    "    axs[index, 0].imshow(imgs[index])\n",
    "    # show image of individual it was wrongly classified as\n",
    "    axs[index, 1].imshow(df[\"image\"][preds[false_predictions_indices[index]].item()])\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
