{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN, KMeans, AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "def apply_dbscan(embeddings, eps=0.5, min_samples=5):\n",
    "    return DBSCAN(eps=eps, min_samples=min_samples).fit_predict(embeddings)\n",
    "\n",
    "def apply_kmeans(embeddings, n_clusters):\n",
    "    return KMeans(n_clusters=n_clusters, random_state=42).fit_predict(embeddings)\n",
    "\n",
    "def apply_agglomerative(embeddings, n_clusters):\n",
    "    return AgglomerativeClustering(n_clusters=n_clusters).fit_predict(embeddings)\n",
    "\n",
    "def compute_metrics(embeddings, labels):\n",
    "    metrics = {\n",
    "        \"silhouette_score\": silhouette_score(embeddings, labels),\n",
    "        \"calinski_harabasz_score\": calinski_harabasz_score(embeddings, labels),\n",
    "        \"davies_bouldin_score\": davies_bouldin_score(embeddings, labels)\n",
    "    }\n",
    "    \n",
    "    # Compute intra-cluster distances\n",
    "    unique_labels = np.unique(labels)\n",
    "    intra_cluster_distances = []\n",
    "    for label in unique_labels:\n",
    "        cluster_points = embeddings[labels == label]\n",
    "        if len(cluster_points) > 1:\n",
    "            distances = cdist(cluster_points, cluster_points)\n",
    "            intra_cluster_distances.extend(distances[np.triu_indices(len(distances), k=1)])\n",
    "    \n",
    "    metrics[\"avg_intra_cluster_distance\"] = np.mean(intra_cluster_distances)\n",
    "    metrics[\"max_intra_cluster_distance\"] = np.max(intra_cluster_distances)\n",
    "    \n",
    "    # Compute inter-cluster distances\n",
    "    centroids = np.array([embeddings[labels == label].mean(axis=0) for label in unique_labels])\n",
    "    inter_cluster_distances = cdist(centroids, centroids)\n",
    "    metrics[\"min_inter_cluster_distance\"] = np.min(inter_cluster_distances[np.triu_indices(len(inter_cluster_distances), k=1)])\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def analyze_subclusters(embeddings, labels, true_labels):\n",
    "    unique_true_labels = np.unique(true_labels)\n",
    "    subcluster_analysis = {}\n",
    "    \n",
    "    for true_label in unique_true_labels:\n",
    "        mask = true_labels == true_label\n",
    "        sub_embeddings = embeddings[mask]\n",
    "        sub_labels = labels[mask]\n",
    "        \n",
    "        if len(np.unique(sub_labels)) > 1:\n",
    "            sub_metrics = compute_metrics(sub_embeddings, sub_labels)\n",
    "            subcluster_analysis[true_label] = sub_metrics\n",
    "    \n",
    "    return subcluster_analysis\n",
    "\n",
    "def run_clustering(df, algorithms):\n",
    "    embeddings = np.stack(df[\"embedding\"].to_numpy())\n",
    "    true_labels = df[\"label\"].to_numpy()\n",
    "    results = {}\n",
    "    \n",
    "    for name, func, params in algorithms:\n",
    "        if name == \"k-means\":\n",
    "            # Grid search for optimal k\n",
    "            param_grid = {\"n_clusters\": range(2, 21)}  # Adjust range as needed\n",
    "            grid_search = GridSearchCV(KMeans(random_state=42), param_grid, cv=5, scoring=\"silhouette_score\")\n",
    "            grid_search.fit(embeddings)\n",
    "            best_k = grid_search.best_params_[\"n_clusters\"]\n",
    "            labels = func(embeddings, best_k)\n",
    "        else:\n",
    "            labels = func(embeddings, **params)\n",
    "        \n",
    "        metrics = compute_metrics(embeddings, labels)\n",
    "        subcluster_analysis = analyze_subclusters(embeddings, labels, true_labels)\n",
    "        \n",
    "        results[name] = {\n",
    "            \"metrics\": metrics,\n",
    "            \"subcluster_analysis\": subcluster_analysis\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Usage\n",
    "df = pd.read_pickle(\"spac.pkl\")  # Load your DataFrame\n",
    "\n",
    "algorithms = [\n",
    "    (\"DBSCAN\", apply_dbscan, {\"eps\": 0.5, \"min_samples\": 5}),\n",
    "    (\"k-means\", apply_kmeans, {}),  # params will be determined by grid search\n",
    "    (\"Agglomerative\", apply_agglomerative, {\"n_clusters\": 10})  # Adjust n_clusters as needed\n",
    "]\n",
    "\n",
    "results = run_clustering(df, algorithms)\n",
    "\n",
    "# Print or save results\n",
    "for algorithm, result in results.items():\n",
    "    print(f\"\\nResults for {algorithm}:\")\n",
    "    print(\"Overall Metrics:\")\n",
    "    for metric, value in result[\"metrics\"].items():\n",
    "        print(f\"  {metric}: {value}\")\n",
    "    print(\"\\nSubcluster Analysis:\")\n",
    "    for true_label, sub_metrics in result[\"subcluster_analysis\"].items():\n",
    "        print(f\"  True Label {true_label}:\")\n",
    "        for metric, value in sub_metrics.items():\n",
    "            print(f\"    {metric}: {value}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
