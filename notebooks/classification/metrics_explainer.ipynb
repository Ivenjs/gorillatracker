{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gorillatracker.classification.reid import compute_metrics\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def test(true, pred):\n",
    "    results = compute_metrics(true, pred, unique_labels)\n",
    "    for metric, value in results.items():\n",
    "        print(f\"{metric}: {value}\")\n",
    "\n",
    "\n",
    "# Define our known class labels\n",
    "unique_labels = [0, 1, 2]\n",
    "\n",
    "print(\"Perfect classification\")\n",
    "y_true = np.array([0, 1, 2, -1])\n",
    "y_pred = np.array([0, 1, 2, -1])\n",
    "test(y_true, y_pred)\n",
    "\n",
    "print(\"\\nLabel in Unique not in True nor Pred\")\n",
    "# tricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior. _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
    "y_true = np.array([0, 1, 1, -1])\n",
    "y_pred = np.array([0, 1, 1, -1])\n",
    "test(y_true, y_pred)\n",
    "\n",
    "print(\"\\nMisclassify a known class as new class\")\n",
    "y_true = np.array([0, 1, 2, -1, -1])\n",
    "y_pred = np.array([0, 1, 1, -1, 1])\n",
    "test(y_true, y_pred)\n",
    "\n",
    "print(\"\\n+Misclassify a known class as a different known\")\n",
    "y_true = np.array([0, 1, 2, -1])\n",
    "y_pred = np.array([0, 1, 1, -1])\n",
    "test(y_true, y_pred)\n",
    "\n",
    "\n",
    "print(\"\\n+asdfasdf\")\n",
    "y_true = np.array([0, 1, 2, 2])\n",
    "y_pred = np.array([0, 1, 1, -1])\n",
    "test(y_true, y_pred)\n",
    "\n",
    "print(\"\\n+Only New\")\n",
    "y_true = np.array([0, 1, 2, -1])\n",
    "y_pred = np.array([-1, -1, -1, -1])\n",
    "test(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "\n",
    "-1 for new class.\n",
    "\n",
    "The threshold will make the new class at first be assigned to too many samples, later at too little (as threshold grows larger that max-cross-point distance)\n",
    "\n",
    "\n",
    "\n",
    "multiclass_* look at all classes (including 'new')\n",
    "\n",
    "multiclass_accuracy - how often are our predictions correct (compare for same value in true|pred columns)\n",
    "\n",
    "multiclass_f1 - is macro weighted: all classes have same importance.\n",
    "\n",
    "multiclass_f1_weighted - is weighted by sample count At the start we should see only -1 in resultset, i.e. strong class imbalance.\n",
    "\n",
    "\n",
    "\n",
    "only_known_* looks\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "new_* looks at '-1' vs rest. It binarizes both columns to 0/1 [actually 1, -1] and then checks for equality.\n",
    "\n",
    "new_precision will start at % of non-new images (they are classified wrong, as all have 'new' label).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "=> Precision is screwed towards the number of samples. E. g. 50% new / 50% known will place starting precision at 0.5; it will then grow to an optimum.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
