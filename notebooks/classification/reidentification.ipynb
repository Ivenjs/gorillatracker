{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def generate_folds(df, k=5, seed=42):\n",
    "    \"\"\"\n",
    "    Generate label-disjunct folds for each (dataset, model) combination using a local random number generator.\n",
    "\n",
    "    Args:\n",
    "    df (pd.DataFrame): Input dataframe with columns 'dataset', 'model', 'label'\n",
    "    k (int): Number of folds\n",
    "    seed (int): Random seed for reproducibility\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: Copy of input dataframe with an additional 'fold' column\n",
    "    \"\"\"\n",
    "    # Create a copy of the input dataframe\n",
    "    df_with_folds = df.copy()\n",
    "    df_with_folds[\"fold\"] = -1  # Initialize fold column\n",
    "\n",
    "    # Create a local random number generator\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    # Group by dataset and model\n",
    "    for (dataset, model), group in df.groupby([\"dataset\", \"model\"]):\n",
    "        # Get unique labels for this group\n",
    "        unique_labels = group[\"label\"].unique()\n",
    "\n",
    "        # Shuffle labels using the local RNG\n",
    "        rng.shuffle(unique_labels)\n",
    "\n",
    "        # Split labels into k groups\n",
    "        label_folds = np.array_split(unique_labels, k)\n",
    "\n",
    "        # Create a mapping from label to fold\n",
    "        label_to_fold = {}\n",
    "        for fold, labels in enumerate(label_folds):\n",
    "            for label in labels:\n",
    "                label_to_fold[label] = fold\n",
    "\n",
    "        # Assign folds to rows\n",
    "        mask = (df_with_folds[\"dataset\"] == dataset) & (df_with_folds[\"model\"] == model)\n",
    "        df_with_folds.loc[mask, \"fold\"] = df_with_folds.loc[mask, \"label\"].map(label_to_fold)\n",
    "\n",
    "    # Ensure all rows have been assigned a fold\n",
    "    assert (df_with_folds[\"fold\"] != -1).all(), \"Some rows were not assigned a fold\"\n",
    "\n",
    "    return df_with_folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Tuple\n",
    "\n",
    "\n",
    "def split(\n",
    "    df: pd.DataFrame, k: int = 5, construction_method: str = \"equal_classes\", seed: int = 42\n",
    ") -> List[Tuple[pd.DataFrame, pd.DataFrame]]:\n",
    "    \"\"\"\n",
    "    Split the dataframe into k train-test combinations based on the 'fold' column.\n",
    "    Then applies the specified construction method to create new classes.\n",
    "\n",
    "    Args:\n",
    "    df (pd.DataFrame): Input dataframe with 'fold', 'dataset', 'model', and 'label' columns\n",
    "    k (int): Number of folds (default 5)\n",
    "    construction_method (str): Method to construct new classes ('equal_classes' or 'equal_images')\n",
    "    seed (int): Random seed for reproducibility\n",
    "\n",
    "    Returns:\n",
    "    List[Tuple[pd.DataFrame, pd.DataFrame]]: List of (train, test) dataframe pairs\n",
    "    \"\"\"\n",
    "    assert \"fold\" in df.columns, \"Dataframe must have a 'fold' column\"\n",
    "    assert \"dataset\" in df.columns and \"model\" in df.columns, \"Dataframe must have 'dataset' and 'model' columns\"\n",
    "    assert \"label\" in df.columns, \"Dataframe must have a 'label' column\"\n",
    "    assert (\n",
    "        len(df[\"dataset\"].unique()) == 1 and len(df[\"model\"].unique()) == 1\n",
    "    ), \"Dataframe should contain only one dataset and model combination\"\n",
    "    MIN_SAMPLES_REMAINING = 1\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    splits = []\n",
    "    for i in range(k):\n",
    "        test_df = df[df[\"fold\"] == i].copy()\n",
    "        test_df[\"is_new\"] = True\n",
    "        train_df = df[df[\"fold\"] != i].copy()\n",
    "        train_df[\"is_new\"] = False\n",
    "\n",
    "        if construction_method == \"equal_classes\":\n",
    "            \"\"\"\n",
    "            Choose a random number of labels from train_df to move to test_df.\n",
    "            Sample upto max_samples from each label. Skip if the label has less than min_samples.\n",
    "            \"\"\"\n",
    "            n = int(np.mean([train_df[train_df[\"fold\"] == j][\"label\"].nunique() for j in range(k) if j != i]))\n",
    "            min_samples, max_samples = 1, 3\n",
    "\n",
    "            train_labels = train_df[\"label\"].unique()\n",
    "            labels_to_move = rng.choice(train_labels, size=min(n, len(train_labels)), replace=False)\n",
    "\n",
    "            for label in labels_to_move:\n",
    "                train_label_group = train_df[train_df[\"label\"] == label]\n",
    "                n_samples = len(train_label_group)\n",
    "                if n_samples < min_samples + MIN_SAMPLES_REMAINING:\n",
    "                    print(\n",
    "                        f\"WARNING: Cannot move samples from train_df to test_df because label '{label}' has too few samples (total {n_samples} < {min_samples} + {MIN_SAMPLES_REMAINING}).\"\n",
    "                    )\n",
    "                    continue\n",
    "                n_samples_to_move = min(max_samples, n_samples - MIN_SAMPLES_REMAINING)\n",
    "                samples_to_move = train_label_group.sample(n=n_samples_to_move, random_state=rng)\n",
    "                train_df = train_df.drop(samples_to_move.index)\n",
    "                test_df = pd.concat([test_df, samples_to_move], ignore_index=True)\n",
    "\n",
    "        elif construction_method == \"equal_images\":\n",
    "            \"\"\"\n",
    "            Choose a random number of images from train_df to move to test_df.\n",
    "            Ensure that at least one image from each label remains in train_df.\n",
    "            \"\"\"\n",
    "            n_images_to_move = len(test_df)\n",
    "            train_labels = train_df[\"label\"].unique()\n",
    "\n",
    "            # Randomly sample images from train_df\n",
    "            samples_to_move = train_df.sample(n=n_images_to_move, random_state=rng)\n",
    "\n",
    "            # Ensure at least one image from each label remains in train_df\n",
    "            for label in train_labels:\n",
    "                label_samples = samples_to_move[samples_to_move[\"label\"] == label]\n",
    "                if len(label_samples) == len(train_df[train_df[\"label\"] == label]):\n",
    "                    # If all samples of a label are selected to move, keep one in train_df\n",
    "                    sample_to_keep = label_samples.sample(n=1, random_state=rng)\n",
    "                    samples_to_move = samples_to_move.drop(sample_to_keep.index)\n",
    "\n",
    "            # Move selected samples from train_df to test_df\n",
    "            train_df = train_df.drop(samples_to_move.index)\n",
    "            samples_to_move[\"is_new\"] = True\n",
    "            test_df = pd.concat([test_df, samples_to_move], ignore_index=True)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown construction method: {construction_method}\")\n",
    "\n",
    "        splits.append((train_df, test_df))\n",
    "\n",
    "    return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "\n",
    "def compute_metrics(y_true, y_pred, unique_labels):\n",
    "    \"\"\"\n",
    "    Compute accuracy and macro F1-score for new vs. known classification and multiclass classification among known classes.\n",
    "    New Class has label -1. The other classes are multiclass.\n",
    "\n",
    "    Args:\n",
    "    y_true (array-like): True labels\n",
    "    y_pred (array-like): Predicted labels\n",
    "    unique_labels (array-like): List of known class labels\n",
    "\n",
    "    Returns:\n",
    "    dict: Dictionary containing computed metrics\n",
    "\n",
    "\n",
    "    T|F\n",
    "    1|1 - Correct\n",
    "    1|2 - Incorrect known\n",
    "    1|-1 - Incorrect new & new vs. known\n",
    "    -1|1 - Incorrect known & new vs. known\n",
    "    -1|-1 - Correct new\n",
    "\n",
    "    multiclass -> always correct if left equal to right side (no matter if new or known class, it's just classification over n+1)\n",
    "    \"\"\"\n",
    "    # Convert to numpy arrays for easier manipulation\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "\n",
    "    # Create binary labels for new vs. known classification\n",
    "    y_true_binary = (y_true == -1).astype(int)\n",
    "    y_pred_binary = (y_pred == -1).astype(int)\n",
    "\n",
    "    # Compute metrics for binary classification (new vs. known)\n",
    "    binary_accuracy = accuracy_score(y_true_binary, y_pred_binary)\n",
    "    binary_f1 = f1_score(y_true_binary, y_pred_binary)\n",
    "\n",
    "    # Compute metrics for multiclass classification (only for known classes)\n",
    "    mask_true = y_true != -1\n",
    "    mask_pred = y_pred != -1\n",
    "    mask = mask_true & mask_pred\n",
    "\n",
    "    # TODO(liamvdv): reconsider if this is the behaviour we want when no match is found. i.d.R. we want NOT to pick this as max, thus assign low precision.\n",
    "    only_known_accuracy = accuracy_score(y_true[mask], y_pred[mask]) if y_true[mask].size > 0 else 0\n",
    "\n",
    "    # we will not add the '-1' new class to the labels; this f1 is only about the known classes\n",
    "    # we want to exclude the 'new' class from the f1 calculation via labels= (note we are not using the mask here)\n",
    "    # this will give us the f1 score over the known classes only [zero_division=1 is used to allow labels to be missing]\n",
    "    only_known_f1 = f1_score(y_true, y_pred, labels=unique_labels, average=\"macro\", zero_division=1)\n",
    "\n",
    "    # Create a dictionary to store the metrics\n",
    "    metrics = {\n",
    "        # how often was new class predicted correctly\n",
    "        # threshold graph: should start at 1 and go down to 0\n",
    "        \"new_vs_all_accuracy\": binary_accuracy,\n",
    "        # threshold graph: should start at 1 and go down to 0\n",
    "        \"new_vs_all_f1\": binary_f1,\n",
    "        # t=-1 or p=-1 will be excluded\n",
    "        \"only_known_accuracy\": only_known_accuracy,\n",
    "        \"only_known_f1\": only_known_f1,\n",
    "        # normal multiclass over n+1 classes\n",
    "        # threshold graph: should start at\n",
    "        \"multiclass_accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"multiclass_f1\": f1_score(y_true, y_pred, average=\"macro\"),\n",
    "        \"multiclass_f1_weighted\": f1_score(y_true, y_pred, average=\"weighted\"),\n",
    "    }\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from typing import List, Dict\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "\n",
    "def knn_openset_recognition(\n",
    "    dataset: pd.DataFrame,\n",
    "    queryset: pd.DataFrame,\n",
    "    thresholds: List[float],\n",
    "    method: str = \"knn1\",\n",
    "    snapshot: List[float] = None,\n",
    ") -> Dict[float, Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Perform KNN + threshold grid search for open-set recognition with centroid caching.\n",
    "\n",
    "    Args:\n",
    "    dataset (pd.DataFrame): Training dataset with 'label' and 'embedding' columns\n",
    "    queryset (pd.DataFrame): Query dataset with 'label' and 'embedding' columns\n",
    "    thresholds (List[float]): List of thresholds to search\n",
    "    method (str): Method to use for classification ('knn1', 'knn5', 'knn1centroid', 'knn1centroid_iqr')\n",
    "    snapshot (List[float]): List of thresholds to store results for.\n",
    "\n",
    "    Returns:\n",
    "    Dict[float, Dict[str, float]]: Results for each threshold\n",
    "    \"\"\"\n",
    "    # Prepare the data\n",
    "    X_train = np.stack(dataset[\"embedding\"].values)\n",
    "    y_train = dataset[\"label\"].values\n",
    "    X_query = np.stack(queryset[\"embedding\"].values)\n",
    "    y_query = queryset[\"label\"].values\n",
    "\n",
    "    # Standardize the features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_query_scaled = scaler.transform(X_query)\n",
    "\n",
    "    # Fit the NearestNeighbors model\n",
    "    nbrs = NearestNeighbors(n_neighbors=5).fit(X_train_scaled)\n",
    "\n",
    "    # Find the nearest neighbors for queryset\n",
    "    distances, indices = nbrs.kneighbors(X_query_scaled)\n",
    "\n",
    "    # Cache for centroids\n",
    "    centroid_cache = {}\n",
    "\n",
    "    def get_centroids(method):\n",
    "        if method not in centroid_cache:\n",
    "            if method == \"knn1centroid\":\n",
    "                centroid_cache[method] = dataset.groupby(\"label\").apply(calculate_centroid, include_groups=False)\n",
    "            elif method == \"knn1centroid_iqr\":\n",
    "                centroid_cache[method] = dataset.groupby(\"label\").apply(\n",
    "                    calculate_centroid_with_iqr, include_groups=False\n",
    "                )\n",
    "        return centroid_cache[method]\n",
    "\n",
    "    results = {}\n",
    "    for t in thresholds:\n",
    "        if method == \"knn1\":\n",
    "            predictions = knn1(dataset, indices, distances, t)\n",
    "        elif method == \"knn5\":\n",
    "            predictions = knnK(dataset, indices, distances, t)\n",
    "        elif method == \"knn5distance\":\n",
    "            predictions = knn_distance(dataset, indices, distances, t)\n",
    "        elif method in [\"knn1centroid\", \"knn1centroid_iqr\"]:\n",
    "            centroids = get_centroids(method)\n",
    "            predictions = knn1centroid_generic(dataset, indices, distances, t, centroids)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown method: {method}\")\n",
    "\n",
    "        results[t] = compute_metrics(y_query, predictions, dataset[\"label\"].unique())\n",
    "\n",
    "        if snapshot and any(s for s in snapshot if math.isclose(t, s, abs_tol=1e-6)):\n",
    "            results[t][\"y_true\"] = y_query\n",
    "            results[t][\"y_pred\"] = predictions\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def calculate_centroid(group):\n",
    "    \"\"\"Calculate the centroid of a group of embeddings.\"\"\"\n",
    "    embeddings = np.vstack(group[\"embedding\"].values)\n",
    "    return np.mean(embeddings, axis=0) if embeddings.size > 0 else np.zeros(embeddings.shape[1])\n",
    "\n",
    "\n",
    "def calculate_centroid_with_iqr(group):\n",
    "    \"\"\"Calculate the centroid of a group of embeddings with IQR-based outlier filtering.\"\"\"\n",
    "    embeddings = filter_outliers_iqr(pd.DataFrame(np.vstack(group[\"embedding\"]))).values\n",
    "    return np.mean(embeddings, axis=0) if embeddings.size > 0 else np.zeros(group[\"embedding\"].iloc[0].shape)\n",
    "\n",
    "\n",
    "def filter_outliers_iqr(group):\n",
    "    \"\"\"Filter outliers using the Interquartile Range method.\"\"\"\n",
    "    Q1 = group.quantile(0.25)\n",
    "    Q3 = group.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    return group[(group >= lower_bound) & (group <= upper_bound)]\n",
    "\n",
    "\n",
    "def knn1centroid_generic(dataset, indices, distances, threshold, centroids):\n",
    "    \"\"\"\n",
    "    Perform 1-NN classification using pre-computed centroids of each class.\n",
    "\n",
    "    Args:\n",
    "    dataset (pd.DataFrame): Training dataset with 'label' and 'embedding' columns\n",
    "    indices (np.array): Indices of nearest neighbors for each query point\n",
    "    distances (np.array): Distances to nearest neighbors for each query point\n",
    "    threshold (float): Distance threshold for classification\n",
    "    centroids (pd.Series): Pre-computed centroids for each class\n",
    "\n",
    "    Returns:\n",
    "    np.array: Predicted labels\n",
    "    \"\"\"\n",
    "    query_embeddings = np.vstack(dataset.iloc[indices[:, 0]][\"embedding\"].values)\n",
    "    centroid_distances = cdist(query_embeddings, np.vstack(centroids.values))\n",
    "\n",
    "    nearest_centroid_indices = np.argmin(centroid_distances, axis=1)\n",
    "    nearest_centroid_distances = np.min(centroid_distances, axis=1)\n",
    "\n",
    "    predictions = np.array(centroids.index[nearest_centroid_indices])\n",
    "    predictions[nearest_centroid_distances > threshold] = -1\n",
    "\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def knn1(dataset, indices, distances, threshold):\n",
    "    return knnK(dataset, indices[:, :1], distances[:, :1], threshold)\n",
    "\n",
    "\n",
    "def knnK(dataset, indices, distances, threshold):\n",
    "    \"\"\"note: k is implicitly set to indices.shape[1]\"\"\"\n",
    "    predictions = []\n",
    "    for idx, dist in zip(indices, distances):\n",
    "        valid = dataset.iloc[idx][dist <= threshold]\n",
    "        if not valid.empty:\n",
    "            prediction = valid[\"label\"].mode()[0]\n",
    "        else:\n",
    "            prediction = -1\n",
    "        predictions.append(prediction)\n",
    "    return np.array(predictions)\n",
    "\n",
    "\n",
    "def knn_distance(dataset, indices, distances, threshold):\n",
    "    \"\"\"note: k is implicitly set to indices.shape[1]\"\"\"\n",
    "    predictions = []\n",
    "    for idx, dist in zip(indices, distances):\n",
    "        if dist[0] > threshold:\n",
    "            predictions.append(-1)\n",
    "        else:\n",
    "            weights = 1 / (1 + dist)\n",
    "            labels = dataset.iloc[idx][\"label\"]\n",
    "            df = pd.DataFrame({\"label\": labels, \"weight\": weights})\n",
    "            weighted_votes = df.groupby(\"label\")[\"weight\"].sum()\n",
    "            predictions.append(weighted_votes.idxmax())\n",
    "    return np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_knn_openset_recognition_cv(\n",
    "    thresholds: List[float],\n",
    "    df: pd.DataFrame,\n",
    "    k_fold: int = 5,\n",
    "    construction_method: str = \"equal_classes\",\n",
    "    seed: int = 42,\n",
    "    method: str = \"knn1\",\n",
    "    snapshots: List[float] = None,\n",
    ") -> Dict[float, Dict[str, List[float]]]:\n",
    "    \"\"\"\n",
    "    Run KNN open-set recognition with cross-validation.\n",
    "\n",
    "    Args:\n",
    "    df (pd.DataFrame): Input dataframe\n",
    "    k_fold (int): Number of folds for cross-validation\n",
    "    n, min_samples, max_samples, seed: Parameters for the split function\n",
    "    thresholds (List[float]): List of thresholds to search\n",
    "    knn_k (int): Number of neighbors for KNN\n",
    "    method (str): KNN method to use\n",
    "\n",
    "    Returns:\n",
    "    Dict[float, Dict[str, List[float]]]: Cross-validation results for each threshold\n",
    "    \"\"\"\n",
    "    new_label = -1\n",
    "    assert new_label not in df[\"label\"].unique(), \"New label already exists in the dataframe\"\n",
    "    df_foldable = generate_folds(df, k=k_fold, seed=seed)\n",
    "    splits = split(df_foldable, k=k_fold, construction_method=construction_method, seed=seed)\n",
    "\n",
    "    cv_results = defaultdict(lambda: defaultdict(list))\n",
    "    for train_df, test_df in splits:\n",
    "        classes_total = test_df[\"label\"].nunique()\n",
    "        classes_new = test_df[test_df[\"is_new\"]][\"label\"].nunique()\n",
    "        images_total = test_df.count()\n",
    "        images_new = test_df[test_df[\"is_new\"]].count()\n",
    "        # set all test_df labels to new_label if is_new column set\n",
    "        test_df.loc[test_df[\"is_new\"], \"label\"] = new_label\n",
    "        test_df.loc[test_df[\"is_new\"], \"label_string\"] = \"new\"\n",
    "        fold_results = knn_openset_recognition(train_df, test_df, thresholds, method=method, snapshot=snapshots)\n",
    "        for t, metrics in fold_results.items():\n",
    "            for metric, value in metrics.items():\n",
    "                cv_results[t][metric].append(value)\n",
    "\n",
    "                cv_results[t][\"count_queryset_images_new\"].append(images_new)\n",
    "                cv_results[t][\"count_queryset_classes_new\"].append(classes_new)\n",
    "                cv_results[t][\"count_queryset_images_total\"].append(images_total)\n",
    "                cv_results[t][\"count_queryset_classes_total\"].append(classes_total)\n",
    "\n",
    "    return cv_results\n",
    "\n",
    "\n",
    "edf = EXT_MERGED_DF\n",
    "\n",
    "# Filter and prepare the data\n",
    "df = edf[(edf[\"dataset\"] == \"SPAC+min3+max10\") & (edf[\"model\"] == \"ViT-Finetuned\")].reset_index(drop=True)\n",
    "analysis = analyse_embedding_space(df)\n",
    "max_distance = analysis[\"global_max_dist\"]\n",
    "min_distance = analysis[\"global_min_dist\"]\n",
    "# Set up parameters\n",
    "thresholds = np.linspace(0, max_distance + 10, 30)\n",
    "method = \"knn1\"\n",
    "cv_results = run_knn_openset_recognition_cv(thresholds, df, method=method, construction_method=\"equal_classes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def test(true, pred):\n",
    "    results = compute_metrics(true, pred, unique_labels)\n",
    "    for metric, value in results.items():\n",
    "        print(f\"{metric}: {value}\")\n",
    "\n",
    "\n",
    "# Define our known class labels\n",
    "unique_labels = [0, 1, 2]\n",
    "\n",
    "print(\"Perfect classification\")\n",
    "y_true = np.array([0, 1, 2, -1])\n",
    "y_pred = np.array([0, 1, 2, -1])\n",
    "test(y_true, y_pred)\n",
    "\n",
    "print(\"\\nLabel in Unique not in True nor Pred\")\n",
    "# tricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior. _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
    "y_true = np.array([0, 1, 1, -1])\n",
    "y_pred = np.array([0, 1, 1, -1])\n",
    "test(y_true, y_pred)\n",
    "\n",
    "print(\"\\nMisclassify a known class as new class\")\n",
    "y_true = np.array([0, 1, 2, -1, -1])\n",
    "y_pred = np.array([0, 1, 1, -1, 1])\n",
    "test(y_true, y_pred)\n",
    "\n",
    "print(\"\\n+Misclassify a known class as a different known\")\n",
    "y_true = np.array([0, 1, 2, -1])\n",
    "y_pred = np.array([0, 1, 1, -1])\n",
    "test(y_true, y_pred)\n",
    "\n",
    "\n",
    "print(\"\\n+asdfasdf\")\n",
    "y_true = np.array([0, 1, 2, 2])\n",
    "y_pred = np.array([0, 1, 1, -1])\n",
    "test(y_true, y_pred)\n",
    "\n",
    "print(\"\\n+Only New\")\n",
    "y_true = np.array([0, 1, 2, -1])\n",
    "y_pred = np.array([-1, -1, -1, -1])\n",
    "test(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from typing import Dict, List\n",
    "import math\n",
    "\n",
    "\n",
    "def visualize_metrics(cv_results: Dict[float, Dict[str, List[float]]], thresholds: List[float]):\n",
    "    \"\"\"\n",
    "    Visualize metrics from cross-validation results, handling None values,\n",
    "    and combining count_* metrics into a single subplot.\n",
    "\n",
    "    Args:\n",
    "    cv_results (Dict[float, Dict[str, List[float]]]): Cross-validation results\n",
    "    thresholds (List[float]): List of thresholds used\n",
    "\n",
    "    Returns:\n",
    "    None (displays the plot)\n",
    "    \"\"\"\n",
    "    metrics = list(cv_results[thresholds[0]].keys())\n",
    "    count_metrics = [m for m in metrics if m.startswith(\"count_\")]\n",
    "    other_metrics = [m for m in metrics if not m.startswith(\"count_\")]\n",
    "\n",
    "    num_metrics = len(other_metrics) + 1  # +1 for the combined count metrics\n",
    "    num_cols = 3\n",
    "    num_rows = math.ceil(num_metrics / num_cols)\n",
    "\n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, 5 * num_rows))\n",
    "    fig.suptitle(\"Metrics across Different Thresholds\", fontsize=16)\n",
    "\n",
    "    # Flatten axes array for easier indexing\n",
    "    axes = axes.flatten() if num_rows > 1 else [axes]\n",
    "\n",
    "    # Plot other metrics\n",
    "    for i, metric in enumerate(other_metrics):\n",
    "        ax = axes[i]\n",
    "\n",
    "        valid_thresholds = []\n",
    "        mean_values = []\n",
    "        std_values = []\n",
    "\n",
    "        for t in thresholds:\n",
    "            values = [v for v in cv_results[t][metric] if v is not None]\n",
    "            if values:\n",
    "                valid_thresholds.append(t)\n",
    "                mean_values.append(np.mean(values))\n",
    "                std_values.append(np.std(values))\n",
    "\n",
    "        if valid_thresholds:\n",
    "            ax.plot(valid_thresholds, mean_values, marker=\"o\")\n",
    "            ax.fill_between(\n",
    "                valid_thresholds,\n",
    "                [m - s for m, s in zip(mean_values, std_values)],\n",
    "                [m + s for m, s in zip(mean_values, std_values)],\n",
    "                alpha=0.2,\n",
    "            )\n",
    "\n",
    "        ax.set_xlabel(\"Threshold\")\n",
    "        ax.set_ylabel(metric)\n",
    "        ax.set_title(metric)\n",
    "        ax.grid(True, linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "    # Plot combined count metrics\n",
    "    ax = axes[len(other_metrics)]\n",
    "    ax.set_title(\"Count Metrics\")\n",
    "    ax.set_xlabel(\"Threshold\")\n",
    "    ax.set_ylabel(\"Count\")\n",
    "\n",
    "    for metric in count_metrics:\n",
    "        mean_values = []\n",
    "        std_values = []\n",
    "\n",
    "        for t in thresholds:\n",
    "            values = [v for v in cv_results[t][metric] if v is not None]\n",
    "            if values:\n",
    "                mean_values.append(np.mean(values))\n",
    "                std_values.append(np.std(values))\n",
    "            else:\n",
    "                mean_values.append(np.nan)\n",
    "                std_values.append(np.nan)\n",
    "\n",
    "        ax.plot(thresholds, mean_values, marker=\"o\", label=metric)\n",
    "        ax.fill_between(\n",
    "            thresholds,\n",
    "            [m - s for m, s in zip(mean_values, std_values)],\n",
    "            [m + s for m, s in zip(mean_values, std_values)],\n",
    "            alpha=0.2,\n",
    "        )\n",
    "\n",
    "    ax.legend()\n",
    "    ax.grid(True, linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "    # Remove any unused subplots\n",
    "    for i in range(num_metrics, len(axes)):\n",
    "        fig.delaxes(axes[i])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "visualize_metrics(cv_results, thresholds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Metrics\n",
    "\n",
    " -1 for new class.\n",
    "\n",
    " The threshold will make the new class at first be assigned to too many samples, later at too little (as threshold grows larger that max-cross-point distance)\n",
    "\n",
    "\n",
    "\n",
    " multiclass_* look at all classes (including 'new')\n",
    "\n",
    " multiclass_accuracy - how often are our predictions correct (compare for same value in true|pred columns)\n",
    "\n",
    " multiclass_f1 - is macro weighted: all classes have same importance.\n",
    "\n",
    " multiclass_f1_weighted - is weighted by sample count At the start we should see only -1 in resultset, i.e. strong class imbalance.\n",
    "\n",
    "\n",
    "\n",
    " only_known_* looks\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " new_* looks at '-1' vs rest. It binarizes both columns to 0/1 [actually 1, -1] and then checks for equality.\n",
    "\n",
    " new_precision will start at % of non-new images (they are classified wrong, as all have 'new' label).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " => Precision is screwed towards the number of samples. E. g. 50% new / 50% known will place starting precision at 0.5; it will then grow to an optimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import numpy as np\n",
    "from typing import Dict, List\n",
    "\n",
    "\n",
    "def get_optimal_threshold(\n",
    "    cv_results: Dict[float, Dict[str, List[float]]], metric: str = \"multiclass_f1\", stability_weight: float = 0.2\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Find the optimal threshold based on the cross-validation results.\n",
    "\n",
    "    Args:\n",
    "    cv_results (Dict[float, Dict[str, List[float]]]): Cross-validation results\n",
    "    metric (str): Metric to optimize. Options: 'multiclass_f1', 'new_vs_all_f1', 'only_known_f1'\n",
    "    stability_weight (float): Weight given to the stability of the metric across folds (0 to 1)\n",
    "\n",
    "    Returns:\n",
    "    float: Optimal threshold\n",
    "    \"\"\"\n",
    "    thresholds = list(cv_results.keys())\n",
    "    scores = []\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        metric_values = [v for v in cv_results[threshold][metric] if v is not None]\n",
    "        if not metric_values:\n",
    "            scores.append(0.0)\n",
    "            continue\n",
    "        mean_score = np.mean(metric_values)\n",
    "        std_score = np.std(metric_values)\n",
    "\n",
    "        # Combine mean and stability (inverse of std) into a single score\n",
    "        combined_score = (1 - stability_weight) * mean_score + stability_weight * (1 / (1 + std_score))\n",
    "        scores.append(combined_score)\n",
    "\n",
    "    optimal_index = np.argmax(scores)\n",
    "    optimal_threshold = thresholds[optimal_index]\n",
    "\n",
    "    print(f\"Optimal threshold for {metric}: {optimal_threshold}\")\n",
    "    mean = np.mean(cv_results[optimal_threshold][metric]) if any(cv_results[optimal_threshold][metric]) else 0\n",
    "    std = np.std(cv_results[optimal_threshold][metric]) if any(cv_results[optimal_threshold][metric]) else 0\n",
    "    print(f\"Mean {metric} at optimal threshold: {mean:.4f}\")\n",
    "    print(f\"Std {metric} at optimal threshold: {std:.4f}\")\n",
    "\n",
    "    return optimal_threshold\n",
    "\n",
    "\n",
    "optimal_threshold_multiclass = get_optimal_threshold(cv_results, metric=\"multiclass_f1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "from gorillatracker.classification.clustering import MERGED_DF, EXT_MERGED_DF\n",
    "from gorillatracker.classification.metrics import analyse_embedding_space\n",
    "\n",
    "warnings.filterwarnings(\"error\", category=RuntimeWarning, message=\"invalid value encountered in scalar divide\")\n",
    "\n",
    "labelling_methods = [\"knn1\", \"knn5\", \"knn5distance\", \"knn1centroid\", \"knn1centroid_iqr\"]\n",
    "\n",
    "construction_methods = {\"equal_classes\": print, \"equal_images\": print}\n",
    "\n",
    "\n",
    "def thresholds_selector(df, n_measures=50):\n",
    "    analysis = analyse_embedding_space(df)\n",
    "    max_distance = analysis[\"global_max_dist\"]\n",
    "    min_distance = analysis[\"global_min_dist\"]\n",
    "    thresholds = np.concatenate([[0], np.linspace(min_distance, max_distance + 1, n_measures)])\n",
    "    return thresholds\n",
    "\n",
    "\n",
    "def sweep_configs(dataframe, configs, resolution=50):\n",
    "    \"\"\"make the sweep faster but also less accurate by choosing lower grid search resolution\"\"\"\n",
    "    print(f\"Running {len(configs)} configurations at resolution {resolution}...\")\n",
    "    results = {}\n",
    "\n",
    "    for dataset, model, labelling_method, construction_method in tqdm(configs):\n",
    "        print(\n",
    "            f\"Started sweep for dataset '{dataset}', model '{model}', \"\n",
    "            f\"labelling method '{labelling_method}', and selector method '{construction_method}'\"\n",
    "        )\n",
    "\n",
    "        # Filter the dataframe for the current dataset and model\n",
    "        df = dataframe[(dataframe[\"dataset\"] == dataset) & (dataframe[\"model\"] == model)]\n",
    "\n",
    "        if df.empty:\n",
    "            print(f\"No data found for dataset '{dataset}' and model '{model}'. Skipping...\")\n",
    "            continue\n",
    "        # Generate thresholds\n",
    "        thresholds = thresholds_selector(df, n_measures=resolution)\n",
    "        # Run cross-validation\n",
    "        cv_results = run_knn_openset_recognition_cv(\n",
    "            thresholds=thresholds,\n",
    "            df=df,\n",
    "            construction_method=construction_method,\n",
    "            method=labelling_method,\n",
    "            k_fold=5,\n",
    "            seed=42,\n",
    "        )\n",
    "\n",
    "        # Get optimal threshold\n",
    "        optimal_threshold = get_optimal_threshold(cv_results, metric=\"multiclass_f1\")\n",
    "\n",
    "        # Store results\n",
    "        results[(dataset, model, labelling_method, construction_method)] = {\n",
    "            \"cv_results\": cv_results,\n",
    "            \"optimal_threshold\": optimal_threshold,\n",
    "            \"thresholds\": thresholds,\n",
    "        }\n",
    "\n",
    "        print(\n",
    "            f\"Completed sweep for dataset '{dataset}', model '{model}', \"\n",
    "            f\"labelling method '{labelling_method}', and selector method '{construction_method}'\"\n",
    "        )\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "labelling_methods = [\"knn1\", \"knn5\", \"knn5distance\", \"knn1centroid\", \"knn1centroid_iqr\"]\n",
    "construction_methods = [\"equal_classes\", \"equal_images\"]\n",
    "df = EXT_MERGED_DF\n",
    "configs = [\n",
    "    (dataset, model, labelling_method, construction_method)\n",
    "    for (dataset, model), _ in df.groupby([\"dataset\", \"model\"])\n",
    "    for labelling_method in labelling_methods\n",
    "    for construction_method in construction_methods\n",
    "]\n",
    "print(\"Total configurations:\", len(configs))\n",
    "partial = [c for c in configs if c[0] == \"SPAC+min3+max10\" and c[1] == \"ViT-Finetuned\"]\n",
    "print(\"Partial configurations:\", len(partial))\n",
    "# results = sweep_configs(df, configs, resolution=30)\n",
    "results = sweep_configs(df, partial, resolution=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from typing import Dict, List, Tuple\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "\n",
    "def batch_visualize_metrics(results: Dict[Tuple, Dict], configs: List[Tuple]):\n",
    "    \"\"\"\n",
    "    Visualize metrics from cross-validation results for multiple configurations,\n",
    "    with lighter standard deviation shading.\n",
    "\n",
    "    Args:\n",
    "    results (Dict[Tuple, Dict]): Results dictionary from sweep_configs\n",
    "    configs (List[Tuple]): List of configurations to visualize\n",
    "\n",
    "    Returns:\n",
    "    None (displays the plots)\n",
    "    \"\"\"\n",
    "    colors = plt.cm.rainbow(np.linspace(0, 1, len(configs)))\n",
    "\n",
    "    _one_result = results[configs[0]][\"cv_results\"]\n",
    "    metric_names = list(_one_result[list(_one_result.keys())[0]].keys())\n",
    "    all_metrics = [m for m in metric_names if not m.startswith(\"count_\")]\n",
    "\n",
    "    num_metrics = len(all_metrics) + 1\n",
    "    num_cols = 1\n",
    "    num_rows = math.ceil(num_metrics / num_cols)\n",
    "\n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(20, 6 * num_rows))\n",
    "    fig.suptitle(\"Metrics across Different Thresholds and Configurations\", fontsize=16)\n",
    "\n",
    "    axes = axes.flatten() if num_rows > 1 else [axes]\n",
    "\n",
    "    for i, metric in enumerate(all_metrics):\n",
    "        ax = axes[i]\n",
    "\n",
    "        for config, color in zip(configs, colors):\n",
    "            dataset, model, labelling_method, construction_method = config\n",
    "            cv_results = results[config][\"cv_results\"]\n",
    "            thresholds = results[config][\"thresholds\"]\n",
    "\n",
    "            valid_thresholds = []\n",
    "            mean_values = []\n",
    "            std_values = []\n",
    "\n",
    "            for t in thresholds:\n",
    "                values = [v for v in cv_results[t][metric] if v is not None]\n",
    "                if values:\n",
    "                    valid_thresholds.append(t)\n",
    "                    mean_values.append(np.mean(values))\n",
    "                    std_values.append(np.std(values))\n",
    "\n",
    "            if valid_thresholds:\n",
    "                ax.plot(\n",
    "                    valid_thresholds,\n",
    "                    mean_values,\n",
    "                    color=color,\n",
    "                    label=f\"{dataset}-{model}-{labelling_method}-{construction_method}\",\n",
    "                )\n",
    "                ax.fill_between(\n",
    "                    valid_thresholds,\n",
    "                    [m - s for m, s in zip(mean_values, std_values)],\n",
    "                    [m + s for m, s in zip(mean_values, std_values)],\n",
    "                    alpha=0.05,  # Reduced alpha for lighter shading\n",
    "                    color=color,\n",
    "                )\n",
    "\n",
    "        ax.set_xlabel(\"Threshold\")\n",
    "        ax.set_ylabel(metric)\n",
    "        ax.set_title(metric)\n",
    "        ax.grid(True, linestyle=\"--\", alpha=0.7)\n",
    "        ax.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\", borderaxespad=0.0)\n",
    "\n",
    "    # Plot combined count metrics\n",
    "    ax = axes[len(all_metrics)]\n",
    "    ax.set_title(\"Count Metrics\")\n",
    "    ax.set_xlabel(\"Threshold\")\n",
    "    ax.set_ylabel(\"Count\")\n",
    "\n",
    "    count_metrics = [m for m in metric_names if m.startswith(\"count_\")]\n",
    "\n",
    "    for config, color in zip(configs, colors):\n",
    "        dataset, model, labelling_method, construction_method = config\n",
    "        cv_results = results[config][\"cv_results\"]\n",
    "        thresholds = results[config][\"thresholds\"]\n",
    "\n",
    "        for metric in count_metrics:\n",
    "            mean_values = []\n",
    "            std_values = []\n",
    "\n",
    "            for t in thresholds:\n",
    "                values = [v for v in cv_results[t][metric] if v is not None]\n",
    "                if values:\n",
    "                    mean_values.append(np.mean(values))\n",
    "                    std_values.append(np.std(values))\n",
    "                else:\n",
    "                    mean_values.append(np.nan)\n",
    "                    std_values.append(np.nan)\n",
    "\n",
    "            ax.plot(\n",
    "                thresholds,\n",
    "                mean_values,\n",
    "                color=color,\n",
    "                linestyle=\"--\",\n",
    "                label=f\"{dataset}-{model}-{labelling_method}-{construction_method}-{metric}\",\n",
    "            )\n",
    "\n",
    "    ax.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\", borderaxespad=0.0)\n",
    "    ax.grid(True, linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "    for i in range(num_metrics, len(axes)):\n",
    "        fig.delaxes(axes[i])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "batch_visualize_metrics(results, partial)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
