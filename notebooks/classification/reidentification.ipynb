{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def generate_folds(df, k=5, seed=42):\n",
    "    \"\"\"\n",
    "    Generate label-disjunct folds for each (dataset, model) combination using a local random number generator.\n",
    "\n",
    "    Args:\n",
    "    df (pd.DataFrame): Input dataframe with columns 'dataset', 'model', 'label'\n",
    "    k (int): Number of folds\n",
    "    seed (int): Random seed for reproducibility\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: Copy of input dataframe with an additional 'fold' column\n",
    "    \"\"\"\n",
    "    # Create a copy of the input dataframe\n",
    "    df_with_folds = df.copy()\n",
    "    df_with_folds[\"fold\"] = -1  # Initialize fold column\n",
    "\n",
    "    # Create a local random number generator\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    # Group by dataset and model\n",
    "    for (dataset, model), group in df.groupby([\"dataset\", \"model\"]):\n",
    "        # Get unique labels for this group\n",
    "        unique_labels = group[\"label\"].unique()\n",
    "\n",
    "        # Shuffle labels using the local RNG\n",
    "        rng.shuffle(unique_labels)\n",
    "\n",
    "        # Split labels into k groups\n",
    "        label_folds = np.array_split(unique_labels, k)\n",
    "\n",
    "        # Create a mapping from label to fold\n",
    "        label_to_fold = {}\n",
    "        for fold, labels in enumerate(label_folds):\n",
    "            for label in labels:\n",
    "                label_to_fold[label] = fold\n",
    "\n",
    "        # Assign folds to rows\n",
    "        mask = (df_with_folds[\"dataset\"] == dataset) & (df_with_folds[\"model\"] == model)\n",
    "        df_with_folds.loc[mask, \"fold\"] = df_with_folds.loc[mask, \"label\"].map(label_to_fold)\n",
    "\n",
    "    # Ensure all rows have been assigned a fold\n",
    "    assert (df_with_folds[\"fold\"] != -1).all(), \"Some rows were not assigned a fold\"\n",
    "\n",
    "    return df_with_folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Tuple\n",
    "\n",
    "\n",
    "def stripe_samples(\n",
    "    train_group: pd.DataFrame, test_group: pd.DataFrame, min_samples: int, max_samples: int, rng: np.random.Generator\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Move a random number of samples (between min_samples and max_samples) from train to test for a given label.\n",
    "\n",
    "    Args:\n",
    "    train_group (pd.DataFrame): Train group of samples for a single label\n",
    "    test_group (pd.DataFrame): Test group of samples for a single label\n",
    "    min_samples (int): Minimum number of samples to move\n",
    "    max_samples (int): Maximum number of samples to move\n",
    "    rng (np.random.Generator): Random number generator\n",
    "\n",
    "    Returns:\n",
    "    Tuple[pd.DataFrame, pd.DataFrame]: Updated train and test groups\n",
    "    \"\"\"\n",
    "    num_samples = min(rng.integers(min_samples, max_samples + 1), len(train_group))\n",
    "    samples_to_move = train_group.sample(n=num_samples, random_state=rng)\n",
    "\n",
    "    updated_train = train_group.drop(samples_to_move.index)\n",
    "    updated_test = pd.concat([test_group, samples_to_move], ignore_index=True)\n",
    "\n",
    "    return updated_train, updated_test\n",
    "\n",
    "\n",
    "def split(\n",
    "    df: pd.DataFrame, k: int = 5, n: int = None, min_samples: int = None, max_samples: int = None, seed: int = 42\n",
    ") -> List[Tuple[pd.DataFrame, pd.DataFrame]]:\n",
    "    \"\"\"\n",
    "    Split the dataframe into k train-test combinations based on the 'fold' column.\n",
    "    Then moves between (min_samples, max_samples) samples from train to test for n randomly selected labels in train.\n",
    "\n",
    "    Args:\n",
    "    df (pd.DataFrame): Input dataframe with 'fold', 'dataset', 'model', and 'label' columns\n",
    "    k (int): Number of folds (default 5)\n",
    "    n (int): Number of labels to move samples from train to test\n",
    "    min_samples (int): Minimum number of samples to move per label\n",
    "    max_samples (int): Maximum number of samples to move per label\n",
    "    seed (int): Random seed for reproducibility\n",
    "\n",
    "    Returns:\n",
    "    List[Tuple[pd.DataFrame, pd.DataFrame]]: List of (train, test) dataframe pairs\n",
    "    \"\"\"\n",
    "    assert \"fold\" in df.columns, \"Dataframe must have a 'fold' column\"\n",
    "    assert \"dataset\" in df.columns and \"model\" in df.columns, \"Dataframe must have 'dataset' and 'model' columns\"\n",
    "    assert \"label\" in df.columns, \"Dataframe must have a 'label' column\"\n",
    "    assert (\n",
    "        len(df[\"dataset\"].unique()) == 1 and len(df[\"model\"].unique()) == 1\n",
    "    ), \"Dataframe should contain only one dataset and model combination\"\n",
    "    # assert new_label not in df[\"label\"].unique(), \"New label already exists in the dataframe\"\n",
    "\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    splits = []\n",
    "    for i in range(k):\n",
    "        # is_new column to track 'new' class samples.\n",
    "        test_df = df[df[\"fold\"] == i].copy()\n",
    "        test_df[\"is_new\"] = True\n",
    "        train_df = df[df[\"fold\"] != i].copy()\n",
    "        train_df[\"is_new\"] = False\n",
    "\n",
    "        if n is not None and min_samples is not None and max_samples is not None:\n",
    "            # Get unique labels in train set\n",
    "            train_labels = train_df[\"label\"].unique()\n",
    "\n",
    "            # Randomly select n labels to move samples from\n",
    "            labels_to_move = rng.choice(train_labels, size=min(n, len(train_labels)), replace=False)\n",
    "\n",
    "            for label in labels_to_move:\n",
    "                train_label_group = train_df[train_df[\"label\"] == label]\n",
    "                test_label_group = test_df[test_df[\"label\"] == label]\n",
    "\n",
    "                updated_train, updated_test = stripe_samples(\n",
    "                    train_label_group, test_label_group, min_samples, max_samples, rng\n",
    "                )\n",
    "\n",
    "                train_df = pd.concat([train_df[train_df[\"label\"] != label], updated_train], ignore_index=True)\n",
    "                test_df = pd.concat([test_df[test_df[\"label\"] != label], updated_test], ignore_index=True)\n",
    "\n",
    "        splits.append((train_df, test_df))\n",
    "\n",
    "    return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "from typing import List, Tuple\n",
    "\n",
    "\n",
    "def knn_openset_recognition(\n",
    "    splits: List[Tuple[pd.DataFrame, pd.DataFrame]],\n",
    "    n_neighbors_range: List[int] = [1, 3, 5, 7],\n",
    "    threshold_range: List[float] = np.arange(0.1, 1.0, 0.1),\n",
    "):\n",
    "    \"\"\"\n",
    "    Perform KNN + threshold grid search for open-set recognition.\n",
    "\n",
    "    Args:\n",
    "    splits (List[Tuple[pd.DataFrame, pd.DataFrame]]): List of (train, test) dataframe pairs\n",
    "    n_neighbors_range (List[int]): Range of n_neighbors to search\n",
    "    threshold_range (List[float]): Range of thresholds to search\n",
    "\n",
    "    Returns:\n",
    "    dict: Best parameters and scores\n",
    "    \"\"\"\n",
    "    best_params = {\"n_neighbors\": 0, \"threshold\": 0}\n",
    "    best_score = {\"f1\": 0, \"auroc\": 0, \"combined\": 0}\n",
    "\n",
    "    for train_df, test_df in splits:\n",
    "        # Prepare the data\n",
    "        X_train = np.stack(train_df[\"embedding\"].values)\n",
    "        y_train = train_df[\"label\"].values\n",
    "        X_test = np.stack(test_df[\"embedding\"].values)\n",
    "        y_test = test_df[\"label\"].values\n",
    "\n",
    "        # Standardize the features\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "        # Identify known and unknown labels\n",
    "        known_labels = set(y_train)\n",
    "        unknown_labels = set(y_test) - known_labels\n",
    "        y_test_binary = np.where(np.isin(y_test, list(unknown_labels)), 1, 0)\n",
    "\n",
    "        for n_neighbors in n_neighbors_range:\n",
    "            # Train KNN\n",
    "            knn = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
    "            knn.fit(X_train_scaled, y_train)\n",
    "\n",
    "            # Get distances and predictions\n",
    "            distances, indices = knn.kneighbors(X_test_scaled)\n",
    "            probabilities = knn.predict_proba(X_test_scaled)\n",
    "\n",
    "            for threshold in threshold_range:\n",
    "                # Classify based on threshold\n",
    "                y_pred = np.where(\n",
    "                    np.max(probabilities, axis=1) >= threshold, knn.classes_[np.argmax(probabilities, axis=1)], -1\n",
    "                )  # -1 for unknown\n",
    "\n",
    "                # Calculate metrics\n",
    "                known_mask = y_pred != -1\n",
    "                f1 = f1_score(y_test[known_mask], y_pred[known_mask], average=\"weighted\")\n",
    "\n",
    "                # For AUROC, use the max probability as the score\n",
    "                auroc = roc_auc_score(y_test_binary, 1 - np.max(probabilities, axis=1))\n",
    "\n",
    "                # Combine metrics (you can adjust the weights as needed)\n",
    "                combined_score = 0.5 * f1 + 0.5 * auroc\n",
    "\n",
    "                # Update best parameters if better score is found\n",
    "                if combined_score > best_score[\"combined\"]:\n",
    "                    best_params[\"n_neighbors\"] = n_neighbors\n",
    "                    best_params[\"threshold\"] = threshold\n",
    "                    best_score[\"f1\"] = f1\n",
    "                    best_score[\"auroc\"] = auroc\n",
    "                    best_score[\"combined\"] = combined_score\n",
    "\n",
    "    return best_params, best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.calibration import label_binarize\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score, average_precision_score, roc_auc_score\n",
    "from typing import List, Tuple, Dict\n",
    "from gorillatracker.classification.clustering import EXT_MERGED_DF\n",
    "from gorillatracker.classification.metrics import analyse_embedding_space\n",
    "\n",
    "# Assuming generate_folds and split functions are available from the previous code\n",
    "# If not, please provide their implementations\n",
    "\n",
    "\n",
    "def knn_openset_recognition(\n",
    "    dataset: pd.DataFrame,\n",
    "    queryset: pd.DataFrame,\n",
    "    thresholds: List[float],\n",
    "    method: str = \"knn1\",\n",
    "    snapshot: List[float] = None,\n",
    ") -> Dict[float, Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Perform KNN + threshold grid search for open-set recognition.\n",
    "\n",
    "    Args:\n",
    "    dataset (pd.DataFrame): Training dataset with 'label' and 'embedding' columns\n",
    "    queryset (pd.DataFrame): Query dataset with 'label' and 'embedding' columns\n",
    "    thresholds (List[float]): List of thresholds to search\n",
    "    k (int): Number of neighbors to consider (default: 5)\n",
    "    method (str): Method to use for classification ('knn1', 'knn5', ...)\n",
    "    snapshot (List[float]): List of thresholds to store results for.\n",
    "\n",
    "    Returns:\n",
    "    Dict[float, Dict[str, float]]: Results for each threshold\n",
    "    \"\"\"\n",
    "    # Prepare the data\n",
    "    X_train = np.stack(dataset[\"embedding\"].values)\n",
    "    y_train = dataset[\"label\"].values\n",
    "    X_query = np.stack(queryset[\"embedding\"].values)\n",
    "    y_query = queryset[\"label\"].values\n",
    "\n",
    "    # Standardize the features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_query_scaled = scaler.transform(X_query)\n",
    "\n",
    "    # Fit the NearestNeighbors model\n",
    "    nbrs = NearestNeighbors(n_neighbors=5).fit(X_train_scaled)\n",
    "\n",
    "    # Find the nearest neighbors for queryset\n",
    "    distances, indices = nbrs.kneighbors(X_query_scaled)\n",
    "\n",
    "    results = {}\n",
    "    for t in thresholds:\n",
    "        if method == \"knn1\":\n",
    "            predictions = knn1_predict(dataset, indices, distances, t)\n",
    "        elif method == \"knn5\":\n",
    "            predictions = knn5_predict(dataset, indices, distances, t)\n",
    "        elif method == \"knn5distance\":\n",
    "            predictions = knn_weighted_distance_predict(dataset, indices, distances, t)\n",
    "        elif method == \"knn1centroid\":\n",
    "            raise NotImplementedError(\"Method not implemented\")\n",
    "        elif method == \"knn1centroid_iqr\":\n",
    "            raise NotImplementedError(\"Method not implemented\")\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown method: {method}\")\n",
    "\n",
    "        # print(t)\n",
    "        # print(y_query, predictions)\n",
    "\n",
    "        results[t] = compute_metrics(y_query, predictions, dataset[\"label\"].unique())\n",
    "\n",
    "        if snapshot:\n",
    "            if any(s for s in snapshot if math.isclose(t, s, abs_tol=1e-6)):\n",
    "                results[t][\"y_true\"] = y_query\n",
    "                results[t][\"y_pred\"] = predictions\n",
    "    return results\n",
    "\n",
    "\n",
    "def knn1_predict(dataset, indices, distances, threshold):\n",
    "    return dataset.iloc[indices[:, 0]][\"label\"].where(distances[:, 0] <= threshold, -1).values\n",
    "\n",
    "\n",
    "def knn5_predict(dataset, indices, distances, threshold):\n",
    "    predictions = []\n",
    "    for idx, dist in zip(indices, distances):\n",
    "        valid = dataset.iloc[idx][dist <= threshold]\n",
    "        if not valid.empty:\n",
    "            prediction = valid[\"label\"].mode()[0]\n",
    "        else:\n",
    "            prediction = -1\n",
    "        predictions.append(prediction)\n",
    "    return np.array(predictions)\n",
    "\n",
    "\n",
    "def knn_weighted_distance_predict(dataset, indices, distances, threshold):\n",
    "    predictions = []\n",
    "    for idx, dist in zip(indices, distances):\n",
    "        if dist[0] > threshold:\n",
    "            predictions.append(-1)\n",
    "        else:\n",
    "            weights = 1 / (1 + dist)\n",
    "            labels = dataset.iloc[idx][\"label\"]\n",
    "            df = pd.DataFrame({\"label\": labels, \"weight\": weights})\n",
    "            weighted_votes = df.groupby(\"label\")[\"weight\"].sum()\n",
    "            predictions.append(weighted_votes.idxmax())\n",
    "    return np.array(predictions)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "\n",
    "def compute_metrics(y_true, y_pred, unique_labels):\n",
    "    \"\"\"\n",
    "    Compute accuracy and macro F1-score for new vs. known classification and multiclass classification among known classes.\n",
    "    New Class has label -1. The other classes are multiclass.\n",
    "\n",
    "    Args:\n",
    "    y_true (array-like): True labels\n",
    "    y_pred (array-like): Predicted labels\n",
    "    unique_labels (array-like): List of known class labels\n",
    "\n",
    "    Returns:\n",
    "    dict: Dictionary containing computed metrics\n",
    "\n",
    "\n",
    "    T|F\n",
    "    1|1 - Correct\n",
    "    1|2 - Incorrect known\n",
    "    1|-1 - Incorrect new & new vs. known\n",
    "    -1|1 - Incorrect known & new vs. known\n",
    "    -1|-1 - Correct new\n",
    "\n",
    "    multiclass -> always correct if left equal to right side (no matter if new or known class, it's just classification over n+1)\n",
    "    \"\"\"\n",
    "    # Convert to numpy arrays for easier manipulation\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "\n",
    "    # Create binary labels for new vs. known classification\n",
    "    y_true_binary = np.where(y_true == -1, -1, 1)\n",
    "    y_pred_binary = np.where(y_pred == -1, -1, 1)\n",
    "\n",
    "    # Compute metrics for binary classification (new vs. known)\n",
    "    binary_accuracy = accuracy_score(y_true_binary, y_pred_binary)\n",
    "    binary_f1 = f1_score(y_true_binary, y_pred_binary, labels=[-1, 1])\n",
    "\n",
    "    # Compute metrics for multiclass classification (only for known classes)\n",
    "    mask_true = y_true != -1\n",
    "    mask_pred = y_pred != -1\n",
    "    mask = mask_true & mask_pred\n",
    "\n",
    "    only_known_accuracy = accuracy_score(y_true[mask], y_pred[mask])\n",
    "    # we will not add the '-1' new class to the labels; this f1 is only about the known classes\n",
    "    # we want to exclude the 'new' class from the f1 calculation via labels= (note we are not using the mask here)\n",
    "    # this will give us the f1 score over the known classes only [zero_division=1 is used to allow labels to be missing]\n",
    "    only_known_f1 = f1_score(y_true, y_pred, labels=unique_labels, average=\"macro\", zero_division=1)\n",
    "\n",
    "    # Create a dictionary to store the metrics\n",
    "    metrics = {\n",
    "        # how often was new class predicted correctly\n",
    "        # threshold graph: should start at 1 and go down to 0\n",
    "        \"new_vs_all_accuracy\": binary_accuracy,\n",
    "        # threshold graph: should start at 1 and go down to 0\n",
    "        \"new_vs_all_f1\": binary_f1,\n",
    "        # t=-1 or p=-1 will be excluded\n",
    "        \"only_known_accuracy\": only_known_accuracy,\n",
    "        \"only_known_f1\": only_known_f1,\n",
    "        # normal multiclass over n+1 classes\n",
    "        # threshold graph: should start at\n",
    "        \"multiclass_accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"multiclass_f1\": f1_score(y_true, y_pred, average=\"macro\"),\n",
    "        \"multiclass_f1_weighted\": f1_score(y_true, y_pred, average=\"weighted\"),\n",
    "    }\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def with_centroid_queryset(dataset, queryset, function, *args, **kwargs):\n",
    "    def calculate_centroid(embeddings):\n",
    "        return np.mean(np.vstack(embeddings), axis=0)\n",
    "\n",
    "    centroid_qs = queryset.groupby(\"label\")[\"embedding\"].apply(calculate_centroid).reset_index()\n",
    "    centroid_qs.columns = [\"label\", \"embedding\"]\n",
    "    return function(dataset, centroid_qs, *args, **kwargs)\n",
    "\n",
    "\n",
    "def run_knn_openset_recognition_cv(\n",
    "    thresholds: List[float],\n",
    "    df: pd.DataFrame,\n",
    "    k_fold: int = 5,\n",
    "    n: int = 3,\n",
    "    min_samples: int = 1,\n",
    "    max_samples: int = 5,\n",
    "    seed: int = 42,\n",
    "    method: str = \"knn1\",\n",
    "    snapshots: List[float] = None,\n",
    ") -> Dict[float, Dict[str, List[float]]]:\n",
    "    \"\"\"\n",
    "    Run KNN open-set recognition with cross-validation.\n",
    "\n",
    "    Args:\n",
    "    df (pd.DataFrame): Input dataframe\n",
    "    k_fold (int): Number of folds for cross-validation\n",
    "    n, min_samples, max_samples, seed: Parameters for the split function\n",
    "    thresholds (List[float]): List of thresholds to search\n",
    "    knn_k (int): Number of neighbors for KNN\n",
    "    method (str): KNN method to use\n",
    "\n",
    "    Returns:\n",
    "    Dict[float, Dict[str, List[float]]]: Cross-validation results for each threshold\n",
    "    \"\"\"\n",
    "    new_label = -1\n",
    "    assert new_label not in df[\"label\"].unique(), \"New label already exists in the dataframe\"\n",
    "    splits = split(df, k=k_fold, n=n, min_samples=min_samples, max_samples=max_samples, seed=seed)\n",
    "\n",
    "    cv_results = defaultdict(lambda: defaultdict(list))\n",
    "    for train_df, test_df in splits:\n",
    "        classes_total = test_df[\"label\"].nunique()\n",
    "        classes_new = test_df[test_df[\"is_new\"]][\"label\"].nunique()\n",
    "        images_total = test_df.count()\n",
    "        images_new = test_df[test_df[\"is_new\"]].count()\n",
    "        # set all test_df labels to new_label if is_new column set\n",
    "        test_df.loc[test_df[\"is_new\"], \"label\"] = new_label\n",
    "        test_df.loc[test_df[\"is_new\"], \"label_string\"] = \"new\"\n",
    "        fold_results = knn_openset_recognition(train_df, test_df, thresholds, method=method, snapshot=snapshots)\n",
    "        for t, metrics in fold_results.items():\n",
    "            for metric, value in metrics.items():\n",
    "                cv_results[t][metric].append(value)\n",
    "\n",
    "                cv_results[t][\"count_queryset_images_new\"].append(images_new)\n",
    "                cv_results[t][\"count_queryset_classes_new\"].append(classes_new)\n",
    "                cv_results[t][\"count_queryset_images_total\"].append(images_total)\n",
    "                cv_results[t][\"count_queryset_classes_total\"].append(classes_total)\n",
    "\n",
    "    return cv_results\n",
    "\n",
    "\n",
    "edf = EXT_MERGED_DF\n",
    "\n",
    "# Filter and prepare the data\n",
    "df = generate_folds(\n",
    "    edf[(edf[\"dataset\"] == \"SPAC+min3+max10\") & (edf[\"model\"] == \"ViT-Finetuned\")].reset_index(drop=True)\n",
    ")\n",
    "analysis = analyse_embedding_space(df)\n",
    "max_distance = analysis[\"global_max_dist\"]\n",
    "min_distance = analysis[\"global_min_dist\"]\n",
    "# Set up parameters\n",
    "thresholds = np.linspace(0, max_distance + 10, 30)\n",
    "method = \"knn1\"\n",
    "\n",
    "# Run cross-validation\n",
    "cv_results = run_knn_openset_recognition_cv(thresholds, df, method=method, n=30, max_samples=2)\n",
    "# TODO(liamvdv): find best threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(true, pred):\n",
    "    results = compute_metrics(true, pred, unique_labels)\n",
    "    for metric, value in results.items():\n",
    "        print(f\"{metric}: {value}\")\n",
    "\n",
    "\n",
    "# Define our known class labels\n",
    "unique_labels = [0, 1, 2]\n",
    "\n",
    "print(\"Perfect classification\")\n",
    "y_true = np.array([0, 1, 2, -1])\n",
    "y_pred = np.array([0, 1, 2, -1])\n",
    "test(y_true, y_pred)\n",
    "\n",
    "print(\"\\nLabel in Unique not in True nor Pred\")\n",
    "# tricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior. _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
    "y_true = np.array([0, 1, 1, -1])\n",
    "y_pred = np.array([0, 1, 1, -1])\n",
    "test(y_true, y_pred)\n",
    "\n",
    "print(\"\\nMisclassify a known class as new class\")\n",
    "y_true = np.array([0, 1, 2, -1, -1])\n",
    "y_pred = np.array([0, 1, 1, -1, 1])\n",
    "test(y_true, y_pred)\n",
    "\n",
    "print(\"\\n+Misclassify a known class as a different known\")\n",
    "y_true = np.array([0, 1, 2, -1])\n",
    "y_pred = np.array([0, 1, 1, -1])\n",
    "test(y_true, y_pred)\n",
    "\n",
    "\n",
    "print(\"\\n+asdfasdf\")\n",
    "y_true = np.array([0, 1, 2, 2])\n",
    "y_pred = np.array([0, 1, 1, -1])\n",
    "test(y_true, y_pred)\n",
    "\n",
    "print(\"\\n+Only New\")\n",
    "y_true = np.array([0, 1, 2, -1])\n",
    "y_pred = np.array([-1, -1, -1, -1])\n",
    "test(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from typing import Dict, List\n",
    "import math\n",
    "\n",
    "\n",
    "def visualize_metrics(cv_results: Dict[float, Dict[str, List[float]]], thresholds: List[float]):\n",
    "    \"\"\"\n",
    "    Visualize metrics from cross-validation results, handling None values,\n",
    "    and combining count_* metrics into a single subplot.\n",
    "\n",
    "    Args:\n",
    "    cv_results (Dict[float, Dict[str, List[float]]]): Cross-validation results\n",
    "    thresholds (List[float]): List of thresholds used\n",
    "\n",
    "    Returns:\n",
    "    None (displays the plot)\n",
    "    \"\"\"\n",
    "    metrics = list(cv_results[thresholds[0]].keys())\n",
    "    count_metrics = [m for m in metrics if m.startswith(\"count_\")]\n",
    "    other_metrics = [m for m in metrics if not m.startswith(\"count_\")]\n",
    "\n",
    "    num_metrics = len(other_metrics) + 1  # +1 for the combined count metrics\n",
    "    num_cols = 3\n",
    "    num_rows = math.ceil(num_metrics / num_cols)\n",
    "\n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, 5 * num_rows))\n",
    "    fig.suptitle(\"Metrics across Different Thresholds\", fontsize=16)\n",
    "\n",
    "    # Flatten axes array for easier indexing\n",
    "    axes = axes.flatten() if num_rows > 1 else [axes]\n",
    "\n",
    "    # Plot other metrics\n",
    "    for i, metric in enumerate(other_metrics):\n",
    "        ax = axes[i]\n",
    "\n",
    "        valid_thresholds = []\n",
    "        mean_values = []\n",
    "        std_values = []\n",
    "\n",
    "        for t in thresholds:\n",
    "            values = [v for v in cv_results[t][metric] if v is not None]\n",
    "            if values:\n",
    "                valid_thresholds.append(t)\n",
    "                mean_values.append(np.mean(values))\n",
    "                std_values.append(np.std(values))\n",
    "\n",
    "        if valid_thresholds:\n",
    "            ax.plot(valid_thresholds, mean_values, marker=\"o\")\n",
    "            ax.fill_between(\n",
    "                valid_thresholds,\n",
    "                [m - s for m, s in zip(mean_values, std_values)],\n",
    "                [m + s for m, s in zip(mean_values, std_values)],\n",
    "                alpha=0.2,\n",
    "            )\n",
    "\n",
    "        ax.set_xlabel(\"Threshold\")\n",
    "        ax.set_ylabel(metric)\n",
    "        ax.set_title(metric)\n",
    "        ax.grid(True, linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "    # Plot combined count metrics\n",
    "    ax = axes[len(other_metrics)]\n",
    "    ax.set_title(\"Count Metrics\")\n",
    "    ax.set_xlabel(\"Threshold\")\n",
    "    ax.set_ylabel(\"Count\")\n",
    "\n",
    "    for metric in count_metrics:\n",
    "        mean_values = []\n",
    "        std_values = []\n",
    "\n",
    "        for t in thresholds:\n",
    "            values = [v for v in cv_results[t][metric] if v is not None]\n",
    "            if values:\n",
    "                mean_values.append(np.mean(values))\n",
    "                std_values.append(np.std(values))\n",
    "            else:\n",
    "                mean_values.append(np.nan)\n",
    "                std_values.append(np.nan)\n",
    "\n",
    "        ax.plot(thresholds, mean_values, marker=\"o\", label=metric)\n",
    "        ax.fill_between(\n",
    "            thresholds,\n",
    "            [m - s for m, s in zip(mean_values, std_values)],\n",
    "            [m + s for m, s in zip(mean_values, std_values)],\n",
    "            alpha=0.2,\n",
    "        )\n",
    "\n",
    "    ax.legend()\n",
    "    ax.grid(True, linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "    # Remove any unused subplots\n",
    "    for i in range(num_metrics, len(axes)):\n",
    "        fig.delaxes(axes[i])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "visualize_metrics(cv_results, thresholds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "-1 for new class.\n",
    "The threshold will make the new class at first be assigned to too many samples, later at too little (as threshold grows larger that max-cross-point distance)\n",
    "\n",
    "multiclass_* look at all classes (including 'new')\n",
    "multiclass_accuracy - how often are our predictions correct (compare for same value in true|pred columns)\n",
    "multiclass_f1 - is macro weighted: all classes have same importance. \n",
    "multiclass_f1_weighted - is weighted by sample count At the start we should see only -1 in resultset, i.e. strong class imbalance. \n",
    "\n",
    "only_known_* looks \n",
    "\n",
    "\n",
    "new_* looks at '-1' vs rest. It binarizes both columns to 0/1 [actually 1, -1] and then checks for equality.\n",
    "new_precision will start at % of non-new images (they are classified wrong, as all have 'new' label). \n",
    "\n",
    "\n",
    "=> Precision is screwed towards the number of samples. E. g. 50% new / 50% known will place starting precision at 0.5; it will then grow to an optimum. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
