{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering, HDBSCAN\n",
    "from sklearn.metrics import adjusted_rand_score, confusion_matrix, f1_score, precision_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from gorillatracker.classification.metrics import analyse_embedding_space, formatted_names\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from scipy.interpolate import UnivariateSpline\n",
    "from tempfile import mkdtemp\n",
    "\n",
    "\n",
    "def calculate_metrics(embeddings: np.ndarray, labels: np.ndarray, true_labels: np.ndarray) -> dict:\n",
    "    \"\"\"wraps analyse_embedding_space and adds class-weighted F1 score and precision\"\"\"\n",
    "    assert len(labels) == len(true_labels) == len(embeddings)\n",
    "    df = pd.DataFrame({\"embedding\": embeddings.tolist(), \"label\": labels.tolist()})\n",
    "    metrics = analyse_embedding_space(df)\n",
    "\n",
    "    # \"label matching problem\" in clustering evaluation\n",
    "    matched_labels = match_labels(true_labels, labels)\n",
    "\n",
    "    # Compute class-weighted F1 score\n",
    "    f1 = f1_score(true_labels, matched_labels, average=\"weighted\")\n",
    "\n",
    "    # Compute class-weighted precision\n",
    "    precision = precision_score(true_labels, matched_labels, average=\"weighted\")\n",
    "\n",
    "    # NOTE(liamvdv): not over matched labels, can handle arbitrary cluster labels\n",
    "    # https://en.wikipedia.org/wiki/Rand_index#/media/File:Example_for_Adjusted_Rand_index.svg\n",
    "    # https://scikit-learn.org/stable/modules/generated/sklearn.metrics.adjusted_rand_score.html\n",
    "    # The adjusted Rand index is thus ensured to have a value close to 0.0 for random labeling independently of the number of clusters and samples and exactly 1.0 when the clusterings are identical (up to a permutation). The adjusted Rand index is bounded below by -0.5 for especially discordant clusterings.\n",
    "    ars = adjusted_rand_score(true_labels, labels)\n",
    "\n",
    "    metrics.update({\"weighted_f1_score\": f1, \"weighted_precision\": precision, \"adjusted_rand_score\": ars})\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def match_labels(true_labels, predicted_labels):\n",
    "    \"\"\"\n",
    "    Match predicted cluster labels to true labels using the Hungarian algorithm.\n",
    "\n",
    "    NOTE(liamvdv): Necessary because cluster labels are arbitrary and may not match the true labels but represent the same clusters.\n",
    "    \"\"\"\n",
    "    assert len(true_labels) == len(predicted_labels)\n",
    "\n",
    "    # Create confusion matrix\n",
    "    cm = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "    # Use the Hungarian algorithm to find the best matching\n",
    "    row_ind, col_ind = linear_sum_assignment(-cm)\n",
    "\n",
    "    label_mapping = {pred: true for pred, true in zip(col_ind, row_ind)}\n",
    "\n",
    "    matched_labels = np.array([label_mapping.get(label, label) for label in predicted_labels])\n",
    "\n",
    "    return matched_labels\n",
    "\n",
    "\n",
    "def find_elbow(inertias: list[float]) -> int:\n",
    "    \"\"\"Find the elbow point in a list of inertia (wcss) values.\"\"\"\n",
    "    # Create x-axis values (number of clusters)\n",
    "    x: np.ndarray = np.arange(1, len(inertias) + 1)\n",
    "\n",
    "    # Fit a smooth curve to the inertia data\n",
    "    spline: UnivariateSpline = UnivariateSpline(x, inertias, s=0)\n",
    "\n",
    "    # Calculate the second derivative\n",
    "    second_derivative: np.ndarray = spline.derivative(n=2)(x)\n",
    "\n",
    "    # Find the point of maximum curvature (elbow)\n",
    "    elbow: int = np.argmax(np.abs(second_derivative)) + 1\n",
    "\n",
    "    return elbow\n",
    "\n",
    "\n",
    "def _run_kmeans(args):\n",
    "    scaled_embeddings, k, true_labels = args\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    labels = kmeans.fit_predict(scaled_embeddings)\n",
    "    metric = calculate_metrics(scaled_embeddings, labels, true_labels)\n",
    "    metric[\"algorithm\"] = \"K-means\"\n",
    "    metric[\"algorithm_arg\"] = k\n",
    "    return metric\n",
    "\n",
    "\n",
    "def find_optimal_kmeans_parallel(embeddings, true_labels, max_clusters=30):\n",
    "    scaler = StandardScaler()\n",
    "    scaled_embeddings = scaler.fit_transform(embeddings)\n",
    "\n",
    "    # Prepare arguments for parallel processing\n",
    "    args_list = [(scaled_embeddings, k, true_labels) for k in range(2, max_clusters + 1)]\n",
    "\n",
    "    metrics = []\n",
    "\n",
    "    with ProcessPoolExecutor(max_workers=10) as executor:\n",
    "        # Submit all jobs\n",
    "        future_to_k = {executor.submit(_run_kmeans, args): args[1] for args in args_list}\n",
    "\n",
    "        # Collect results as they complete\n",
    "        for future in tqdm(as_completed(future_to_k), total=len(future_to_k), desc=\"Running K-means\"):\n",
    "            k = future_to_k[future]\n",
    "            try:\n",
    "                metric = future.result()\n",
    "                metrics.append(metric)\n",
    "            except Exception as exc:\n",
    "                print(f\"K-means for k={k} generated an exception: {exc}\")\n",
    "\n",
    "    # Sort metrics by number of clusters\n",
    "    metrics.sort(key=lambda x: x[\"algorithm_arg\"])\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def find_optimal_kmeans(embeddings, true_labels, max_clusters=30, step=1):\n",
    "    scaler = StandardScaler()\n",
    "    scaled_embeddings = scaler.fit_transform(embeddings)\n",
    "\n",
    "    metrics = []\n",
    "\n",
    "    for k in tqdm(range(2, max_clusters + 1, step), desc=\"Running K-means\", total=max_clusters // step):\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "        labels = kmeans.fit_predict(scaled_embeddings)\n",
    "        metric = calculate_metrics(scaled_embeddings, labels, true_labels)\n",
    "        metric[\"algorithm\"] = \"K-means\"\n",
    "        metric[\"algorithm_arg\"] = k\n",
    "        metrics.append(metric)\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def find_optimal_eps_for_dbscan(embeddings):\n",
    "    scaler = StandardScaler()\n",
    "    scaled_embeddings = scaler.fit_transform(embeddings)\n",
    "\n",
    "    # Find optimal eps\n",
    "    \"\"\"\n",
    "    The purpose of this code is to create what's called a \"k-distance graph\". This graph, when plotted, shows the distance to the nearest neighbor for each point, sorted in ascending order. It's used to help determine a good value for the 'eps' parameter in DBSCAN.\n",
    "    \n",
    "    In a k-distance graph, you typically look for an \"elbow\" - a point where the distance starts increasing more rapidly. This elbow often indicates a good value for 'eps'. Points before the elbow are considered \"close\" to their neighbors and might form clusters, while points after the elbow are farther from their neighbors and might be considered noise or outliers.\n",
    "    \"\"\"\n",
    "    neighbors = NearestNeighbors(n_neighbors=2)\n",
    "    neighbors_fit = neighbors.fit(scaled_embeddings)\n",
    "    distances, indices = neighbors_fit.kneighbors(scaled_embeddings)\n",
    "    distances = np.sort(distances, axis=0)\n",
    "    distances = distances[:, 1]\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(distances)\n",
    "    plt.xlabel(\"Points\")\n",
    "    plt.ylabel(\"Distance\")\n",
    "    plt.title(\"K-distance Graph\")\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Please examine the K-distance graph and input the 'elbow' point for eps:\")\n",
    "    eps = float(input(\"Enter the eps value: \"))\n",
    "\n",
    "    return eps\n",
    "\n",
    "\n",
    "def find_optimal_dbscan(embeddings, true_labels, eps: float):\n",
    "    scaler = StandardScaler()\n",
    "    scaled_embeddings = scaler.fit_transform(embeddings)\n",
    "\n",
    "    # Find optimal min_samples\n",
    "    metrics = []\n",
    "    min_samples_range = range(2, 11)\n",
    "\n",
    "    for min_samples in min_samples_range:\n",
    "        dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "        labels = dbscan.fit_predict(scaled_embeddings)\n",
    "        metric = calculate_metrics(scaled_embeddings, labels, true_labels)\n",
    "        metric[\"algorithm\"] = \"DBSCAN\"\n",
    "        metric[\"algorithm_arg\"] = min_samples\n",
    "        metrics.append(metric)\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def find_optimal_hdbscan(\n",
    "    embeddings: np.ndarray, true_labels, min_cluster_size_range: range = range(2, 10), min_samples: int = None\n",
    "):\n",
    "    scaler = StandardScaler()\n",
    "    scaled_embeddings = scaler.fit_transform(embeddings)\n",
    "\n",
    "    metrics = []\n",
    "\n",
    "    for min_cluster_size in tqdm(min_cluster_size_range, desc=\"Running HDBSCAN\"):\n",
    "        hdbscan = HDBSCAN(min_cluster_size=min_cluster_size, min_samples=min_samples)\n",
    "        labels = hdbscan.fit_predict(scaled_embeddings)\n",
    "\n",
    "        metric = calculate_metrics(scaled_embeddings, labels, true_labels)\n",
    "        metric[\"algorithm\"] = \"HDBSCAN\"\n",
    "        metric[\"algorithm_arg\"] = min_cluster_size\n",
    "        metrics.append(metric)\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def find_optimal_agglomerative(embeddings: np.ndarray, true_labels, max_clusters: int = 30, step: int = 1):\n",
    "    scaler = StandardScaler()\n",
    "    scaled_embeddings = scaler.fit_transform(embeddings)\n",
    "\n",
    "    # Create a temporary directory for caching\n",
    "    cachedir = mkdtemp()\n",
    "    metrics = []\n",
    "\n",
    "    # Iterate from 2 to max_clusters\n",
    "    for k in tqdm(range(2, max_clusters + 1, step), desc=\"Running Agglomerative\", total=max_clusters // step):\n",
    "        # Perform Agglomerative Clustering with k clusters\n",
    "        # TODO(liamvdv): use cahcing: cachedir = mkdtemp(); memory = Memory(cachedir=cachedir, verbose=0)\n",
    "        agg_clustering = AgglomerativeClustering(n_clusters=k, linkage=\"ward\", memory=cachedir)\n",
    "        labels = agg_clustering.fit_predict(scaled_embeddings)\n",
    "\n",
    "        metric = calculate_metrics(scaled_embeddings, labels, true_labels)\n",
    "        metric[\"algorithm\"] = \"Agglomerative Clustering\"\n",
    "        metric[\"algorithm_arg\"] = k\n",
    "        metrics.append(metric)\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "# agg_metrics = find_optimal_agglomerative(embeddings, true_labels, max_clusters=30)\n",
    "# eps = find_optimal_eps_for_dbscan(embeddings)\n",
    "# dbscan_metrics = find_optimal_dbscan(embeddings, true_labels, eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "\n",
    "def visualize_alg_metrics(in_metrics, formatted_names):\n",
    "    \"\"\"\n",
    "    Create a grid of charts where every metric is shown for in_metrics.\n",
    "\n",
    "    Parameters:\n",
    "    in_metrics (list): List of dictionaries containing metrics for each run\n",
    "    formatted_names (dict): Dictionary mapping metric names to formatted display names\n",
    "\n",
    "    Returns:\n",
    "    None (displays the plot)\n",
    "    \"\"\"\n",
    "    alg = in_metrics[0][\"algorithm\"]\n",
    "    # Get the list of metrics (excluding 'algorithm' and 'algorithm_arg')\n",
    "    metrics = [\n",
    "        key\n",
    "        for key in in_metrics[0].keys()\n",
    "        if key not in [\"algorithm\", \"algorithm_arg\"] and not key.startswith(\"global_\")\n",
    "    ]\n",
    "\n",
    "    # Calculate the grid dimensions\n",
    "    n_metrics = len(metrics)\n",
    "    n_cols = 5  # You can adjust this for a different layout\n",
    "    n_rows = math.ceil(n_metrics / n_cols)\n",
    "\n",
    "    # Create the plot\n",
    "    fig, axs = plt.subplots(n_rows, n_cols, figsize=(20, 5 * n_rows))\n",
    "\n",
    "    # Flatten the axs array for easier indexing\n",
    "    axs = axs.flatten()\n",
    "\n",
    "    # Plot each metric\n",
    "    for i, metric in enumerate(metrics):\n",
    "        x = [m[\"algorithm_arg\"] for m in in_metrics]\n",
    "        y = [m[metric] for m in in_metrics]\n",
    "\n",
    "        axs[i].plot(x, y, marker=\"o\")\n",
    "        axs[i].set_title(formatted_names.get(metric, metric))\n",
    "        axs[i].set_xlabel(\"Number of Clusters\")\n",
    "        axs[i].set_ylabel(\"Value\")\n",
    "        axs[i].grid(True)\n",
    "\n",
    "    # Remove any unused subplots\n",
    "    for j in range(i + 1, len(axs)):\n",
    "        fig.delaxes(axs[j])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def with_min_label_count(df: pd.DataFrame, min: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create a copy of the DataFrame, keeping only rows where the label appears\n",
    "    at least 'min' times in the original DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Input DataFrame. Must have a 'label' column.\n",
    "    min (int): Minimum number of occurrences for a label to be included.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A new DataFrame with filtered rows.\n",
    "\n",
    "    Raises:\n",
    "    ValueError: If 'label' column is not present in the DataFrame.\n",
    "    \"\"\"\n",
    "    if \"label\" not in df.columns:\n",
    "        raise ValueError(\"DataFrame must have a 'label' column\")\n",
    "\n",
    "    # Count label occurrences\n",
    "    label_counts = df[\"label\"].value_counts()\n",
    "\n",
    "    # Get labels that appear at least 'min' times\n",
    "    valid_labels = label_counts[label_counts >= min].index\n",
    "\n",
    "    # Create a new DataFrame with only the valid labels\n",
    "    filtered_df = df[df[\"label\"].isin(valid_labels)].copy()\n",
    "\n",
    "    return filtered_df\n",
    "\n",
    "\n",
    "def upto_max_label_count(df: pd.DataFrame, max: int, seed: int = 42) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create a copy of the DataFrame, randomly sampling rows for labels that exceed\n",
    "    the specified maximum count, while keeping all rows for labels that don't exceed the maximum.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Input DataFrame. Must have a 'label' column.\n",
    "    max (int): Maximum number of occurrences for each label.\n",
    "    seed (int): Random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A new DataFrame with sampled rows.\n",
    "\n",
    "    Raises:\n",
    "    ValueError: If 'label' column is not present in the DataFrame.\n",
    "    \"\"\"\n",
    "    if \"label\" not in df.columns:\n",
    "        raise ValueError(\"DataFrame must have a 'label' column\")\n",
    "\n",
    "    # Create a local random number generator\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    # Group the DataFrame by label\n",
    "    grouped = df.groupby(\"label\")\n",
    "\n",
    "    sampled_dfs = []\n",
    "\n",
    "    for label, group in grouped:\n",
    "        if len(group) > max:\n",
    "            # Randomly sample 'max' rows from the group\n",
    "            sampled_group = group.sample(n=max, random_state=rng)\n",
    "        else:\n",
    "            # Keep all rows if the count doesn't exceed max\n",
    "            sampled_group = group\n",
    "\n",
    "        sampled_dfs.append(sampled_group)\n",
    "\n",
    "    # Concatenate all sampled groups\n",
    "    if len(sampled_dfs) == 0:\n",
    "        return pd.DataFrame(columns=df.columns)\n",
    "    else:\n",
    "        return pd.concat(sampled_dfs).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MERGED_DF = pd.read_pickle(\"merged.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SPAC  - ViT-Finetuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = MERGED_DF\n",
    "df = df[(df[\"model\"] == \"ViT-Finetuned\") & (df[\"dataset\"] == \"SPAC\")]\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "embeddings = np.stack(df[\"embedding\"].to_numpy())\n",
    "true_labels = df[\"label\"].to_numpy()\n",
    "\n",
    "print(\"Actual Number of Individuals\", len(np.unique(true_labels)))\n",
    "kmeans_metrics = find_optimal_kmeans(embeddings, true_labels, max_clusters=180, step=5)\n",
    "visualize_alg_metrics(kmeans_metrics, formatted_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m3df = with_min_label_count(df, 3)\n",
    "m3embeddings = np.stack(m3df[\"embedding\"].to_numpy())\n",
    "m3true_labels = m3df[\"label\"].to_numpy()\n",
    "\n",
    "print(\"Actual Number of Individuals\", len(np.unique(m3true_labels)))\n",
    "m3kmeans_metrics = find_optimal_kmeans(m3embeddings, m3true_labels, max_clusters=160, step=5)\n",
    "visualize_alg_metrics(m3kmeans_metrics, formatted_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m3ma10df = upto_max_label_count(m3df, 10)\n",
    "m3ma10embeddings = np.stack(m3ma10df[\"embedding\"].to_numpy())\n",
    "m3ma10true_labels = m3ma10df[\"label\"].to_numpy()\n",
    "\n",
    "print(\"Actual Number of Individuals\", len(np.unique(m3ma10true_labels)))\n",
    "m3ma10kmeans_metrics = find_optimal_kmeans(m3ma10embeddings, m3ma10true_labels, max_clusters=160, step=4)\n",
    "visualize_alg_metrics(m3ma10kmeans_metrics, formatted_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = MERGED_DF\n",
    "df = df[(df[\"model\"] == \"ViT-Pretrained\") & (df[\"dataset\"] == \"MNIST\")]\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "embeddings = np.stack(df[\"embedding\"].to_numpy())\n",
    "true_labels = df[\"label\"].to_numpy()\n",
    "\n",
    "print(\"Actual Number of Individuals\", len(np.unique(true_labels)))\n",
    "kmeans_metrics = find_optimal_kmeans(embeddings, true_labels, max_clusters=20, step=1)\n",
    "visualize_alg_metrics(kmeans_metrics, formatted_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthetic Test Small (20 clusters; 20 images per cluster)\n",
    "\n",
    "- Note that we might be off by 1-2 clusters if the starting K-Means initializiations points are in the same cluster. This is true if adjusted_rand_score cannot reach 1. (also reason for large dunn_index drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = MERGED_DF\n",
    "df = df[(df[\"model\"] == \"Synthetic\") & (df[\"dataset\"] == \"Synthetic 20c 20n\")]\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "embeddings = np.stack(df[\"embedding\"].to_numpy())\n",
    "true_labels = df[\"label\"].to_numpy()\n",
    "\n",
    "print(\"Actual Number of Individuals\", len(np.unique(true_labels)))\n",
    "kmeans_metrics = find_optimal_kmeans(embeddings, true_labels, max_clusters=30, step=1)\n",
    "visualize_alg_metrics(kmeans_metrics, formatted_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthetic Test Large (200 clusters; 10 image per cluster)\n",
    "\n",
    "We see very clearly that these metrics perform well at identifying clusters.\n",
    "\n",
    "- min David Bouldin Index\n",
    "- max siluette score\n",
    "- != 0: 2nd derivative of wcss (inertia) [elbow method] perform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = MERGED_DF\n",
    "df = df[(df[\"model\"] == \"Synthetic\") & (df[\"dataset\"] == \"Synthetic 200c 10n\")]\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "embeddings = np.stack(df[\"embedding\"].to_numpy())\n",
    "true_labels = df[\"label\"].to_numpy()\n",
    "\n",
    "print(\"Actual Number of Individuals\", len(np.unique(true_labels)))\n",
    "kmeans_metrics = find_optimal_kmeans(embeddings, true_labels, max_clusters=400, step=20)\n",
    "visualize_alg_metrics(kmeans_metrics, formatted_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agglomorative Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = MERGED_DF\n",
    "df = df[(df[\"model\"] == \"Synthetic\") & (df[\"dataset\"] == \"Synthetic 200c 10n\")]\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "embeddings = np.stack(df[\"embedding\"].to_numpy())\n",
    "true_labels = df[\"label\"].to_numpy()\n",
    "\n",
    "print(\"Actual Number of Individuals\", len(np.unique(true_labels)))\n",
    "agg_metrics = find_optimal_agglomerative(embeddings, true_labels, max_clusters=400, step=20)\n",
    "visualize_alg_metrics(agg_metrics, formatted_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = MERGED_DF\n",
    "df = df[(df[\"model\"] == \"Synthetic\") & (df[\"dataset\"] == \"Synthetic 20c 20n\")]\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "embeddings = np.stack(df[\"embedding\"].to_numpy())\n",
    "true_labels = df[\"label\"].to_numpy()\n",
    "\n",
    "print(\"Actual Number of Individuals\", len(np.unique(true_labels)))\n",
    "agg_metrics = find_optimal_agglomerative(embeddings, true_labels, max_clusters=40, step=2)\n",
    "visualize_alg_metrics(agg_metrics, formatted_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SPAC - ViT-Finetuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = MERGED_DF\n",
    "df = df[(df[\"model\"] == \"ViT-Finetuned\") & (df[\"dataset\"] == \"SPAC\")]\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "embeddings = np.stack(df[\"embedding\"].to_numpy())\n",
    "true_labels = df[\"label\"].to_numpy()\n",
    "\n",
    "print(\"Actual Number of Individuals\", len(np.unique(true_labels)))\n",
    "kmeans_metrics = find_optimal_agglomerative(embeddings, true_labels, max_clusters=180, step=5)\n",
    "visualize_alg_metrics(kmeans_metrics, formatted_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m3df = with_min_label_count(df, 3)\n",
    "m3embeddings = np.stack(m3df[\"embedding\"].to_numpy())\n",
    "m3true_labels = m3df[\"label\"].to_numpy()\n",
    "\n",
    "print(\"Actual Number of Individuals\", len(np.unique(m3true_labels)))\n",
    "m3kmeans_metrics = find_optimal_agglomerative(m3embeddings, m3true_labels, max_clusters=160, step=5)\n",
    "visualize_alg_metrics(m3kmeans_metrics, formatted_names)\n",
    "\n",
    "\n",
    "m3ma10df = upto_max_label_count(m3df, 10)\n",
    "m3ma10embeddings = np.stack(m3ma10df[\"embedding\"].to_numpy())\n",
    "m3ma10true_labels = m3ma10df[\"label\"].to_numpy()\n",
    "\n",
    "print(\"Actual Number of Individuals\", len(np.unique(m3ma10true_labels)))\n",
    "m3ma10kmeans_metrics = find_optimal_agglomerative(m3ma10embeddings, m3ma10true_labels, max_clusters=160, step=5)\n",
    "visualize_alg_metrics(m3ma10kmeans_metrics, formatted_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HDBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = MERGED_DF\n",
    "df = df[(df[\"model\"] == \"Synthetic\") & (df[\"dataset\"] == \"Synthetic 200c 10n\")]\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "embeddings = np.stack(df[\"embedding\"].to_numpy())\n",
    "true_labels = df[\"label\"].to_numpy()\n",
    "\n",
    "print(\"Actual Number of Individuals\", len(np.unique(true_labels)))\n",
    "agg_metrics = find_optimal_hdbscan(embeddings, true_labels, min_cluster_size_range=list(range(2, 40)))\n",
    "visualize_alg_metrics(agg_metrics, formatted_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend_merge_df(df, name: str, min=None, max=None):\n",
    "    lo = with_min_label_count(df, min) if isinstance(min, int) else df\n",
    "    hi = upto_max_label_count(lo, max) if isinstance(max, int) else lo\n",
    "    hi[\"dataset\"] = name\n",
    "    return hi\n",
    "\n",
    "\n",
    "df = MERGED_DF\n",
    "spac_min3_max10 = [\n",
    "    extend_merge_df(df[(df[\"model\"] == model) & (df[\"dataset\"] == \"SPAC\")], \"SPAC+min3+max10\", min=3, max=10)\n",
    "    for model in df[\"model\"].unique().tolist()\n",
    "]\n",
    "spac_min3 = [\n",
    "    extend_merge_df(df[(df[\"model\"] == model) & (df[\"dataset\"] == \"SPAC\")], \"SPAC+min3\", min=3)\n",
    "    for model in df[\"model\"].unique().tolist()\n",
    "]\n",
    "bristol_min25max25 = [\n",
    "    # 25 because every individual has 25 samples\n",
    "    extend_merge_df(df[(df[\"model\"] == model) & (df[\"dataset\"] == \"Bristol\")], \"Bristol+min25+max25\", min=25, max=25)\n",
    "    for model in df[\"model\"].unique().tolist()\n",
    "]\n",
    "dfs = [df, *spac_min3_max10, *spac_min3, *bristol_min25max25]\n",
    "nonempty_dfs = [df for df in dfs if len(df) > 0]\n",
    "EXT_MERGED_DF = pd.concat(nonempty_dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN, SpectralClustering, HDBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from tqdm import tqdm\n",
    "\n",
    "# NOTE(liamvdv): configure the speed level here\n",
    "speed_level = \"fast\"\n",
    "\n",
    "\n",
    "speed_levels = [None, \"detailed\", \"fast\", \"veryfast\"]\n",
    "\n",
    "\n",
    "def speed(*args):\n",
    "    \"\"\"pass upto 3 arguments; the later ones will should indicate more resource usage\"\"\"\n",
    "    global speed_level, speed_levels\n",
    "    speed = speed_levels.index(speed_level)\n",
    "    assert speed != -1, f\"Invalid speed level: '{speed_level}'. Must be one of {speed_levels}\"\n",
    "    return args[:speed][-1]\n",
    "\n",
    "\n",
    "def param_grid(params):\n",
    "    param_combinations = [dict(zip(params.keys(), values)) for values in itertools.product(*params.values())]\n",
    "    return param_combinations\n",
    "\n",
    "\n",
    "def sweep_clustering_algorithms(df, configs):\n",
    "    results = []\n",
    "\n",
    "    for dataset, model, algorithm, param_combinations in tqdm(configs, desc=\"Processing configurations\"):\n",
    "        # Filter the dataframe\n",
    "        subset = df[(df[\"model\"] == model) & (df[\"dataset\"] == dataset)]\n",
    "        subset = subset.reset_index(drop=True)\n",
    "\n",
    "        embeddings = np.stack(subset[\"embedding\"].to_numpy())\n",
    "        true_labels = subset[\"label\"].to_numpy()\n",
    "\n",
    "        # Scale the embeddings\n",
    "        scaler = StandardScaler()\n",
    "        scaled_embeddings = scaler.fit_transform(embeddings)\n",
    "\n",
    "        for params in param_combinations:\n",
    "            if algorithm == \"KMeans\":\n",
    "                clusterer = KMeans(random_state=42, **params)\n",
    "            elif algorithm == \"AgglomerativeClustering\":\n",
    "                clusterer = AgglomerativeClustering(**params)\n",
    "            elif algorithm == \"HDBSCAN\":\n",
    "                clusterer = HDBSCAN(**params)\n",
    "            elif algorithm == \"DBSCAN\":\n",
    "                clusterer = DBSCAN(**params)\n",
    "            elif algorithm == \"GaussianMixture\":\n",
    "                clusterer = GaussianMixture(random_state=42, **params)\n",
    "            elif algorithm == \"SpectralClustering\":\n",
    "                clusterer = SpectralClustering(random_state=42, **params)\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported algorithm: {algorithm}\")\n",
    "\n",
    "            labels = clusterer.fit_predict(scaled_embeddings)\n",
    "\n",
    "            metric = calculate_metrics(scaled_embeddings, labels, true_labels)\n",
    "            metric.update(\n",
    "                {\n",
    "                    \"dataset\": dataset,\n",
    "                    \"model\": model,\n",
    "                    \"algorithm\": algorithm,\n",
    "                    \"algorithm_params\": params,\n",
    "                    \"n_clusters\": len(np.unique(labels[labels != -1])),  # Excluding noise points\n",
    "                    \"n_true_clusters\": len(np.unique(true_labels)),\n",
    "                }\n",
    "            )\n",
    "            results.append(metric)\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "configs = []\n",
    "\n",
    "# Add configurations for correctness assuring synthetic datasets/models\n",
    "synthetic = [\n",
    "    (\"Synthetic 200c 10n\", \"Synthetic\", \"KMeans\", param_grid({\"n_clusters\": range(2, 401, 20)})),\n",
    "    (\"Synthetic 200c 10n\", \"Synthetic\", \"AgglomerativeClustering\", param_grid({\"n_clusters\": range(2, 401, 20)})),\n",
    "    (\n",
    "        \"Synthetic 200c 10n\",\n",
    "        \"Synthetic\",\n",
    "        \"HDBSCAN\",\n",
    "        param_grid({\"min_cluster_size\": range(2, 21), \"min_samples\": [2]}),\n",
    "    ),\n",
    "    (\n",
    "        \"Synthetic 200c 10n\",\n",
    "        \"Synthetic\",\n",
    "        \"DBSCAN\",\n",
    "        param_grid({\"eps\": np.arange(0.1, 2.1, 0.2), \"min_samples\": [2, 4, 8]}),\n",
    "    ),\n",
    "    (\"Synthetic 20c 20n\", \"Synthetic\", \"KMeans\", param_grid({\"n_clusters\": range(2, 44, 2)})),\n",
    "    (\"Synthetic 20c 20n\", \"Synthetic\", \"AgglomerativeClustering\", param_grid({\"n_clusters\": range(2, 44, 2)})),\n",
    "    (\n",
    "        \"Synthetic 20c 20n\",\n",
    "        \"Synthetic\",\n",
    "        \"HDBSCAN\",\n",
    "        param_grid({\"min_cluster_size\": range(2, 10), \"min_samples\": [2]}),\n",
    "    ),\n",
    "    (\n",
    "        \"Synthetic 20c 20n\",\n",
    "        \"Synthetic\",\n",
    "        \"DBSCAN\",\n",
    "        param_grid({\"eps\": np.arange(0.1, 2.1, 0.2), \"min_samples\": [2, 4, 8]}),\n",
    "    ),\n",
    "    # (\"Synthetic 200c 10n\", \"Synthetic\", \"GaussianMixture\", flatten_grid({\"n_components\": range(2, 401, 20), \"covariance_type\": [\"full\", \"tied\", \"diag\", \"spherical\"]})),\n",
    "    # (\"Synthetic 200c 10n\", \"Synthetic\", \"SpectralClustering\" ,flatten_grid( {\"n_clusters\": range(2, 401, 20), \"affinity\": [\"rbf\", \"nearest_neighbors\"]}))\n",
    "]\n",
    "configs.extend(synthetic)\n",
    "\n",
    "# Add SPAC dataset configurations\n",
    "models = [\"ViT-Finetuned\", \"ViT-Pretrained\", \"EfN-Pretrained\", \"EfN-Finetuned\"]\n",
    "spac = [\n",
    "    config\n",
    "    for model in models\n",
    "    for ds in [\"SPAC\", \"SPAC+min3\", \"SPAC+min3+max10\"]\n",
    "    for config in [\n",
    "        (ds, model, \"KMeans\", param_grid({\"n_clusters\": range(2, 181, speed(1, 5, 20))})),\n",
    "        (ds, model, \"AgglomerativeClustering\", param_grid({\"n_clusters\": range(2, 181, speed(1, 5, 20))})),\n",
    "        (ds, model, \"HDBSCAN\", param_grid({\"min_cluster_size\": [2]})),\n",
    "        # (\"SPAC\", model, \"DBSCAN\", param_grid({\"eps\": [0.1, 0.5, 1.0], \"min_samples\": [2, 4, 8]})),\n",
    "        # (\"SPAC\", \"ViT-Finetuned\", \"GaussianMixture\", flatten_grid({\"n_components\": range(2, 181, 5), \"covariance_type\": [\"full\", \"tied\", \"diag\", \"spherical\"]})),\n",
    "        # (\"SPAC\", \"ViT-Finetuned\", \"SpectralClustering\", flatten_grid({\"n_clusters\": range(2, 181, 5), \"affinity\": [\"rbf\", \"nearest_neighbors\"]})),\n",
    "    ]\n",
    "]\n",
    "configs.extend(spac)\n",
    "\n",
    "# Add Bristol dataset configurations\n",
    "bristol = [\n",
    "    config\n",
    "    for model in models\n",
    "    for ds in [\"Bristol\", \"Bristol+min25+max25\"]\n",
    "    for config in [\n",
    "        (ds, model, \"KMeans\", param_grid({\"n_clusters\": range(2, 181, speed(1, 5, 20))})),\n",
    "        (ds, model, \"AgglomerativeClustering\", param_grid({\"n_clusters\": range(2, 181, speed(1, 5, 20))})),\n",
    "        (ds, model, \"HDBSCAN\", param_grid({\"min_cluster_size\": [2]})),\n",
    "        # (\"Bristol\", model, \"DBSCAN\", param_grid({\"eps\": [0.1, 0.5, 1.0], \"min_samples\": [2, 4, 8]})),\n",
    "        # (\"Bristol\", model, \"GaussianMixture\", flatten_grid({\"n_components\": range(2, 181, 5), \"covariance_type\": [\"full\", \"tied\", \"diag\", \"spherical\"]})),\n",
    "        # (\"Bristol\", model, \"SpectralClustering\", flatten_grid({\"n_clusters\": range(2, 181, 5), \"affinity\": [\"rbf\", \"nearest_neighbors\"]})),\n",
    "    ]\n",
    "]\n",
    "configs.extend(bristol)\n",
    "\n",
    "\n",
    "print(\"Number of configurations:\", len(configs))\n",
    "print(\"Number of algorithm configurations:\", sum(len(params) for _, _, _, params in configs))\n",
    "\n",
    "results_df = sweep_clustering_algorithms(MERGED_DF, configs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
