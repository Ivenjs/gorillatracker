{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.cluster import KMeans\n",
    "# from sklearn.metrics import silhouette_score\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# import matplotlib.pyplot as plt\n",
    "# from tqdm import tqdm\n",
    "\n",
    "\n",
    "# def find_optimal_clusters(embeddings, max_clusters=30):\n",
    "#     \"\"\"\n",
    "#     Find the optimal number of clusters using the elbow method and silhouette analysis.\n",
    "\n",
    "#     Args:\n",
    "#     embeddings (np.array): The embedding vectors\n",
    "#     max_clusters (int): The maximum number of clusters to consider\n",
    "\n",
    "#     Returns:\n",
    "#     int: The optimal number of clusters\n",
    "#     \"\"\"\n",
    "#     scaler = StandardScaler()\n",
    "#     scaled_embeddings = scaler.fit_transform(embeddings)\n",
    "\n",
    "#     inertias = []\n",
    "#     silhouette_scores = []\n",
    "\n",
    "#     for k in tqdm(range(2, max_clusters + 1), desc=\"Finding optimal clusters\", total=max_clusters + 1):\n",
    "#         kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "#         kmeans.fit(scaled_embeddings)\n",
    "#         inertias.append(kmeans.inertia_)\n",
    "#         silhouette_scores.append(silhouette_score(scaled_embeddings, kmeans.labels_))\n",
    "\n",
    "#     # Plot elbow curve\n",
    "#     plt.figure(figsize=(12, 5))\n",
    "#     plt.subplot(1, 2, 1)\n",
    "#     plt.plot(range(2, max_clusters + 1), inertias, marker=\"o\")\n",
    "#     plt.xlabel(\"Number of clusters\")\n",
    "#     plt.ylabel(\"Inertia\")\n",
    "#     plt.title(\"Elbow Method\")\n",
    "\n",
    "#     # Plot silhouette scores\n",
    "#     plt.subplot(1, 2, 2)\n",
    "#     plt.plot(range(2, max_clusters + 1), silhouette_scores, marker=\"o\")\n",
    "#     plt.xlabel(\"Number of clusters\")\n",
    "#     plt.ylabel(\"Silhouette Score\")\n",
    "#     plt.title(\"Silhouette Analysis\")\n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "#     # Find the optimal number of clusters\n",
    "#     optimal_clusters = silhouette_scores.index(max(silhouette_scores)) + 2\n",
    "#     print(f\"Optimal number of clusters: {optimal_clusters}\")\n",
    "\n",
    "#     return optimal_clusters\n",
    "\n",
    "\n",
    "# def cluster_embeddings(embeddings, n_clusters):\n",
    "#     \"\"\"\n",
    "#     Perform K-means clustering on the embeddings.\n",
    "\n",
    "#     Args:\n",
    "#     embeddings (np.array): The embedding vectors\n",
    "#     n_clusters (int): The number of clusters to use\n",
    "\n",
    "#     Returns:\n",
    "#     np.array: The cluster labels for each embedding\n",
    "#     \"\"\"\n",
    "#     scaler = StandardScaler()\n",
    "#     scaled_embeddings = scaler.fit_transform(embeddings)\n",
    "\n",
    "#     kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "#     labels = kmeans.fit_predict(scaled_embeddings)\n",
    "\n",
    "#     return labels\n",
    "\n",
    "\n",
    "# def analyze_clusters(df, embeddings, labels):\n",
    "#     \"\"\"\n",
    "#     Analyze the resulting clusters.\n",
    "\n",
    "#     Args:\n",
    "#     df (pd.DataFrame): The original dataframe\n",
    "#     embeddings (np.array): The embedding vectors\n",
    "#     labels (np.array): The cluster labels\n",
    "\n",
    "#     Returns:\n",
    "#     None\n",
    "#     \"\"\"\n",
    "#     df[\"cluster\"] = labels\n",
    "\n",
    "#     print(\"\\nCluster Analysis:\")\n",
    "#     for cluster in range(max(labels) + 1):\n",
    "#         cluster_df = df[df[\"cluster\"] == cluster]\n",
    "#         print(f\"\\nCluster {cluster}:\")\n",
    "#         print(f\"  Size: {len(cluster_df)}\")\n",
    "#         print(\"  Top 5 most common original labels:\")\n",
    "#         print(cluster_df[\"label\"].value_counts().head().to_string())\n",
    "\n",
    "#     # Visualize clusters in 2D (you may want to use t-SNE or UMAP for high-dimensional data)\n",
    "#     from sklearn.decomposition import PCA\n",
    "\n",
    "#     pca = PCA(n_components=2)\n",
    "#     embeddings_2d = pca.fit_transform(embeddings)\n",
    "\n",
    "#     plt.figure(figsize=(12, 8))\n",
    "#     scatter = plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], c=labels, cmap=\"viridis\")\n",
    "#     plt.colorbar(scatter)\n",
    "#     plt.title(\"Cluster Visualization (PCA)\")\n",
    "#     plt.xlabel(\"First Principal Component\")\n",
    "#     plt.ylabel(\"Second Principal Component\")\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "# # Main execution\n",
    "# df = pd.read_pickle(\"merged.pkl\")\n",
    "# df = df[(df[\"model\"] == \"ViT-Finetuned\") & (df[\"dataset\"] == \"SPAC\")]\n",
    "# df = df.reset_index(drop=True)\n",
    "\n",
    "# embeddings = np.stack(df[\"embedding\"].to_numpy())\n",
    "\n",
    "# # Find optimal number of clusters\n",
    "# optimal_clusters = find_optimal_clusters(embeddings, max_clusters=300)\n",
    "\n",
    "# # Perform clustering\n",
    "# labels = cluster_embeddings(embeddings, optimal_clusters)\n",
    "\n",
    "# # Analyze clusters\n",
    "# analyze_clusters(df, embeddings, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.calibration import LabelEncoder\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering, MiniBatchKMeans\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from gorillatracker.classification.metrics import analyse_embedding_space, formatted_names\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "\n",
    "def calculate_metrics(embeddings, labels, true_labels):\n",
    "    \"\"\"wraps analyse_embedding_space and adds class-weighted F1 score and precision\"\"\"\n",
    "    assert len(labels) == len(true_labels) == len(embeddings)\n",
    "    df = pd.DataFrame({\"embedding\": embeddings.tolist(), \"label\": labels.tolist()})\n",
    "    metrics = analyse_embedding_space(df)\n",
    "\n",
    "    # \"label matching problem\" in clustering evaluation\n",
    "    matched_labels = match_labels(true_labels, labels)\n",
    "\n",
    "    # Compute class-weighted F1 score\n",
    "    f1 = f1_score(true_labels, matched_labels, average=\"weighted\")\n",
    "\n",
    "    # Compute class-weighted precision\n",
    "    precision = precision_score(true_labels, matched_labels, average=\"weighted\")\n",
    "\n",
    "    metrics.update({\"weighted_f1_score\": f1, \"weighted_precision\": precision})\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def match_labels(true_labels, predicted_labels):\n",
    "    \"\"\"\n",
    "    Match predicted cluster labels to true labels using the Hungarian algorithm.\n",
    "\n",
    "    NOTE(liamvdv): Necessary because cluster labels are arbitrary and may not match the true labels but represent the same clusters.\n",
    "    \"\"\"\n",
    "    assert len(true_labels) == len(predicted_labels)\n",
    "\n",
    "    # Create confusion matrix\n",
    "    cm = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "    # Use the Hungarian algorithm to find the best matching\n",
    "    row_ind, col_ind = linear_sum_assignment(-cm)\n",
    "\n",
    "    label_mapping = {pred: true for pred, true in zip(col_ind, row_ind)}\n",
    "\n",
    "    matched_labels = np.array([label_mapping.get(label, label) for label in predicted_labels])\n",
    "\n",
    "    return matched_labels\n",
    "\n",
    "\n",
    "def _run_kmeans(args):\n",
    "    scaled_embeddings, k, true_labels = args\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    labels = kmeans.fit_predict(scaled_embeddings)\n",
    "    metric = calculate_metrics(scaled_embeddings, labels, true_labels)\n",
    "    metric[\"algorithm\"] = \"K-means\"\n",
    "    metric[\"algorithm_arg\"] = k\n",
    "    return metric\n",
    "\n",
    "\n",
    "def find_optimal_kmeans_parallel(embeddings, true_labels, max_clusters=30):\n",
    "    scaler = StandardScaler()\n",
    "    scaled_embeddings = scaler.fit_transform(embeddings)\n",
    "\n",
    "    # Prepare arguments for parallel processing\n",
    "    args_list = [(scaled_embeddings, k, true_labels) for k in range(2, max_clusters + 1)]\n",
    "\n",
    "    metrics = []\n",
    "\n",
    "    with ProcessPoolExecutor(max_workers=10) as executor:\n",
    "        # Submit all jobs\n",
    "        future_to_k = {executor.submit(_run_kmeans, args): args[1] for args in args_list}\n",
    "\n",
    "        # Collect results as they complete\n",
    "        for future in tqdm(as_completed(future_to_k), total=len(future_to_k), desc=\"Running K-means\"):\n",
    "            k = future_to_k[future]\n",
    "            try:\n",
    "                metric = future.result()\n",
    "                metrics.append(metric)\n",
    "            except Exception as exc:\n",
    "                print(f\"K-means for k={k} generated an exception: {exc}\")\n",
    "\n",
    "    # Sort metrics by number of clusters\n",
    "    metrics.sort(key=lambda x: x[\"algorithm_arg\"])\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def find_optimal_kmeans(embeddings, true_labels, max_clusters=30):\n",
    "    scaler = StandardScaler()\n",
    "    scaled_embeddings = scaler.fit_transform(embeddings)\n",
    "\n",
    "    metrics = []\n",
    "\n",
    "    for k in tqdm(range(2, max_clusters + 1), desc=\"Running K-means\", total=max_clusters):\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "        labels = kmeans.fit_predict(scaled_embeddings)\n",
    "        metric = calculate_metrics(scaled_embeddings, labels, true_labels)\n",
    "        metric[\"algorithm\"] = \"K-means\"\n",
    "        metric[\"algorithm_arg\"] = k\n",
    "        metrics.append(metric)\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def find_optimal_eps_for_dbscan(embeddings):\n",
    "    scaler = StandardScaler()\n",
    "    scaled_embeddings = scaler.fit_transform(embeddings)\n",
    "\n",
    "    # Find optimal eps\n",
    "    \"\"\"\n",
    "    The purpose of this code is to create what's called a \"k-distance graph\". This graph, when plotted, shows the distance to the nearest neighbor for each point, sorted in ascending order. It's used to help determine a good value for the 'eps' parameter in DBSCAN.\n",
    "    \n",
    "    In a k-distance graph, you typically look for an \"elbow\" - a point where the distance starts increasing more rapidly. This elbow often indicates a good value for 'eps'. Points before the elbow are considered \"close\" to their neighbors and might form clusters, while points after the elbow are farther from their neighbors and might be considered noise or outliers.\n",
    "    \"\"\"\n",
    "    neighbors = NearestNeighbors(n_neighbors=2)\n",
    "    neighbors_fit = neighbors.fit(scaled_embeddings)\n",
    "    distances, indices = neighbors_fit.kneighbors(scaled_embeddings)\n",
    "    distances = np.sort(distances, axis=0)\n",
    "    distances = distances[:, 1]\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(distances)\n",
    "    plt.xlabel(\"Points\")\n",
    "    plt.ylabel(\"Distance\")\n",
    "    plt.title(\"K-distance Graph\")\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Please examine the K-distance graph and input the 'elbow' point for eps:\")\n",
    "    eps = float(input(\"Enter the eps value: \"))\n",
    "\n",
    "    return eps\n",
    "\n",
    "\n",
    "def find_optimal_dbscan(embeddings, true_labels, eps: float):\n",
    "    scaler = StandardScaler()\n",
    "    scaled_embeddings = scaler.fit_transform(embeddings)\n",
    "\n",
    "    # Find optimal min_samples\n",
    "    metrics = []\n",
    "    min_samples_range = range(2, 11)\n",
    "\n",
    "    for min_samples in min_samples_range:\n",
    "        dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "        labels = dbscan.fit_predict(scaled_embeddings)\n",
    "        metric = calculate_metrics(scaled_embeddings, labels, true_labels)\n",
    "        metric[\"algorithm\"] = \"Agglomerative Clustering\"\n",
    "        metric[\"algorithm_arg\"] = min_samples\n",
    "        metrics.append(metric)\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def find_optimal_agglomerative(embeddings, true_labels, max_clusters=30):\n",
    "    scaler = StandardScaler()\n",
    "    scaled_embeddings = scaler.fit_transform(embeddings)\n",
    "\n",
    "    # TODO(liamvdv): unsure if this is the correct way to cache the dendrogram\n",
    "\n",
    "    # Initialize AgglomerativeClustering with max_clusters and compute_full_tree=True\n",
    "    agg_clustering = AgglomerativeClustering(n_clusters=max_clusters, compute_full_tree=True, linkage=\"ward\")\n",
    "\n",
    "    # Fit the model to compute the full tree\n",
    "    agg_clustering.fit(scaled_embeddings)\n",
    "\n",
    "    metrics = []\n",
    "\n",
    "    # Iterate in reverse order from max_clusters to 2\n",
    "    for k in range(max_clusters, 1, -1):\n",
    "        # Extract labels for k clusters\n",
    "        labels = agg_clustering.labels_\n",
    "\n",
    "        metric = calculate_metrics(scaled_embeddings, labels, true_labels)\n",
    "        metric[\"algorithm\"] = \"Agglomerative Clustering\"\n",
    "        metric[\"algorithm_arg\"] = k\n",
    "        metrics.append(metric)\n",
    "\n",
    "        # If not at the last iteration, update the number of clusters\n",
    "        if k > 2:\n",
    "            agg_clustering.n_clusters = k - 1\n",
    "            agg_clustering.labels_ = agg_clustering.labels_[agg_clustering.n_clusters_ - 1]\n",
    "\n",
    "    # Reverse the metrics list to have it in ascending order of clusters\n",
    "    metrics.reverse()\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "# agg_metrics = find_optimal_agglomerative(embeddings, true_labels, max_clusters=30)\n",
    "# eps = find_optimal_eps_for_dbscan(embeddings)\n",
    "# dbscan_metrics = find_optimal_dbscan(embeddings, true_labels, eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "\n",
    "def visualize_alg_metrics(in_metrics, formatted_names):\n",
    "    \"\"\"\n",
    "    Create a grid of charts where every metric is shown for in_metrics.\n",
    "\n",
    "    Parameters:\n",
    "    in_metrics (list): List of dictionaries containing metrics for each run\n",
    "    formatted_names (dict): Dictionary mapping metric names to formatted display names\n",
    "\n",
    "    Returns:\n",
    "    None (displays the plot)\n",
    "    \"\"\"\n",
    "    alg = in_metrics[0][\"algorithm\"]\n",
    "    # Get the list of metrics (excluding 'algorithm' and 'algorithm_arg')\n",
    "    metrics = [key for key in in_metrics[0].keys() if key not in [\"algorithm\", \"algorithm_arg\"]]\n",
    "\n",
    "    # Calculate the grid dimensions\n",
    "    n_metrics = len(in_metrics)\n",
    "    n_cols = 3  # You can adjust this for a different layout\n",
    "    n_rows = math.ceil(n_metrics / n_cols)\n",
    "\n",
    "    # Create the plot\n",
    "    fig, axs = plt.subplots(n_rows, n_cols, figsize=(20, 5 * n_rows))\n",
    "    fig.suptitle(f\"{alg} Clustering Metrics\", fontsize=16)\n",
    "\n",
    "    # Flatten the axs array for easier indexing\n",
    "    axs = axs.flatten()\n",
    "\n",
    "    # Plot each metric\n",
    "    for i, metric in enumerate(metrics):\n",
    "        x = [m[\"algorithm_arg\"] for m in in_metrics]\n",
    "        y = [m[metric] for m in in_metrics]\n",
    "\n",
    "        axs[i].plot(x, y, marker=\"o\")\n",
    "        axs[i].set_title(formatted_names.get(metric, metric))\n",
    "        axs[i].set_xlabel(\"Number of Clusters\")\n",
    "        axs[i].set_ylabel(\"Value\")\n",
    "        axs[i].grid(True)\n",
    "\n",
    "    # Remove any unused subplots\n",
    "    for j in range(i + 1, len(axs)):\n",
    "        fig.delaxes(axs[j])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"merged.pkl\")\n",
    "df = df[(df[\"model\"] == \"ViT-Finetuned\") & (df[\"dataset\"] == \"SPAC\")]\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "embeddings = np.stack(df[\"embedding\"].to_numpy())\n",
    "true_labels = df[\"label\"].to_numpy()\n",
    "\n",
    "# Find optimal number of clusters for K-means\n",
    "kmeans_metrics = find_optimal_kmeans(embeddings, true_labels, max_clusters=200)\n",
    "visualize_alg_metrics(kmeans_metrics, formatted_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def with_min_label_count(df: pd.DataFrame, min: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create a copy of the DataFrame, keeping only rows where the label appears\n",
    "    at least 'min' times in the original DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Input DataFrame. Must have a 'label' column.\n",
    "    min (int): Minimum number of occurrences for a label to be included.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A new DataFrame with filtered rows.\n",
    "\n",
    "    Raises:\n",
    "    ValueError: If 'label' column is not present in the DataFrame.\n",
    "    \"\"\"\n",
    "    if 'label' not in df.columns:\n",
    "        raise ValueError(\"DataFrame must have a 'label' column\")\n",
    "\n",
    "    # Count label occurrences\n",
    "    label_counts = df['label'].value_counts()\n",
    "\n",
    "    # Get labels that appear at least 'min' times\n",
    "    valid_labels = label_counts[label_counts >= min].index\n",
    "\n",
    "    # Create a new DataFrame with only the valid labels\n",
    "    filtered_df = df[df['label'].isin(valid_labels)].copy()\n",
    "\n",
    "    return filtered_df\n",
    "\n",
    "m3df = with_min_label_count(df, 3)\n",
    "m3embeddings = np.stack(m3df[\"embedding\"].to_numpy())\n",
    "m3true_labels = m3df[\"label\"].to_numpy()\n",
    "\n",
    "m3kmeans_metrics = find_optimal_kmeans(m3embeddings, m3true_labels, max_clusters=200)\n",
    "visualize_alg_metrics(m3kmeans_metrics, formatted_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
