{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gorillatracker.classification.clustering import (\n",
    "    batch_visualize_alg_metrics,\n",
    ")\n",
    "from gorillatracker.classification.metrics import formatted_names\n",
    "import pandas as pd\n",
    "\n",
    "metrics_df = pd.read_pickle(\"/workspaces/gorillatracker/sep26_clustering_results.pkl\")\n",
    "# visualize_alg_metrics(metrics_df, \"SPAC\", \"ViT-Finetuned\", \"KMeans\", formatted_names)\n",
    "batch_visualize_alg_metrics(\n",
    "    metrics_df,\n",
    "    \"SPAC+min3\",  # \"Bristol\",\n",
    "    [\"ViT-Pretrained\", \"ViT-Finetuned\", \"EfN-Pretrained\"],\n",
    "    [\"KMeans\", \"AgglomerativeClustering\"],\n",
    "    formatted_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics_df.columns)\n",
    "print(metrics_df.dataset.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def format_value(value):\n",
    "    if pd.isna(value) or value == \"nan\":\n",
    "        return \"nan\"\n",
    "    elif isinstance(value, (int, float)):\n",
    "        return f\"{value:.3f}\"\n",
    "    else:\n",
    "        return str(value)\n",
    "\n",
    "\n",
    "def generate_first_graphic_data(df, dataset, model):\n",
    "    df = df.copy()\n",
    "\n",
    "    # First, filter the DataFrame\n",
    "    spac_max3_vit_df = df[(df[\"dataset\"] == dataset) & (df[\"model\"] == model)]\n",
    "\n",
    "    # List to store the summary of metrics\n",
    "    metrics_summary = []\n",
    "\n",
    "    # List of algorithms to check\n",
    "    algorithms_to_check = spac_max3_vit_df[\"algorithm\"].unique()\n",
    "\n",
    "    # Iterate over each algorithm\n",
    "    for algorithm in algorithms_to_check:\n",
    "        # Filter DataFrame for the current algorithm\n",
    "        algorithm_df = spac_max3_vit_df[spac_max3_vit_df[\"algorithm\"] == algorithm]\n",
    "\n",
    "        # Get the rows with max silhouette score, max dunn index, min davies-bouldin index, and max calinski-harabasz index\n",
    "        max_silhouette_row = algorithm_df.loc[algorithm_df[\"silhouette_coefficient\"].idxmax()]\n",
    "        max_dunn_row = algorithm_df.loc[algorithm_df[\"dunn_index\"].idxmax()]\n",
    "        min_davies_bouldin_row = algorithm_df.loc[algorithm_df[\"davies_bouldin_index\"].idxmin()]\n",
    "        max_calinski_harabasz_row = algorithm_df.loc[algorithm_df[\"calinski_harabasz_index\"].idxmax()]\n",
    "\n",
    "        # Add rows to the metrics summary\n",
    "        metrics_summary.extend(\n",
    "            [\n",
    "                [\n",
    "                    \"Silhouette Score (max)\",\n",
    "                    algorithm,\n",
    "                    max_silhouette_row[\"adjusted_rand_score\"],\n",
    "                    max_silhouette_row[\"homogeneity_score\"],\n",
    "                    max_silhouette_row[\"completeness_score\"],\n",
    "                    max_silhouette_row[\"v_measure_score\"],\n",
    "                    max_silhouette_row[\"n_clusters\"],\n",
    "                ],\n",
    "                [\n",
    "                    \"Dunn Index (max)\",\n",
    "                    algorithm,\n",
    "                    max_dunn_row[\"adjusted_rand_score\"],\n",
    "                    max_dunn_row[\"homogeneity_score\"],\n",
    "                    max_dunn_row[\"completeness_score\"],\n",
    "                    max_dunn_row[\"v_measure_score\"],\n",
    "                    max_dunn_row[\"n_clusters\"],\n",
    "                ],\n",
    "                [\n",
    "                    \"Davies-Bouldin Index (min)\",\n",
    "                    algorithm,\n",
    "                    min_davies_bouldin_row[\"adjusted_rand_score\"],\n",
    "                    min_davies_bouldin_row[\"homogeneity_score\"],\n",
    "                    min_davies_bouldin_row[\"completeness_score\"],\n",
    "                    min_davies_bouldin_row[\"v_measure_score\"],\n",
    "                    min_davies_bouldin_row[\"n_clusters\"],\n",
    "                ],\n",
    "                [\n",
    "                    \"Calinski-Harabasz Index (max)\",\n",
    "                    algorithm,\n",
    "                    max_calinski_harabasz_row[\"adjusted_rand_score\"],\n",
    "                    max_calinski_harabasz_row[\"homogeneity_score\"],\n",
    "                    max_calinski_harabasz_row[\"completeness_score\"],\n",
    "                    max_calinski_harabasz_row[\"v_measure_score\"],\n",
    "                    max_calinski_harabasz_row[\"n_clusters\"],\n",
    "                ],\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    # Convert to DataFrame for easy display\n",
    "    return pd.DataFrame(\n",
    "        metrics_summary,\n",
    "        columns=[\n",
    "            \"Approach\",\n",
    "            \"Algorithm\",\n",
    "            \"Rand Score\",\n",
    "            \"Homogeneity\",\n",
    "            \"Completeness\",\n",
    "            \"V-Measure\",\n",
    "            \"n_clusters\",\n",
    "        ],\n",
    "    )\n",
    "\n",
    "\n",
    "def generate_latex_table(df, dataset, model):\n",
    "    latex_code = [\n",
    "        r\"\\begin{table}[H]\",\n",
    "        r\"    \\centering\",\n",
    "        r\"    \\resizebox{\\textwidth}{!}{%\",\n",
    "        r\"    \\begin{tabular}{llccccc}\",\n",
    "        r\"    \\toprule\",\n",
    "        r\"    \\multicolumn{2}{l}{Approaches} & \\multicolumn{5}{c}{Metrics} \\\\\",\n",
    "        r\"     \\cmidrule(lr){3-7}\",\n",
    "        r\"    & & ARI & Homogeneity & Completeness & V-Measure & Clusters Found \\\\\",\n",
    "        r\"    \\midrule\",\n",
    "    ]\n",
    "\n",
    "    for approach in df[\"Approach\"].unique():\n",
    "        latex_code.append(f\"    \\multicolumn{{7}}{{l}}{{\\\\textbf{{{approach}}}}} \\\\\\\\\")\n",
    "        group = df[df[\"Approach\"] == approach]\n",
    "        for _, row in group.iterrows():\n",
    "            formatted_values = [\n",
    "                format_value(row[\"Rand Score\"]),\n",
    "                format_value(row[\"Homogeneity\"]),\n",
    "                format_value(row[\"Completeness\"]),\n",
    "                format_value(row[\"V-Measure\"]),\n",
    "                str(row[\"n_clusters\"]),\n",
    "            ]\n",
    "            latex_code.append(f\"    & {row['Algorithm']} & {' & '.join(formatted_values)} \\\\\\\\\")\n",
    "        if approach != df[\"Approach\"].unique()[-1]:\n",
    "            latex_code.append(r\"    \\midrule\")\n",
    "\n",
    "    latex_code.extend(\n",
    "        [\n",
    "            r\"    \\bottomrule\",\n",
    "            r\"    \\end{tabular}%\",\n",
    "            r\"    }\",\n",
    "            r\"    \\caption{Comparison of Clustering Approaches and Algorithms across Various Metrics. Dataset is \"\n",
    "            + dataset\n",
    "            + \" and Model is \"\n",
    "            + model\n",
    "            + \"}\",\n",
    "            r\"    \\label{tab:clustering-metrics}\",\n",
    "            r\"\\end{table}\",\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return \"\\n\".join(latex_code)\n",
    "\n",
    "\n",
    "dataset = \"SPAC+min3\"\n",
    "model = \"ViT-Finetuned\"\n",
    "metrics_summary_df = generate_first_graphic_data(metrics_df, dataset, model)\n",
    "print(generate_latex_table(metrics_summary_df, dataset, model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def extract_data_best_algorithms_can_do(df):\n",
    "    # Group by Dataset, Algorithm, and Model\n",
    "    grouped = df.groupby(['dataset', 'algorithm', 'model'])\n",
    "    \n",
    "    report_data = []\n",
    "    \n",
    "    for (dataset, algorithm, model), group in grouped:\n",
    "        # Find the row with the top ARI (adjusted_rand_score)\n",
    "        top_ari_row = group.loc[group['adjusted_rand_score'].idxmax()]\n",
    "        \n",
    "        # Get the true number of clusters (assuming it's constant for each dataset-algorithm-model combination)\n",
    "        true_clusters = group['n_true_clusters'].iloc[0]\n",
    "        \n",
    "        # Find the row with n_clusters closest to true_clusters\n",
    "        closest_clusters_row = group.loc[(group['n_clusters'] - true_clusters).abs().idxmin()]\n",
    "        \n",
    "        report_data.append({\n",
    "            'Dataset': dataset,\n",
    "            'Algorithm': algorithm,\n",
    "            'Model': model,\n",
    "            'True Clusters': true_clusters,\n",
    "            'ARI': top_ari_row['adjusted_rand_score'],\n",
    "            'Clusters': top_ari_row['n_clusters'],\n",
    "            'True ARI': closest_clusters_row['adjusted_rand_score']\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(report_data)\n",
    "\n",
    "def format_value(value):\n",
    "    if abs(value) < 1e-10:  # Consider values very close to zero as zero\n",
    "        return \"0\"\n",
    "    return f\"{value:.3f}\".rstrip('0').rstrip('.').replace('-0', '0')\n",
    "\n",
    "def latex_table_best_algorithms_can_do(df):\n",
    "    latex_code = [\n",
    "        r\"\\begin{table}[H]\",\n",
    "        r\"    \\centering\",\n",
    "        r\"    \\resizebox{\\textwidth}{!}{%\",\n",
    "        r\"    \\begin{tabular}{l|c|ccc|ccc|ccc}\",\n",
    "        r\"    \\toprule\",\n",
    "        r\"    & True & \\multicolumn{3}{c|}{KMeans} & \\multicolumn{3}{c|}{AgglomerativeClustering} & \\multicolumn{3}{c}{HDBSCAN} \\\\\",\n",
    "        r\"    Embedding Space & Clusters & True ARI & ARI & Clusters & True ARI & ARI & Clusters & True ARI & ARI & Clusters \\\\\",\n",
    "        r\"    \\midrule\",\n",
    "    ]\n",
    "\n",
    "    datasets = df['Dataset'].unique()\n",
    "    for i, dataset in enumerate(datasets):\n",
    "        group = df[df['Dataset'] == dataset]\n",
    "        latex_code.append(f\"    \\\\multicolumn{{11}}{{l}}{{\\\\textbf{{{dataset}}}}} \\\\\\\\\")\n",
    "\n",
    "        # Pivot the data\n",
    "        pivot = group.pivot(index='Model', columns='Algorithm', \n",
    "                            values=['True Clusters', 'ARI', 'Clusters', 'True ARI'])\n",
    "        \n",
    "        for model in pivot.index:\n",
    "            row = pivot.loc[model]\n",
    "            true_clusters = row['True Clusters'].iloc[0]  # All algorithms have same True Clusters\n",
    "            values = [\n",
    "                model,\n",
    "                str(int(true_clusters)),\n",
    "            ]\n",
    "            for alg in ['KMeans', 'AgglomerativeClustering', 'HDBSCAN']:\n",
    "                values.extend([\n",
    "                    format_value(row['True ARI'][alg]),\n",
    "                    format_value(row['ARI'][alg]),\n",
    "                    str(int(row['Clusters'][alg]))\n",
    "                ])\n",
    "            latex_code.append(\"    \" + \" & \".join(values) + r\" \\\\\")\n",
    "        \n",
    "        # Add a midrule between datasets, except after the last dataset\n",
    "        if i < len(datasets) - 1:\n",
    "            latex_code.append(r\"    \\midrule\")\n",
    "\n",
    "    latex_code.extend([\n",
    "        r\"    \\bottomrule\",\n",
    "        r\"    \\end{tabular}%\",\n",
    "        r\"    }\",\n",
    "        r\"    \\caption{Clustering Performance Metrics for Different Models and Algorithms}\",\n",
    "        r\"    \\label{tab:clustering-algorithms}\",\n",
    "        r\"\\end{table}\",\n",
    "    ])\n",
    "\n",
    "    return \"\\n\".join(latex_code)\n",
    "\n",
    "# Example usage:\n",
    "# Assuming metrics_df is your input DataFrame\n",
    "report_df = extract_data_best_algorithms_can_do(metrics_df)\n",
    "print(latex_table_best_algorithms_can_do(report_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE(liamvdv): We're investiagting the performance of silouhette score -- maybe we didn't push the clustering far enough.\n",
    "from gorillatracker.classification.clustering import (\n",
    "    sweep_clustering_algorithms,\n",
    "    configs,\n",
    "    speed_level,\n",
    "    visualize_alg_metrics,\n",
    "    calculate_metrics,\n",
    "    batch_visualize_alg_metrics,\n",
    "    EXT_MERGED_DF,\n",
    "    MERGED_DF,\n",
    "    param_grid,\n",
    "    speed,\n",
    ")\n",
    "from gorillatracker.classification.metrics import formatted_names\n",
    "import pandas as pd\n",
    "\n",
    "results2_df = sweep_clustering_algorithms(\n",
    "    EXT_MERGED_DF,\n",
    "    [(\"SPAC+min3\", \"ViT-Finetuned\", \"KMeans\", param_grid({\"n_clusters\": range(3, 1000, speed(1, 5, 20))}))],\n",
    "    cache_dir=\"./cache_cluster_sweep\",\n",
    ")\n",
    "# visualize_alg_metrics(metrics_df, \"SPAC\", \"ViT-Finetuned\", \"KMeans\", formatted_names)\n",
    "batch_visualize_alg_metrics(\n",
    "    results2_df,\n",
    "    \"SPAC+min3\",\n",
    "    [\"ViT-Pretrained\", \"ViT-Finetuned\", \"EfN-Pretrained\"],\n",
    "    [\"KMeans\", \"AgglomerativeClustering\"],\n",
    "    formatted_names,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
