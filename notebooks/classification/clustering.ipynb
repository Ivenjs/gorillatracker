{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.metrics import adjusted_rand_score, confusion_matrix, f1_score, precision_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from gorillatracker.classification.metrics import analyse_embedding_space, formatted_names\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "\n",
    "def calculate_metrics(embeddings, labels, true_labels):\n",
    "    \"\"\"wraps analyse_embedding_space and adds class-weighted F1 score and precision\"\"\"\n",
    "    assert len(labels) == len(true_labels) == len(embeddings)\n",
    "    df = pd.DataFrame({\"embedding\": embeddings.tolist(), \"label\": labels.tolist()})\n",
    "    metrics = analyse_embedding_space(df)\n",
    "\n",
    "    # \"label matching problem\" in clustering evaluation\n",
    "    matched_labels = match_labels(true_labels, labels)\n",
    "\n",
    "    # Compute class-weighted F1 score\n",
    "    f1 = f1_score(true_labels, matched_labels, average=\"weighted\")\n",
    "\n",
    "    # Compute class-weighted precision\n",
    "    precision = precision_score(true_labels, matched_labels, average=\"weighted\")\n",
    "\n",
    "    # NOTE(liamvdv): not over matched labels, can handle arbitrary cluster labels\n",
    "    # https://en.wikipedia.org/wiki/Rand_index#/media/File:Example_for_Adjusted_Rand_index.svg\n",
    "    # https://scikit-learn.org/stable/modules/generated/sklearn.metrics.adjusted_rand_score.html\n",
    "    # The adjusted Rand index is thus ensured to have a value close to 0.0 for random labeling independently of the number of clusters and samples and exactly 1.0 when the clusterings are identical (up to a permutation). The adjusted Rand index is bounded below by -0.5 for especially discordant clusterings.\n",
    "    ars = adjusted_rand_score(true_labels, labels)\n",
    "\n",
    "    metrics.update({\"weighted_f1_score\": f1, \"weighted_precision\": precision, \"adjusted_rand_score\": ars})\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def match_labels(true_labels, predicted_labels):\n",
    "    \"\"\"\n",
    "    Match predicted cluster labels to true labels using the Hungarian algorithm.\n",
    "\n",
    "    NOTE(liamvdv): Necessary because cluster labels are arbitrary and may not match the true labels but represent the same clusters.\n",
    "    \"\"\"\n",
    "    assert len(true_labels) == len(predicted_labels)\n",
    "\n",
    "    # Create confusion matrix\n",
    "    cm = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "    # Use the Hungarian algorithm to find the best matching\n",
    "    row_ind, col_ind = linear_sum_assignment(-cm)\n",
    "\n",
    "    label_mapping = {pred: true for pred, true in zip(col_ind, row_ind)}\n",
    "\n",
    "    matched_labels = np.array([label_mapping.get(label, label) for label in predicted_labels])\n",
    "\n",
    "    return matched_labels\n",
    "\n",
    "\n",
    "def _run_kmeans(args):\n",
    "    scaled_embeddings, k, true_labels = args\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    labels = kmeans.fit_predict(scaled_embeddings)\n",
    "    metric = calculate_metrics(scaled_embeddings, labels, true_labels)\n",
    "    metric[\"algorithm\"] = \"K-means\"\n",
    "    metric[\"algorithm_arg\"] = k\n",
    "    return metric\n",
    "\n",
    "\n",
    "def find_optimal_kmeans_parallel(embeddings, true_labels, max_clusters=30):\n",
    "    scaler = StandardScaler()\n",
    "    scaled_embeddings = scaler.fit_transform(embeddings)\n",
    "\n",
    "    # Prepare arguments for parallel processing\n",
    "    args_list = [(scaled_embeddings, k, true_labels) for k in range(2, max_clusters + 1)]\n",
    "\n",
    "    metrics = []\n",
    "\n",
    "    with ProcessPoolExecutor(max_workers=10) as executor:\n",
    "        # Submit all jobs\n",
    "        future_to_k = {executor.submit(_run_kmeans, args): args[1] for args in args_list}\n",
    "\n",
    "        # Collect results as they complete\n",
    "        for future in tqdm(as_completed(future_to_k), total=len(future_to_k), desc=\"Running K-means\"):\n",
    "            k = future_to_k[future]\n",
    "            try:\n",
    "                metric = future.result()\n",
    "                metrics.append(metric)\n",
    "            except Exception as exc:\n",
    "                print(f\"K-means for k={k} generated an exception: {exc}\")\n",
    "\n",
    "    # Sort metrics by number of clusters\n",
    "    metrics.sort(key=lambda x: x[\"algorithm_arg\"])\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def find_optimal_kmeans(embeddings, true_labels, max_clusters=30, step=1):\n",
    "    scaler = StandardScaler()\n",
    "    scaled_embeddings = scaler.fit_transform(embeddings)\n",
    "\n",
    "    metrics = []\n",
    "\n",
    "    for k in tqdm(range(2, max_clusters + 1, step), desc=\"Running K-means\", total=max_clusters // step):\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "        labels = kmeans.fit_predict(scaled_embeddings)\n",
    "        metric = calculate_metrics(scaled_embeddings, labels, true_labels)\n",
    "        metric[\"algorithm\"] = \"K-means\"\n",
    "        metric[\"algorithm_arg\"] = k\n",
    "        metrics.append(metric)\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def find_optimal_eps_for_dbscan(embeddings):\n",
    "    scaler = StandardScaler()\n",
    "    scaled_embeddings = scaler.fit_transform(embeddings)\n",
    "\n",
    "    # Find optimal eps\n",
    "    \"\"\"\n",
    "    The purpose of this code is to create what's called a \"k-distance graph\". This graph, when plotted, shows the distance to the nearest neighbor for each point, sorted in ascending order. It's used to help determine a good value for the 'eps' parameter in DBSCAN.\n",
    "    \n",
    "    In a k-distance graph, you typically look for an \"elbow\" - a point where the distance starts increasing more rapidly. This elbow often indicates a good value for 'eps'. Points before the elbow are considered \"close\" to their neighbors and might form clusters, while points after the elbow are farther from their neighbors and might be considered noise or outliers.\n",
    "    \"\"\"\n",
    "    neighbors = NearestNeighbors(n_neighbors=2)\n",
    "    neighbors_fit = neighbors.fit(scaled_embeddings)\n",
    "    distances, indices = neighbors_fit.kneighbors(scaled_embeddings)\n",
    "    distances = np.sort(distances, axis=0)\n",
    "    distances = distances[:, 1]\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(distances)\n",
    "    plt.xlabel(\"Points\")\n",
    "    plt.ylabel(\"Distance\")\n",
    "    plt.title(\"K-distance Graph\")\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Please examine the K-distance graph and input the 'elbow' point for eps:\")\n",
    "    eps = float(input(\"Enter the eps value: \"))\n",
    "\n",
    "    return eps\n",
    "\n",
    "\n",
    "def find_optimal_dbscan(embeddings, true_labels, eps: float):\n",
    "    scaler = StandardScaler()\n",
    "    scaled_embeddings = scaler.fit_transform(embeddings)\n",
    "\n",
    "    # Find optimal min_samples\n",
    "    metrics = []\n",
    "    min_samples_range = range(2, 11)\n",
    "\n",
    "    for min_samples in min_samples_range:\n",
    "        dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "        labels = dbscan.fit_predict(scaled_embeddings)\n",
    "        metric = calculate_metrics(scaled_embeddings, labels, true_labels)\n",
    "        metric[\"algorithm\"] = \"Agglomerative Clustering\"\n",
    "        metric[\"algorithm_arg\"] = min_samples\n",
    "        metrics.append(metric)\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def find_optimal_agglomerative(embeddings, true_labels, max_clusters=30):\n",
    "    scaler = StandardScaler()\n",
    "    scaled_embeddings = scaler.fit_transform(embeddings)\n",
    "\n",
    "    # TODO(liamvdv): unsure if this is the correct way to cache the dendrogram\n",
    "\n",
    "    # Initialize AgglomerativeClustering with max_clusters and compute_full_tree=True\n",
    "    agg_clustering = AgglomerativeClustering(n_clusters=max_clusters, compute_full_tree=True, linkage=\"ward\")\n",
    "\n",
    "    # Fit the model to compute the full tree\n",
    "    agg_clustering.fit(scaled_embeddings)\n",
    "\n",
    "    metrics = []\n",
    "\n",
    "    # Iterate in reverse order from max_clusters to 2\n",
    "    for k in range(max_clusters, 1, -1):\n",
    "        # Extract labels for k clusters\n",
    "        labels = agg_clustering.labels_\n",
    "\n",
    "        metric = calculate_metrics(scaled_embeddings, labels, true_labels)\n",
    "        metric[\"algorithm\"] = \"Agglomerative Clustering\"\n",
    "        metric[\"algorithm_arg\"] = k\n",
    "        metrics.append(metric)\n",
    "\n",
    "        # If not at the last iteration, update the number of clusters\n",
    "        if k > 2:\n",
    "            agg_clustering.n_clusters = k - 1\n",
    "            agg_clustering.labels_ = agg_clustering.labels_[agg_clustering.n_clusters_ - 1]\n",
    "\n",
    "    # Reverse the metrics list to have it in ascending order of clusters\n",
    "    metrics.reverse()\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "# agg_metrics = find_optimal_agglomerative(embeddings, true_labels, max_clusters=30)\n",
    "# eps = find_optimal_eps_for_dbscan(embeddings)\n",
    "# dbscan_metrics = find_optimal_dbscan(embeddings, true_labels, eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "\n",
    "def visualize_alg_metrics(in_metrics, formatted_names):\n",
    "    \"\"\"\n",
    "    Create a grid of charts where every metric is shown for in_metrics.\n",
    "\n",
    "    Parameters:\n",
    "    in_metrics (list): List of dictionaries containing metrics for each run\n",
    "    formatted_names (dict): Dictionary mapping metric names to formatted display names\n",
    "\n",
    "    Returns:\n",
    "    None (displays the plot)\n",
    "    \"\"\"\n",
    "    alg = in_metrics[0][\"algorithm\"]\n",
    "    # Get the list of metrics (excluding 'algorithm' and 'algorithm_arg')\n",
    "    metrics = [\n",
    "        key\n",
    "        for key in in_metrics[0].keys()\n",
    "        if key not in [\"algorithm\", \"algorithm_arg\"] and not key.startswith(\"global_\")\n",
    "    ]\n",
    "\n",
    "    # Calculate the grid dimensions\n",
    "    n_metrics = len(in_metrics)\n",
    "    n_cols = 5  # You can adjust this for a different layout\n",
    "    n_rows = math.ceil(n_metrics / n_cols)\n",
    "\n",
    "    # Create the plot\n",
    "    fig, axs = plt.subplots(n_rows, n_cols, figsize=(20, 5 * n_rows))\n",
    "\n",
    "    # Flatten the axs array for easier indexing\n",
    "    axs = axs.flatten()\n",
    "\n",
    "    # Plot each metric\n",
    "    for i, metric in enumerate(metrics):\n",
    "        x = [m[\"algorithm_arg\"] for m in in_metrics]\n",
    "        y = [m[metric] for m in in_metrics]\n",
    "\n",
    "        axs[i].plot(x, y, marker=\"o\")\n",
    "        axs[i].set_title(formatted_names.get(metric, metric))\n",
    "        axs[i].set_xlabel(\"Number of Clusters\")\n",
    "        axs[i].set_ylabel(\"Value\")\n",
    "        axs[i].grid(True)\n",
    "\n",
    "    # Remove any unused subplots\n",
    "    for j in range(i + 1, len(axs)):\n",
    "        fig.delaxes(axs[j])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def with_min_label_count(df: pd.DataFrame, min: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create a copy of the DataFrame, keeping only rows where the label appears\n",
    "    at least 'min' times in the original DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Input DataFrame. Must have a 'label' column.\n",
    "    min (int): Minimum number of occurrences for a label to be included.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A new DataFrame with filtered rows.\n",
    "\n",
    "    Raises:\n",
    "    ValueError: If 'label' column is not present in the DataFrame.\n",
    "    \"\"\"\n",
    "    if \"label\" not in df.columns:\n",
    "        raise ValueError(\"DataFrame must have a 'label' column\")\n",
    "\n",
    "    # Count label occurrences\n",
    "    label_counts = df[\"label\"].value_counts()\n",
    "\n",
    "    # Get labels that appear at least 'min' times\n",
    "    valid_labels = label_counts[label_counts >= min].index\n",
    "\n",
    "    # Create a new DataFrame with only the valid labels\n",
    "    filtered_df = df[df[\"label\"].isin(valid_labels)].copy()\n",
    "\n",
    "    return filtered_df\n",
    "\n",
    "\n",
    "def upto_max_label_count(df: pd.DataFrame, max: int, seed: int = 42) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create a copy of the DataFrame, randomly sampling rows for labels that exceed\n",
    "    the specified maximum count, while keeping all rows for labels that don't exceed the maximum.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Input DataFrame. Must have a 'label' column.\n",
    "    max (int): Maximum number of occurrences for each label.\n",
    "    seed (int): Random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A new DataFrame with sampled rows.\n",
    "\n",
    "    Raises:\n",
    "    ValueError: If 'label' column is not present in the DataFrame.\n",
    "    \"\"\"\n",
    "    if \"label\" not in df.columns:\n",
    "        raise ValueError(\"DataFrame must have a 'label' column\")\n",
    "\n",
    "    # Set random seed for reproducibility\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Group the DataFrame by label\n",
    "    grouped = df.groupby(\"label\")\n",
    "\n",
    "    sampled_dfs = []\n",
    "\n",
    "    for label, group in grouped:\n",
    "        if len(group) > max:\n",
    "            # Randomly sample 'max' rows from the group\n",
    "            sampled_group = group.sample(n=max, random_state=seed)\n",
    "        else:\n",
    "            # Keep all rows if the count doesn't exceed max\n",
    "            sampled_group = group\n",
    "\n",
    "        sampled_dfs.append(sampled_group)\n",
    "\n",
    "    # Concatenate all sampled groups\n",
    "    result_df = pd.concat(sampled_dfs).reset_index(drop=True)\n",
    "\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MERGED_DF = pd.read_pickle(\"merged.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPAC  - ViT-Finetuned - K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = MERGED_DF\n",
    "df = df[(df[\"model\"] == \"ViT-Finetuned\") & (df[\"dataset\"] == \"SPAC\")]\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "embeddings = np.stack(df[\"embedding\"].to_numpy())\n",
    "true_labels = df[\"label\"].to_numpy()\n",
    "\n",
    "print(\"Actual Number of Individuals\", len(np.unique(true_labels)))\n",
    "kmeans_metrics = find_optimal_kmeans(embeddings, true_labels, max_clusters=180, step=5)\n",
    "visualize_alg_metrics(kmeans_metrics, formatted_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m3df = with_min_label_count(df, 3)\n",
    "m3embeddings = np.stack(m3df[\"embedding\"].to_numpy())\n",
    "m3true_labels = m3df[\"label\"].to_numpy()\n",
    "\n",
    "print(\"Actual Number of Individuals\", len(np.unique(m3true_labels)))\n",
    "m3kmeans_metrics = find_optimal_kmeans(m3embeddings, m3true_labels, max_clusters=25, step=1)\n",
    "visualize_alg_metrics(m3kmeans_metrics, formatted_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m3ma10df = upto_max_label_count(m3df, 10)\n",
    "m3ma10embeddings = np.stack(m3ma10df[\"embedding\"].to_numpy())\n",
    "m3ma10true_labels = m3ma10df[\"label\"].to_numpy()\n",
    "\n",
    "print(\"Actual Number of Individuals\", len(np.unique(m3ma10true_labels)))\n",
    "m3ma10kmeans_metrics = find_optimal_kmeans(m3ma10embeddings, m3ma10true_labels, max_clusters=20, step=1)\n",
    "visualize_alg_metrics(m3ma10kmeans_metrics, formatted_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = MERGED_DF\n",
    "df = df[(df[\"model\"] == \"ViT-Pretrained\") & (df[\"dataset\"] == \"MNIST\")]\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "embeddings = np.stack(df[\"embedding\"].to_numpy())\n",
    "true_labels = df[\"label\"].to_numpy()\n",
    "\n",
    "print(\"Actual Number of Individuals\", len(np.unique(true_labels)))\n",
    "kmeans_metrics = find_optimal_kmeans(embeddings, true_labels, max_clusters=20, step=1)\n",
    "visualize_alg_metrics(kmeans_metrics, formatted_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic Test Small (20 clusters; 20 images per cluster)\n",
    "\n",
    "- Note that we might be off by 1-2 clusters if the starting K-Means initializiations points are in the same cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = MERGED_DF\n",
    "df = df[(df[\"model\"] == \"Synthetic\") & (df[\"dataset\"] == \"Synthetic 20c 20n\")]\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "embeddings = np.stack(df[\"embedding\"].to_numpy())\n",
    "true_labels = df[\"label\"].to_numpy()\n",
    "\n",
    "print(\"Actual Number of Individuals\", len(np.unique(true_labels)))\n",
    "kmeans_metrics = find_optimal_kmeans(embeddings, true_labels, max_clusters=30, step=1)\n",
    "visualize_alg_metrics(kmeans_metrics, formatted_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic Test Large (200 clusters; 10 image per cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = MERGED_DF\n",
    "df = df[(df[\"model\"] == \"Synthetic\") & (df[\"dataset\"] == \"Synthetic 200c 10n\")]\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "embeddings = np.stack(df[\"embedding\"].to_numpy())\n",
    "true_labels = df[\"label\"].to_numpy()\n",
    "\n",
    "print(\"Actual Number of Individuals\", len(np.unique(true_labels)))\n",
    "kmeans_metrics = find_optimal_kmeans(embeddings, true_labels, max_clusters=400, step=20)\n",
    "visualize_alg_metrics(kmeans_metrics, formatted_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
