{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gorillatracker.model.wrappers_ssl import MoCoWrapper\n",
    "from gorillatracker.utils.embedding_generator import generate_embeddings, df_from_predictions\n",
    "from gorillatracker.model.wrappers_supervised import TimmEvalWrapper, BaseModuleSupervised\n",
    "from pathlib import Path\n",
    "from gorillatracker.data.nlet_dm import NletDataModule\n",
    "from gorillatracker.data.nlet import build_onelet, SupervisedDataset\n",
    "from torchvision.transforms import Resize, Normalize, Compose\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import timm\n",
    "\n",
    "\n",
    "def get_finetuned_vit() -> MoCoWrapper:\n",
    "    # ViT Large + DinoV2; finetuned with SSL and MoCo Loss\n",
    "    # https://wandb.ai/gorillas/Embedding-VitLarge-MoCo-Face-Sweep/runs/rlemhfix\n",
    "    finetuned = \"/workspaces/gorillatracker/models/ssl/moco-accuracy-0.58.ckpt\"\n",
    "    return MoCoWrapper.load_from_checkpoint(\n",
    "        checkpoint_path=finetuned,\n",
    "        data_module=None,\n",
    "        wandb_run=None,\n",
    "    )\n",
    "\n",
    "\n",
    "def get_mock_loss_kwargs() -> dict:\n",
    "    return {\n",
    "        \"margin\": 1.0,  # From the file\n",
    "        \"s\": 64.0,  # From the file\n",
    "        \"temperature\": 0.07,  # Default value, not specified in the file\n",
    "        \"memory_bank_size\": 4096,  # Default value, not specified in the file\n",
    "        \"embedding_size\": 128,  # From the file\n",
    "        \"batch_size\": 64,  # From the file\n",
    "        \"num_classes\": None,  # Default value, not specified in the file\n",
    "        \"class_distribution\": None,  # Default value, not specified in the file\n",
    "        \"use_focal_loss\": False,  # Default value, not specified in the file\n",
    "        \"k_subcenters\": 1,  # Default value, not specified in the file\n",
    "        \"accelerator\": \"cuda\",  # From the file\n",
    "        \"label_smoothing\": 0.1,  # Default value, not specified in the file\n",
    "        \"l2_alpha\": 0.1,  # From the file\n",
    "        \"l2_beta\": 0.01,  # From the file\n",
    "        \"path_to_pretrained_weights\": \"\",  # From the file\n",
    "        \"use_class_weights\": False,  # Default value, not specified in the file\n",
    "        \"use_dist_term\": False,  # Default value, not specified in the file\n",
    "    }\n",
    "\n",
    "\n",
    "def get_pretrained_vit() -> TimmEvalWrapper:\n",
    "    # ViT Large + DinoV2\n",
    "    model = BaseModuleSupervised(\n",
    "        model_name_or_path=\"timm_eval/vit_large_patch14_dinov2.lvd142m\",\n",
    "        fix_img_size=224,\n",
    "        freeze_backbone=True,\n",
    "        wandb_run=None,\n",
    "        data_module=None,\n",
    "        loss_mode=\"offline\",\n",
    "        **get_mock_loss_kwargs(),\n",
    "    )\n",
    "    # model = TimmEvalWrapper(\n",
    "    #     backbone_name=\"vit_large_patch14_dinov2.lvd142m\",\n",
    "    #     img_size=224,\n",
    "    # )\n",
    "    # model.freeze = lambda: None\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_pretrained_efnet() -> TimmEvalWrapper:\n",
    "    # EfficientNetV2 RW_M + ImageNet V2 1k\n",
    "    model = BaseModuleSupervised(\n",
    "        model_name_or_path=\"timm_eval/efficientnetv2_rw_m\",\n",
    "        # Eff Net does not take img_size as an argument\n",
    "        freeze_backbone=True,\n",
    "        wandb_run=None,\n",
    "        data_module=None,\n",
    "        loss_mode=\"offline\",\n",
    "        **get_mock_loss_kwargs(),\n",
    "    )\n",
    "    c = timm.data.resolve_model_data_config(model)\n",
    "    assert c[\"input_size\"] == (3, 224, 224)\n",
    "    # model = TimmEvalWrapper(backbone_name=\"efficientnetv2_rw_m\")\n",
    "    # model.freeze = lambda: None\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_finetuned_efnet() -> TimmEvalWrapper:\n",
    "    # EfficientNetV2 RW_M + ImageNet V2 1k; finetuned with ????\n",
    "    # TODO(liamvdv): add SSL trained effnet model.\n",
    "    return None\n",
    "\n",
    "\n",
    "def get_model_transforms(model):\n",
    "    resize = getattr(model, \"data_resize_transform\", (224, 224))\n",
    "    model_transforms = Resize(resize)\n",
    "    normalize_transform = Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    use_normalization = getattr(model, \"use_normalization\", True)\n",
    "    # NOTE(liamvdv): normalization_mean, normalization_std are always default.\n",
    "    if use_normalization:\n",
    "        model_transforms = Compose([model_transforms, normalize_transform])\n",
    "    return model_transforms\n",
    "\n",
    "\n",
    "def _get_dataloader(model, path: Path):\n",
    "    data_module = NletDataModule(\n",
    "        data_dir=path,\n",
    "        dataset_class=SupervisedDataset,\n",
    "        nlet_builder=build_onelet,\n",
    "        batch_size=64,\n",
    "        workers=10,\n",
    "        model_transforms=get_model_transforms(model),\n",
    "        training_transforms=lambda x: x,\n",
    "        dataset_names=[\"Showcase\"],\n",
    "    )\n",
    "\n",
    "    data_module.setup(\"validate\")\n",
    "    dls = data_module.val_dataloader()  # val for transforms\n",
    "    assert len(dls) == 1\n",
    "    dl = dls[0]\n",
    "    return dl\n",
    "\n",
    "\n",
    "def get_df(model, path: Path):\n",
    "    dl = _get_dataloader(model, path)\n",
    "    preds = generate_embeddings(model, dl)\n",
    "    df = df_from_predictions(preds)\n",
    "    # TODO(liamvdv): Should be DF of\n",
    "    #                id, embedding, label, label_string, input, model, dataset\n",
    "\n",
    "    def transform_embedding(embedding_list):\n",
    "        return np.array([tensor.item() for tensor in embedding_list])\n",
    "\n",
    "    df[\"embedding\"] = df[\"embedding\"].apply(transform_embedding)\n",
    "    df[\"label\"] = df[\"label\"].apply(lambda x: x.item())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import Resize, Compose, ToTensor, Grayscale, Normalize\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def custom_collate(batch):\n",
    "    model_inputs, original_images, targets = zip(*batch)\n",
    "    return torch.stack(model_inputs), list(original_images), torch.tensor(targets)\n",
    "\n",
    "\n",
    "def get_mnist_dataloader(batch_size=128, num_samples=2000):\n",
    "    # Define transforms for the model input\n",
    "    model_transform = Compose(\n",
    "        [\n",
    "            Resize((224, 224)),\n",
    "            Grayscale(3),  # Convert to 3 channels\n",
    "            ToTensor(),\n",
    "            Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),  # ImageNet normalization\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Define transforms for storing the original image\n",
    "    storage_transform = Compose(\n",
    "        [\n",
    "            Resize((224, 224)),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    class TransformedMNIST(MNIST):\n",
    "        def __init__(self, *args, **kwargs):\n",
    "            super().__init__(*args, **kwargs)\n",
    "            self.model_transform = model_transform\n",
    "            self.storage_transform = storage_transform\n",
    "\n",
    "        def __getitem__(self, index):\n",
    "            img, target = self.data[index], int(self.targets[index])\n",
    "\n",
    "            # Convert to PIL Image\n",
    "            img = Image.fromarray(img.numpy(), mode=\"L\")\n",
    "\n",
    "            return self.model_transform(img), self.storage_transform(img), target\n",
    "\n",
    "    mnist_dataset = TransformedMNIST(root=\"./data\", train=False, download=True)\n",
    "\n",
    "    # Stratified sampling to maintain label distribution\n",
    "    indices = list(range(len(mnist_dataset)))\n",
    "    _, sampled_indices = train_test_split(\n",
    "        indices, test_size=num_samples, stratify=mnist_dataset.targets, random_state=42\n",
    "    )\n",
    "\n",
    "    sampled_dataset = Subset(mnist_dataset, sampled_indices)\n",
    "\n",
    "    return DataLoader(sampled_dataset, batch_size=batch_size, shuffle=False, drop_last=True, collate_fn=custom_collate)\n",
    "\n",
    "\n",
    "def get_mnist_df(model, batch_size=128, num_samples=2000):\n",
    "    model.eval()\n",
    "    dataloader = get_mnist_dataloader(batch_size=batch_size, num_samples=num_samples)\n",
    "\n",
    "    all_embeddings = []\n",
    "    all_labels = []\n",
    "    all_images = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for model_input, original_image, target in tqdm(dataloader, desc=\"Generating embeddings\"):\n",
    "            if torch.cuda.is_available():\n",
    "                model_input = model_input.cuda()\n",
    "                model = model.cuda()\n",
    "\n",
    "            embeddings = model(model_input)\n",
    "\n",
    "            all_embeddings.append(embeddings.cpu().numpy())\n",
    "            all_labels.append(target.numpy())\n",
    "            all_images.extend(original_image)  # original_image is already a list of PIL Images\n",
    "\n",
    "    all_embeddings = np.vstack(all_embeddings)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "\n",
    "    num_samples = len(all_labels)\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"id\": range(num_samples),\n",
    "            \"embedding\": list(all_embeddings),\n",
    "            \"label\": all_labels,\n",
    "            \"label_string\": [str(label) for label in all_labels],\n",
    "            \"input\": all_images,  # Store the actual PIL Image objects\n",
    "        }\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def generate_synthetic_dataset(num_clusters, points_per_cluster, embedding_size=256, image_size=(224, 224), seed=None):\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    all_embeddings = []\n",
    "    all_labels = []\n",
    "    all_images = []\n",
    "\n",
    "    for cluster in range(num_clusters):\n",
    "        # Generate cluster center\n",
    "        center = rng.standard_normal(embedding_size)\n",
    "\n",
    "        # Generate points around the center\n",
    "        # Use standard deviation of 1, which means 95% of points will be within 2 std dev\n",
    "        points = rng.standard_normal((points_per_cluster, embedding_size)) + center\n",
    "\n",
    "        all_embeddings.extend(points)\n",
    "        all_labels.extend([cluster] * points_per_cluster)\n",
    "\n",
    "        # Generate random images (you might want to make these more meaningful)\n",
    "        for _ in range(points_per_cluster):\n",
    "            img = Image.fromarray(rng.integers(0, 256, image_size, dtype=np.uint8), \"L\")\n",
    "            all_images.append(img)\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"id\": range(len(all_labels)),\n",
    "            \"embedding\": list(all_embeddings),\n",
    "            \"label\": all_labels,\n",
    "            \"label_string\": [str(label) for label in all_labels],\n",
    "            \"input\": all_images,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "on_cpu = True\n",
    "models = {\n",
    "    \"ViT-Pretrained\": get_pretrained_vit,\n",
    "    \"ViT-Finetuned\": get_finetuned_vit,\n",
    "    \"EfN-Pretrained\": get_pretrained_efnet,\n",
    "    \"EfN-Finetuned\": get_finetuned_efnet,\n",
    "}\n",
    "\n",
    "# TODO(liamvdv): @robert: why filtered? Worauf sind die Dataset Stats?\n",
    "BRISTOL = Path(\n",
    "    \"/workspaces/gorillatracker/data/supervised/bristol/cross_encounter_validation/cropped_frames_square_filtered\"\n",
    ")\n",
    "SPAC = Path(\"/workspaces/gorillatracker/data/supervised/cxl_all/face_images_square\")\n",
    "datasets = {\n",
    "    \"Bristol\": BRISTOL,\n",
    "    \"SPAC\": SPAC,\n",
    "}\n",
    "dfs = []\n",
    "\n",
    "# Testing Datasets\n",
    "m = \"ViT-Pretrained\"\n",
    "df = get_mnist_df(models[m]())\n",
    "df[\"dataset\"] = \"MNIST\"\n",
    "df[\"model\"] = m\n",
    "dfs.append(df)\n",
    "\n",
    "m = \"EfN-Pretrained\"\n",
    "df = get_mnist_df(models[m]())\n",
    "df[\"dataset\"] = \"MNIST\"\n",
    "df[\"model\"] = m\n",
    "dfs.append(df)\n",
    "\n",
    "testS = generate_synthetic_dataset(20, 20)\n",
    "# c - clusters, n - points per cluster\n",
    "testS[\"dataset\"] = \"Synthetic 20c 20n\"\n",
    "testS[\"model\"] = \"Synthetic\"\n",
    "dfs.append(testS)\n",
    "\n",
    "testL = generate_synthetic_dataset(200, 10)\n",
    "testL[\"dataset\"] = \"Synthetic 200c 10n\"\n",
    "testL[\"model\"] = \"Synthetic\"\n",
    "dfs.append(testL)\n",
    "\n",
    "# Actual Datasets\n",
    "for model_name, get_model in models.items():\n",
    "    for dataset_name, dataset_path in datasets.items():\n",
    "        print(\"Model:\", model_name, \"| Dataset:\", dataset_name, end=\" \")\n",
    "        model = get_model()\n",
    "        if not model:\n",
    "            print(\"Skipping model: Model not yet implemented.\")\n",
    "            continue\n",
    "        if on_cpu:\n",
    "            model = model.cpu()\n",
    "\n",
    "        df = get_df(model, dataset_path)\n",
    "        df[\"dataset\"] = dataset_name\n",
    "        df[\"model\"] = model_name\n",
    "        print(\"| Done. Appending\", len(df), \"rows. Embedding Size:\", df[\"embedding\"].iloc[0].shape)\n",
    "        dfs.append(df)\n",
    "\n",
    "        # Cleanup\n",
    "        del model  # and?: torch.cuda.empty_cache()\n",
    "\n",
    "merged_df = pd.concat(dfs, ignore_index=True)\n",
    "merged_df.to_pickle(\"merged.pkl\")\n",
    "print(\"done\")\n",
    "# vitf_spac = merged_df[(merged_df['model'] == 'ViT-Finetuned') & (merged_df['dataset'] == 'SPAC')]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
