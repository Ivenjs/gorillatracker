{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gorillatracker.model.wrappers_ssl import MoCoWrapper\n",
    "from gorillatracker.utils.embedding_generator import generate_embeddings, df_from_predictions\n",
    "from pathlib import Path\n",
    "from gorillatracker.data.nlet_dm import NletDataModule\n",
    "from gorillatracker.data.nlet import build_onelet, SupervisedDataset, SupervisedKFoldDataset\n",
    "from torchvision.transforms import Resize, Normalize, Compose\n",
    "import pandas as pd\n",
    "\n",
    "# TODO(liamvdv): @robert: why filtered? Worauf sind die Dataset Stats?\n",
    "BRISTOL = Path(\n",
    "    \"/workspaces/gorillatracker/data/supervised/bristol/cross_encounter_validation/cropped_frames_square_filtered\"\n",
    ")\n",
    "SPAC = Path(\"/workspaces/gorillatracker/data/supervised/cxl_all/face_images_square\")\n",
    "\n",
    "\n",
    "def get_moco_model(\n",
    "    checkpoint_path: str = \"/workspaces/gorillatracker/models/ssl/moco-accuracy-0.58.ckpt\",\n",
    ") -> MoCoWrapper:\n",
    "    return MoCoWrapper.load_from_checkpoint(checkpoint_path=checkpoint_path, data_module=None, wandb_run=None)\n",
    "\n",
    "\n",
    "def get_model_transforms(model):\n",
    "    resize = getattr(model, \"data_resize_transform\", (224, 224))\n",
    "    model_transforms = Resize(resize)\n",
    "    normalize_transform = Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    use_normalization = getattr(model, \"use_normalization\", True)\n",
    "    # NOTE(liamvdv): normalization_mean, normalization_std are always default.\n",
    "    if use_normalization:\n",
    "        model_transforms = Compose([model_transforms, normalize_transform])\n",
    "    return model_transforms\n",
    "\n",
    "\n",
    "def _get_dataloader(model, path: Path):\n",
    "    data_module = NletDataModule(\n",
    "        data_dir=path,\n",
    "        dataset_class=SupervisedDataset,\n",
    "        nlet_builder=build_onelet,\n",
    "        batch_size=64,\n",
    "        workers=10,\n",
    "        model_transforms=get_model_transforms(model),\n",
    "        training_transforms=lambda x: x,\n",
    "        dataset_names=[\"Showcase\"],\n",
    "    )\n",
    "\n",
    "    data_module.setup(\"validate\")\n",
    "    dls = data_module.val_dataloader()  # val for transforms\n",
    "    assert len(dls) == 1\n",
    "    dl = dls[0]\n",
    "    return dl\n",
    "\n",
    "\n",
    "def get_df(model, path: Path):\n",
    "    dl = _get_dataloader(model, path)\n",
    "    preds = generate_embeddings(model, dl)\n",
    "    df = df_from_predictions(preds)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73bafb40302b4546a94e604af69b2684",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.22G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/research/lib/python3.10/site-packages/lightly/models/modules/memory_bank.py:88: UserWarning: Memory bank size 'size=4096' does not specify feature dimension. It is recommended to set the feature dimension with 'size=(n, dim)' when creating the memory bank. Distributed training might fail if the feature dimension is not set.\n",
      "  warnings.warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/envs/research/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e72513f219724886b22d79fa732bfe6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Class 120 has less than two samples (samples: 1).\n",
      "Class 125 has less than two samples (samples: 1).\n",
      "Class 127 has less than two samples (samples: 1).\n",
      "Class 129 has less than two samples (samples: 1).\n",
      "Class 130 has less than two samples (samples: 1).\n",
      "Class 132 has less than two samples (samples: 1).\n",
      "Class 134 has less than two samples (samples: 1).\n",
      "Class 135 has less than two samples (samples: 1).\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5643cddaf5fa416aa21dfe96c08fddf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "model = get_moco_model()\n",
    "bristol = get_df(model, BRISTOL)\n",
    "bristol.to_pickle(\"bristol.pkl\")\n",
    "spac = get_df(model, SPAC)\n",
    "spac.to_pickle(\"spac.pkl\")\n",
    "print(\"done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
