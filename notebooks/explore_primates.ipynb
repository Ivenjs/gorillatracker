{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports / Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchvision.transforms import Compose, Resize, ToTensor\n",
    "from gorillatracker.data.builder import build_onelet\n",
    "\n",
    "import logging\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import Any, Literal\n",
    "\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from torch import Tensor\n",
    "from wildlife_datasets import datasets, loader\n",
    "\n",
    "import gorillatracker.type_helper as gtypes\n",
    "from gorillatracker.data.contrastive_sampler import ContrastiveImage, ContrastiveSampler, FlatNlet\n",
    "from gorillatracker.data.nlet import NletDataset, group_images_by_label\n",
    "from gorillatracker.type_helper import Id, Label, Nlet\n",
    "from gorillatracker.utils.labelencoder import LabelEncoder\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "import matplotlib\n",
    "\n",
    "params = {\n",
    "    \"font.size\": 10,\n",
    "    \"font.family\": \"serif\",\n",
    "}\n",
    "\n",
    "\n",
    "matplotlib.rcParams.update(params)\n",
    "\n",
    "\n",
    "def get_ds_dfs(train_factor: float = 0.7, val_factor: float = 0.15, test_factor: float = 0.15) -> pd.DataFrame:\n",
    "    rng = random.Random(42)\n",
    "    dfs = []\n",
    "\n",
    "    for ds_cls in datasets.names_primates:\n",
    "        name = str(ds_cls).split(\".\")[-1][:-2]\n",
    "        if name.endswith(\"v2\"):\n",
    "            name = name[:-2]\n",
    "\n",
    "        if name in {\n",
    "            \"AAUZebraFish\",\n",
    "            \"GreenSeaTurtles\",\n",
    "            \"Drosophila\",\n",
    "            \"SMALST\",\n",
    "            \"AerialCattle2017\",  # DATA off (too big here -> prob video processing needed)\n",
    "            \"PolarBearVidID\",  # DATA off\n",
    "            \"SealIDSegmented\",  # no segmentation\n",
    "            \"BirdIndividualID\",  # License: None\n",
    "            \"BirdIndividualIDSegmented\",  # License: None\n",
    "            \"Giraffes\",  # License: None\n",
    "            \"IPanda50\",  # License: None\n",
    "            \"NyalaData\",  # License: None\n",
    "            # check for license problems\n",
    "            # \"HumpbackWhaleID\" # not in paper, fine license\n",
    "            # \"HappyWhale\" # not in paper, fine license\n",
    "            # \"LionData\" # not in paper, not found online? (license?)\n",
    "            # \"MacaqueFaces\" # not in paper, fine license -> CC BY 4.0\n",
    "            \"NOAARightWhale\",  # not in paper, license raises questions\n",
    "        }:\n",
    "            continue\n",
    "        d = loader.load_dataset(\n",
    "            ds_cls,\n",
    "            \"/workspaces/gorillatracker/data/WildlifeReID-10k/data\",\n",
    "            \"/workspaces/gorillatracker/data/WildlifeReID-10k/dataframes\",\n",
    "        )\n",
    "        columns = [\"identity\", \"path\", \"bbox\"] if \"bbox\" in d.df.columns else [\"identity\", \"path\"]\n",
    "        df = d.df.loc[:, columns]\n",
    "        df.drop(df[df[\"identity\"] == d.unknown_name].index, inplace=True)\n",
    "        to_drop = df[\"identity\"].value_counts()[df[\"identity\"].value_counts() < 2].index\n",
    "        df.drop(df[df[\"identity\"].isin(to_drop)].index, inplace=True)\n",
    "        df[\"origin\"] = name\n",
    "        df[\"identity\"] = df[\"identity\"].apply(lambda x: f\"{name}_{x}\")\n",
    "        df[\"path\"] = name + \"/\" + df[\"path\"]\n",
    "        df[\"label\"] = LabelEncoder().encode_list(df[\"identity\"].values.tolist())\n",
    "\n",
    "        dfs.append(df)\n",
    "    combined_df = pd.concat(dfs)\n",
    "    combined_df.reset_index(drop=False, inplace=True)\n",
    "\n",
    "    # perform train-val-test split\n",
    "    train_individuals = []\n",
    "    val_individuals = []\n",
    "    test_individuals = []\n",
    "    for name in combined_df[\"origin\"].unique():\n",
    "        df = combined_df[combined_df[\"origin\"] == name]\n",
    "        individuals = df[\"label\"].unique().tolist()\n",
    "        rng.shuffle(individuals)\n",
    "\n",
    "        # split into train-val-test\n",
    "        train_individuals += individuals[: int(train_factor * len(individuals))]\n",
    "        val_individuals += individuals[\n",
    "            int(train_factor * len(individuals)) : int((train_factor + val_factor) * len(individuals))\n",
    "        ]\n",
    "        test_individuals += individuals[int((train_factor + val_factor) * len(individuals)) :]\n",
    "\n",
    "    combined_df[\"split\"] = \"train\"\n",
    "    combined_df.loc[combined_df[\"label\"].isin(val_individuals), \"split\"] = \"val\"\n",
    "    combined_df.loc[combined_df[\"label\"].isin(test_individuals), \"split\"] = \"test\"\n",
    "\n",
    "    return combined_df\n",
    "\n",
    "\n",
    "class MultiSpeciesContrastiveSampler(ContrastiveSampler):\n",
    "    def __init__(self, base_dir: Path, partition: Literal[\"train\", \"val\", \"test\", \"all\"] = \"train\") -> None:\n",
    "        self.base_dir = base_dir\n",
    "        self.ds = get_ds_dfs()\n",
    "        self.ds = self.ds[self.ds[\"split\"] == partition] if partition != \"all\" else self.ds\n",
    "        self.ds.reset_index(drop=False, inplace=True)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> ContrastiveImage:\n",
    "        img = ContrastiveImage(\n",
    "            id=str(idx),\n",
    "            image_path=self.base_dir / self.ds.iloc[idx][\"path\"],\n",
    "            class_label=self.ds.iloc[idx][\"label\"],\n",
    "        )\n",
    "        return img\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.ds)\n",
    "\n",
    "    @property\n",
    "    def class_labels(self) -> list[gtypes.Label]:\n",
    "        return self.ds[\"label\"].unique().tolist()\n",
    "\n",
    "    def positive(\n",
    "        self, sample: ContrastiveImage\n",
    "    ) -> ContrastiveImage:  # must map whatever __getitem__ returns to another sample\n",
    "        positive_class = sample.class_label\n",
    "        if len(self.ds[self.ds[\"label\"] == positive_class]) == 1:\n",
    "            # logger.warning(f\"Only one sample in class {positive_class}. Returning same sample as positive.\")\n",
    "            return sample\n",
    "        positive_indices = self.ds[self.ds[\"label\"] == positive_class].index\n",
    "        positive_indices = positive_indices[positive_indices != sample.id]\n",
    "        positive_index = random.choice(positive_indices)\n",
    "        sample_row = self.ds.iloc[positive_index]\n",
    "\n",
    "        sample = ContrastiveImage(\n",
    "            id=str(positive_index),\n",
    "            image_path=self.base_dir / sample_row[\"path\"],\n",
    "            class_label=sample_row[\"label\"],\n",
    "        )\n",
    "        return sample\n",
    "\n",
    "    # NOTE(memben): First samples a negative class to ensure a more balanced distribution of negatives,\n",
    "    # independent of the number of samples per class\n",
    "    def negative(self, sample: ContrastiveImage) -> ContrastiveImage:\n",
    "        \"\"\"Different class is sampled uniformly at random and a random sample from that class is returned\"\"\"\n",
    "        # filter for the same origin -> species\n",
    "        sample_origin = self.ds.iloc[int(sample.id)][\"origin\"]\n",
    "        same_ds = self.ds[self.ds[\"origin\"] == sample_origin]\n",
    "        same_ds = same_ds[same_ds[\"label\"] != sample.class_label]\n",
    "        negative_class = random.choice(same_ds[\"label\"].unique())\n",
    "        negative_indices = same_ds[same_ds[\"label\"] == negative_class].index\n",
    "        negative_index = random.choice(negative_indices)\n",
    "        sample_row = self.ds.iloc[negative_index]\n",
    "\n",
    "        sample = ContrastiveImage(\n",
    "            id=str(negative_index),\n",
    "            image_path=self.base_dir / sample_row[\"path\"],\n",
    "            class_label=sample_row[\"label\"],\n",
    "        )\n",
    "        return sample\n",
    "\n",
    "    def negative_classes(self, img: ContrastiveImage) -> list[Label]:\n",
    "        class_label = img.class_label\n",
    "        return [label for label in self.contrastive_sampler.class_labels if label != class_label]  # type: ignore\n",
    "\n",
    "\n",
    "class MultiSpeciesSupervisedDataset(NletDataset):\n",
    "    \"\"\"\n",
    "    A dataset that assumes the following directory structure:\n",
    "        base_dir/\n",
    "            train/\n",
    "                ...\n",
    "            val/\n",
    "                ...\n",
    "            test/\n",
    "                ...\n",
    "    Each file is prefixed with the class label, e.g. \"label1_1.jpg\"\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, use_gorillas: bool = True, *args: Any, **kwargs: Any) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        if use_gorillas:\n",
    "            path = (\n",
    "                (\n",
    "                    Path(\"/workspaces/gorillatracker/data/supervised/splits/cxl_faces_openset_seed42_square/\")\n",
    "                    / self.partition\n",
    "                )\n",
    "                if self.partition != \"all\"\n",
    "                else Path(\"/workspaces/gorillatracker/data/supervised/cxl_all/face_images_square\")\n",
    "            )\n",
    "\n",
    "            classes = group_images_by_label(path)\n",
    "            # convert into dataframe with: path, label, origin, split, identity\n",
    "            labels = []\n",
    "            identities = []\n",
    "            paths = []\n",
    "            for _, contrastive_imgs in classes.items():\n",
    "                paths += [img.image_path for img in contrastive_imgs]\n",
    "                labels += [img.class_label for img in contrastive_imgs]\n",
    "                identities += [img.id for img in contrastive_imgs]\n",
    "\n",
    "            gorilla_df = pd.DataFrame(\n",
    "                {\n",
    "                    \"path\": paths,\n",
    "                    \"label\": labels,\n",
    "                    \"identity\": identities,\n",
    "                    \"origin\": \"SPACGorillas2023\",\n",
    "                    \"split\": self.partition,\n",
    "                    \"bbox\": None,\n",
    "                }\n",
    "            )\n",
    "            gorilla_df.reset_index(drop=False, inplace=True)\n",
    "\n",
    "            self.contrastive_sampler.ds = pd.concat([self.contrastive_sampler.ds, gorilla_df])  # type: ignore\n",
    "            self.contrastive_sampler.ds = self.contrastive_sampler.ds.reset_index(drop=True)  # type: ignore\n",
    "\n",
    "    def _get_item(self, idx: Label) -> tuple[tuple[Id, ...], tuple[Tensor, ...], tuple[Label, ...]]:\n",
    "        return super()._get_item(idx)\n",
    "\n",
    "    @property\n",
    "    def num_classes(self) -> int:\n",
    "        return len(self.contrastive_sampler)\n",
    "\n",
    "    @property\n",
    "    def class_distribution(self) -> dict[Label, int]:\n",
    "        return self.contrastive_sampler.ds[\"label\"].value_counts().to_dict()  # type: ignore\n",
    "\n",
    "    def create_contrastive_sampler(self, base_dir: Path) -> MultiSpeciesContrastiveSampler:\n",
    "        \"\"\"\n",
    "        Assumes directory structure:\n",
    "            base_dir/\n",
    "                train/\n",
    "                    ...\n",
    "                val/\n",
    "                    ...\n",
    "                test/\n",
    "                    ...\n",
    "        \"\"\"\n",
    "        return MultiSpeciesContrastiveSampler(base_dir, partition=self.partition)\n",
    "\n",
    "    def _stack_flat_nlet(self, flat_nlet: FlatNlet) -> Nlet:\n",
    "        ids = tuple(str(img.image_path) for img in flat_nlet)\n",
    "        labels = tuple(img.class_label for img in flat_nlet)\n",
    "\n",
    "        values = tuple(self.transform(self._crop_if_necessary(img)) for img in flat_nlet)\n",
    "        return ids, values, labels\n",
    "\n",
    "    def _crop_if_necessary(self, img: ContrastiveImage) -> Image.Image:\n",
    "        pilimg = Image.open(img.image_path)\n",
    "        if \"bbox\" in self.contrastive_sampler.ds.columns and isinstance(  # type: ignore\n",
    "            self.contrastive_sampler.ds.iloc[int(img.id)][\"bbox\"], list  # type: ignore\n",
    "        ):\n",
    "            bbox = self.contrastive_sampler.ds.iloc[int(img.id)][\"bbox\"]  # type: ignore\n",
    "            x, y, w, h = bbox\n",
    "            bbox = (x, y, x + w, y + h)\n",
    "            pilimg = pilimg.crop(bbox)  # type: ignore\n",
    "\n",
    "        pilimg = pilimg.convert(\"RGB\")  # type: ignore\n",
    "\n",
    "        if pilimg.width > 300 and pilimg.height > 300:\n",
    "            ratio = pilimg.width / pilimg.height\n",
    "            if ratio > 1:\n",
    "                pilimg = pilimg.resize((300, int(300 / ratio)))  # type: ignore\n",
    "            else:\n",
    "                pilimg = pilimg.resize((int(300 * ratio), 300))  # type: ignore\n",
    "        return pilimg\n",
    "\n",
    "    def __len__(self) -> Label:\n",
    "        return len(self.contrastive_sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataset ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import RandAugment\n",
    "from torchvision import transforms\n",
    "\n",
    "transform = lambda x: x\n",
    "\n",
    "dataset = MultiSpeciesSupervisedDataset(\n",
    "    use_gorillas=True,\n",
    "    partition=\"val\",\n",
    "    base_dir=Path(\"/workspaces/gorillatracker/data/WildlifeReID-10k/data\"),\n",
    "    transform=transform,\n",
    "    nlet_builder=build_onelet,\n",
    "    aug_num_ops=2,\n",
    "    aug_magnitude=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dataset.contrastive_sampler.ds\n",
    "\n",
    "sample = dataset[0]\n",
    "img = transforms.ToPILImage()(sample[1][0])\n",
    "img.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
