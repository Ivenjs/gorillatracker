{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gorillatracker.utils.embedding_generator import generate_embeddings_from_run, read_embeddings_from_disk\n",
    "import numpy as np\n",
    "\n",
    "regen = False\n",
    "\n",
    "examples = [\n",
    "    (\n",
    "        \"https://wandb.ai/gorillas/Embedding-SwinV2Large-CXL-Open/runs/bp5e1rnx/workspace?nw=nwuserliamvdv\",\n",
    "        \"../swin-large-example-embeddings.pkl\",\n",
    "        {\n",
    "            \"dataset_cls\": \"gorillatracker.datasets.cxl.CXLDataset\",\n",
    "            \"data_dir\": \"/workspaces/gorillatracker/data/splits/ground_truth-cxl-face_images-openset-reid-val-0-test-0-mintraincount-3-seed-42-train-50-val-25-test-25\",\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        \"https://wandb.ai/gorillas/Embedding-SwinV2Large-CXL-Open/runs/bp5e1rnx/workspace?nw=nwuserliamvdv\",\n",
    "        \"../bristol_embeddings-cxl_trained.pkl\",\n",
    "        {\n",
    "            \"dataset_cls\": \"gorillatracker.datasets.bristol.BristolDataset\",\n",
    "            \"data_dir\": \"/workspaces/gorillatracker/data/splits/ground_truth-bristol-cropped_images_face-closedset-mintraincount-3-seed-42-train-0-val-100-test-0\",\n",
    "        },\n",
    "    ),\n",
    "]\n",
    "current = 1\n",
    "\n",
    "if regen:\n",
    "    df = generate_embeddings_from_run(examples[current][0], examples[current][1], **examples[current][2])\n",
    "else:\n",
    "    # df = read_embeddings_from_disk(\"../example-embeddings.pkl\")\n",
    "    df = read_embeddings_from_disk(examples[current][1])\n",
    "\n",
    "df[\"embedding\"] = df[\"embedding\"].apply(lambda x: np.array(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vc = df[\"label_string\"].value_counts()\n",
    "vc.plot(kind=\"box\")\n",
    "vc.mean(), vc.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"label_string\"].unique(), len(df[\"label_string\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df[\"label_string\"].nunique() != df[\"label\"].nunique():\n",
    "    grouped = df.groupby(\"label\")[\n",
    "        \"label_string\"\n",
    "    ].nunique()  # label_string -> label unique, label -> label_string NOT unique\n",
    "    non_unique_mappings = grouped[grouped > 1]\n",
    "    mismatches = df[df[\"label\"].isin(non_unique_mappings.index)]\n",
    "    print(mismatches.head())\n",
    "    # TODO(liamvdv)\n",
    "    # raise ValueError(\"WARNING: Label does not have a 1:1 mapping with label_string\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gorillatracker.scripts.visualize_embeddings import EmbeddingProjector\n",
    "from io import BytesIO\n",
    "from bokeh.io import show, output_notebook, reset_output\n",
    "import base64\n",
    "\n",
    "output_notebook()\n",
    "\n",
    "images = []\n",
    "for image in df[\"input\"]:\n",
    "    buffer = BytesIO()\n",
    "    image.save(buffer, format=\"JPEG\")\n",
    "    image_byte = base64.b64encode(buffer.getvalue()).decode(\"utf-8\")\n",
    "    images.append(image_byte)\n",
    "\n",
    "ep = EmbeddingProjector()\n",
    "low_dim_embeddings = ep.reduce_dimensions(np.stack(df[\"embedding\"]), method=\"tsne\")\n",
    "fig = ep.plot_clusters(\n",
    "    low_dim_embeddings, df[\"label\"], df[\"label_string\"], images, title=\"Embedding Projector\", figsize=(12, 10)\n",
    ")\n",
    "show(fig)\n",
    "reset_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "centroids = df.groupby([\"label\", \"label_string\"])[\"embedding\"].apply(lambda x: np.mean(np.vstack(x), axis=0))\n",
    "centroid_df = pd.DataFrame({\"centroid\": centroids.values})\n",
    "centroid_df[[\"label\", \"label_string\"]] = pd.DataFrame(centroids.index.tolist(), index=centroid_df.index)\n",
    "\n",
    "for label in centroid_df[\"label\"]:\n",
    "    centroid = centroid_df[centroid_df[\"label\"] == label][\"centroid\"].values[0]\n",
    "    embeddings = df[df[\"label\"] == label][\"embedding\"].tolist()\n",
    "    distances = cdist(embeddings, [centroid])\n",
    "    min_distance = np.min(distances)\n",
    "    max_distance = np.max(distances)\n",
    "    avg_distance = np.mean(distances)\n",
    "    centroid_df.loc[centroid_df[\"label\"] == label, \"min_distance\"] = min_distance\n",
    "    centroid_df.loc[centroid_df[\"label\"] == label, \"max_distance\"] = max_distance\n",
    "    centroid_df.loc[centroid_df[\"label\"] == label, \"avg_distance\"] = avg_distance\n",
    "\n",
    "all_dist = cdist(df[\"embedding\"].tolist(), df[\"embedding\"].tolist())\n",
    "all_centroid_dist = cdist(centroid_df[\"centroid\"].tolist(), centroid_df[\"centroid\"].tolist())\n",
    "print(\"all: Global Maximum Embedding Distance\", np.max(all_dist))\n",
    "print(\"all: Global Minimum Embedding Distance\", np.min(all_dist))\n",
    "print(\"all: Global Average Embedding Distance\", np.mean(all_dist))\n",
    "print(\"all: Standard deviation Embedding Distance\", np.std(all_dist))\n",
    "print(\"=\" * 40)\n",
    "print(\"intra: In Class (Self, Centroid) Minimum Distance\", centroid_df[\"min_distance\"].min())\n",
    "print(\"intra: In Class (Self, Centroid) Maximum Distance\", centroid_df[\"max_distance\"].max())\n",
    "print(\"intra: In Class (Self, Centroid) Average Distance\", centroid_df[\"avg_distance\"].mean())\n",
    "print(\"intra: In Class (Self, Centroid) Standard deviation Distance\", centroid_df[\"avg_distance\"].std())\n",
    "print(\"=\" * 40)\n",
    "print(\"inter: Between Class (Centroid1, Centroid2) Minimum Distance\", np.min(all_centroid_dist))\n",
    "print(\"inter: Between Class (Centroid1, Centroid2) Maximum Distance\", np.max(all_centroid_dist))\n",
    "print(\"inter: Between Class (Centroid1, Centroid2) Average Distance\", np.mean(all_centroid_dist))\n",
    "print(\"inter: Between Class (Centroid1, Centroid2) Standard deviation Distance\", np.std(all_centroid_dist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a frequency plot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming dataframe[label_string] contains the number of individuals\n",
    "df[\"label_string\"].value_counts().plot(kind=\"bar\", title=\"Image Distribution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverse_df = df.groupby('label_string').size().reset_index(name='counts')\n",
    "inverse_df['counts'].value_counts().sort_index().plot(kind='bar', title=\"Image/Label Frequency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gorillatracker.metrics import tsne\n",
    "import torch\n",
    "\n",
    "centroid_marker = 1000000\n",
    "# p = tsne(torch.tensor(centroid_df.centroid.tolist()), torch.tensor(centroid_df.label.tolist()), perplexity=min(30, len(centroid_df)-1))\n",
    "p = tsne(\n",
    "    torch.tensor(df.embedding.tolist() + centroid_df.centroid.tolist()),\n",
    "    torch.tensor(df.label.tolist() + [centroid_marker + c for c in centroid_df.label.tolist()]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "# Set autoreload to automatically reload all modules\n",
    "%autoreload 2\n",
    "\n",
    "# from gorillatracker.clustering.thresholds import norm_label_distribution, find_threshold\n",
    "\n",
    "# result = find_threshold(\n",
    "#     df,\n",
    "#     label_column=\"label_string\",\n",
    "#     grid_start=10.0,\n",
    "#     grid_end=22.0,\n",
    "#     grid_num=50,\n",
    "#     unique_percentage=0.2,\n",
    "#     normalize_label_distribution=False, # because we use f1 score average=\"weighted\", thus distribution is cancelled out\n",
    "#     seed=47,\n",
    "# )\n",
    "\n",
    "# result\n",
    "\n",
    "from gorillatracker.clustering.thresholds2 import (\n",
    "    k_fold_threshold_search,\n",
    "    knn1,\n",
    "    knn1_centroid,\n",
    "    knn1_centroid_iqr,\n",
    "    knn5,\n",
    "    knnk_weighted_by_distance,\n",
    "    knnk_weighted_by_geometric_sequence,\n",
    "    downsample_class,\n",
    ")\n",
    "\n",
    "# NOTE(liamvdv): we do not normalize the distribution because we use f1 score average=\"weighted\", thus distribution is cancelled out\n",
    "results, new_perc_folds = k_fold_threshold_search(\n",
    "    downsample_class(df, \"label_string\"),\n",
    "    label_column=\"label_string\",\n",
    "    grid_start=2.0,\n",
    "    grid_end=20.0,\n",
    "    grid_num=100,\n",
    "    unique_percentage=0.2,\n",
    "    seed=47,\n",
    "    function=knn1,\n",
    ")\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Extracting the data for plotting\n",
    "def plot_metric(eval, new_perc_folds, metric: str = \"f1\"):\n",
    "    thresholds = list(eval.keys())\n",
    "    mAP_overall = [values[f\"all/{metric}\"] for values in eval.values()]\n",
    "    mAP_new = [values[f\"new/{metric}\"] for values in eval.values()]\n",
    "    mAP_non_new = [values[f\"non_new/{metric}\"] for values in eval.values()]\n",
    "    perc_of_new_label = np.mean(new_perc_folds)\n",
    "    assert all(a == b for a, b in zip(new_perc_folds, new_perc_folds))\n",
    "    # Creating the plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(thresholds, mAP_overall, label=f\"Overall {metric}\", marker=\"o\")\n",
    "    plt.plot(thresholds, mAP_new, label=f\"{metric} for new labels\", marker=\"o\")\n",
    "    plt.plot(thresholds, mAP_non_new, label=f\"{metric} for non-new labels\", marker=\"o\")\n",
    "\n",
    "    # Adding title and labels\n",
    "    plt.title(f\"Threshold vs {metric}\" + f\" @ mean {perc_of_new_label * 100:.2f}% new individuals in query set\")\n",
    "    plt.xlabel(\"Threshold\")\n",
    "    plt.ylabel(metric)\n",
    "    plt.legend()\n",
    "\n",
    "    # Showing the plot\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# NOTE(liamvdv): only supports one fold\n",
    "plot_metric(results, new_perc_folds, \"f1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
